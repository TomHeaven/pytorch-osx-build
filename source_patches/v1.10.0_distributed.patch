diff --git a/CMakeLists.txt b/CMakeLists.txt
index 0c11507838..2559ea78ea 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -130,8 +130,16 @@ endif()
 
 # For non-supported platforms, turn USE_DISTRIBUTED off by default.
 # It is not tested and likely won't work without additional changes.
+# Enable  NCCL and Gloo
+set(USE_NCCL OFF)
+set(USE_SYSTEM_NCCL ON)
+set(USE_GLOO ON)
+set(USE_DISTRIBUTED ON)
+set(USE_LIBUV ON)
+set(USE_TENSORPIPE OFF)
+set(USE_MPI OFF)
 if(NOT LINUX AND NOT WIN32)
-  set(USE_DISTRIBUTED OFF CACHE STRING "Use distributed")
+  #set(USE_DISTRIBUTED OFF CACHE STRING "Use distributed")
   # On macOS, if USE_DISTRIBUTED is enabled (specified by the user),
   # then make Gloo build with the libuv transport.
   if(APPLE AND USE_DISTRIBUTED)
@@ -221,7 +229,7 @@ cmake_dependent_option(
     "MLCOMPUTE_FOUND" OFF)
 cmake_dependent_option(
     USE_NCCL "Use NCCL" ON
-    "USE_CUDA OR USE_ROCM;UNIX;NOT APPLE" OFF)
+    "USE_CUDA OR USE_ROCM;UNIX" OFF)
 cmake_dependent_option(USE_RCCL "Use RCCL" ON
     USE_NCCL OFF)
 cmake_dependent_option(
@@ -322,7 +330,6 @@ cmake_dependent_option(USE_CCACHE "Attempt using CCache to wrap the compilation"
 option(WERROR "Build with -Werror supported by the compiler" OFF)
 option(USE_COREML_DELEGATE "Use the CoreML backend through delegate APIs" OFF)
 
-
 if(USE_CCACHE)
   find_program(CCACHE_PROGRAM ccache)
   if(CCACHE_PROGRAM)
diff --git a/aten/src/ATen/cuda/detail/LazyNVRTC.cpp b/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
index efdca84838..598a31dfc3 100644
--- a/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
+++ b/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
@@ -13,6 +13,8 @@ namespace _stubs {
 at::DynamicLibrary& getCUDALibrary() {
 #if defined(_WIN32)
   static at::DynamicLibrary lib("nvcuda.dll");
+#elif defined(__APPLE__)
+  static at::DynamicLibrary lib("libcuda.dylib");
 #else
   static at::DynamicLibrary lib("libcuda.so.1");
 #endif
diff --git a/aten/src/ATen/native/ReduceOps.cpp b/aten/src/ATen/native/ReduceOps.cpp
index 3674a35dfb..87028a9109 100644
--- a/aten/src/ATen/native/ReduceOps.cpp
+++ b/aten/src/ATen/native/ReduceOps.cpp
@@ -24,6 +24,7 @@
 #include <numeric>
 #include <vector>
 #include <map>
+#include <iostream>
 #include <cmath>
 #include <cfloat>
 #include <type_traits>
@@ -632,6 +633,43 @@ template<typename T>
 inline typename std::enable_if<!std::is_integral<T>::value, bool>::type isnan_(T x) {
   return std::isnan(x);
 }
+#elif __APPLE__
+// TODO: This the a workaround, which need furthur fix.
+/**
+ ../aten/src/ATen/native/ReduceOps.cpp:634:10: error: no matching function for call to 'isnan'
+  return std::isnan(x);
+         ^~~~~~~~~~
+../aten/src/ATen/native/ReduceOps.cpp:652:12: note: in instantiation of function template specialization 'at::native::(anonymous namespace)::isnan_<c10::BFloat16>' requested here
+        if(isnan_(curr_elem) || (!isnan_(out) && op(curr_elem, out))) {
+           ^
+../aten/src/ATen/native/ReduceOps.cpp:665:84: note: in instantiation of function template specialization 'at::native::cummax_cummin_helper<c10::BFloat16, long long, std::__1::greater_equal<c10::BFloat16> >' requested here
+      at::native::tensor_dim_apply3<scalar_t, int64_t>(self, values, indices, dim, cummax_cummin_helper<scalar_t, int64_t, std::greater_equal<scalar_t>>);
+                                                                                   ^
+/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/math.h:505:1: note: candidate template ignored: requirement 'std::is_floating_point<BFloat16>::value' was not satisfied [with _A1 = c10::BFloat16]
+isnan(_A1 __lcpp_x) _NOEXCEPT
+^
+/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/math.h:513:1: note: candidate template ignored: requirement 'std::is_integral<BFloat16>::value' was not satisfied [with _A1 = c10::BFloat16]
+isnan(_A1) _NOEXCEPT
+^
+../aten/src/ATen/native/ReduceOps.cpp:652:12: error: no matching function for call to 'isnan_'
+        if(isnan_(curr_elem) || (!isnan_(out) && op(curr_elem, out))) {
+           ^~~~~~
+../aten/src/ATen/native/ReduceOps.cpp:700:84: note: in instantiation of function template specialization 'at::native::cummax_cummin_helper<c10::BFloat16, long long, std::__1::less_equal<c10::BFloat16> >' requested here
+      at::native::tensor_dim_apply3<scalar_t, int64_t>(self, values, indices, dim, cummax_cummin_helper<scalar_t, int64_t, std::less_equal<scalar_t>>);
+                                                                                   ^
+../aten/src/ATen/native/ReduceOps.cpp:629:72: note: candidate template ignored: requirement 'std::is_integral<BFloat16>::value' was not satisfied [with T = c10::BFloat16]
+inline typename std::enable_if<std::is_integral<T>::value, bool>::type isnan_(T x) {
+                                                                       ^
+../aten/src/ATen/native/ReduceOps.cpp:633:73: note: candidate template ignored: substitution failure [with T = c10::BFloat16]
+inline typename std::enable_if<!std::is_integral<T>::value, bool>::type isnan_(T x) {
+                                                                        ^
+2 errors generated.
+ */
+template<typename T>
+inline bool isnan_(T x) {
+  //return std::isnan(x);
+  return false;
+}
 #else
 template<typename T>
 inline bool isnan_(T x) {
diff --git a/aten/src/ATen/native/cuda/EmbeddingBag.cu b/aten/src/ATen/native/cuda/EmbeddingBag.cu
index 8d1ef8b8fa..a3b3ae15b0 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBag.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBag.cu
@@ -162,7 +162,7 @@ Tensor embedding_bag_backward_cuda_sum_avg(
 
   cudaStream_t stream = at::cuda::getCurrentCUDAStream();
 
-  ptrdiff_t num_indices = indices.numel();
+  int64_t num_indices = reinterpret_cast<int64_t>(indices.numel());
 
   if (num_indices == 0) {
     // all empty bags
diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake
index ca560288a4..d162fa5757 100644
--- a/cmake/Dependencies.cmake
+++ b/cmake/Dependencies.cmake
@@ -1326,8 +1326,8 @@ if(USE_NCCL)
         "Not using CUDA/ROCM, so disabling USE_NCCL. Suppress this warning with "
         "-DUSE_NCCL=OFF.")
     caffe2_update_option(USE_NCCL OFF)
-  elseif(NOT CMAKE_SYSTEM_NAME STREQUAL "Linux")
-    message(WARNING "NCCL is currently only supported under Linux.")
+  elseif(NOT (CMAKE_SYSTEM_NAME STREQUAL "Linux" OR CMAKE_SYSTEM_NAME STREQUAL "Darwin"))
+    message(WARNING "NCCL is currently only supported under Linux and macOS.")
     caffe2_update_option(USE_NCCL OFF)
   elseif(USE_CUDA)
     include(${CMAKE_CURRENT_LIST_DIR}/External/nccl.cmake)
diff --git a/cmake/Summary.cmake b/cmake/Summary.cmake
index 6d021536c1..c4eed83bb4 100644
--- a/cmake/Summary.cmake
+++ b/cmake/Summary.cmake
@@ -142,6 +142,7 @@ function(caffe2_print_configuration_summary)
   message(STATUS "  USE_NCCL              : ${USE_NCCL}")
   if(${USE_NCCL})
     message(STATUS "    USE_SYSTEM_NCCL     : ${USE_SYSTEM_NCCL}")
+    message(STATUS "    USE_C10D_NCCL       : ${USE_C10D_NCCL}")
   endif()
   message(STATUS "  USE_NNPACK            : ${USE_NNPACK}")
   message(STATUS "  USE_NUMPY             : ${USE_NUMPY}")
diff --git a/cmake/public/cuda.cmake b/cmake/public/cuda.cmake
index 7ba2bb6d4c..93e4356a7c 100644
--- a/cmake/public/cuda.cmake
+++ b/cmake/public/cuda.cmake
@@ -38,8 +38,8 @@ endif()
 message(STATUS "Caffe2: CUDA detected: " ${CUDA_VERSION})
 message(STATUS "Caffe2: CUDA nvcc is: " ${CUDA_NVCC_EXECUTABLE})
 message(STATUS "Caffe2: CUDA toolkit directory: " ${CUDA_TOOLKIT_ROOT_DIR})
-if(CUDA_VERSION VERSION_LESS 10.2)
-  message(FATAL_ERROR "PyTorch requires CUDA 10.2 or above.")
+if(CUDA_VERSION VERSION_LESS 10.1)
+  message(FATAL_ERROR "PyTorch requires CUDA 10.1 or above.")
 endif()
 
 if(CUDA_FOUND)
diff --git a/modules/detectron/CMakeLists.txt b/modules/detectron/CMakeLists.txt
index 8041e71d35..705784da64 100644
--- a/modules/detectron/CMakeLists.txt
+++ b/modules/detectron/CMakeLists.txt
@@ -4,7 +4,11 @@ file(GLOB_RECURSE Detectron_HIP_SRCS ${CMAKE_CURRENT_SOURCE_DIR}/*.hip)
 
 if(BUILD_CAFFE2_OPS)
   if(USE_OPENMP AND OPENMP_FOUND)
-    Set(OpenMP_link ${OpenMP_CXX_LIBRARIES})
+    if (${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
+      Set(OpenMP_link -Xpreprocessor -fopenmp -lomp -lgomp)
+    else()
+      Set(OpenMP_link ${OpenMP_CXX_LIBRARIES})
+    endif()
   endif()
 
   # Note(ilijar): Since Detectron ops currently have no
diff --git a/test/cpp/api/CMakeLists.txt b/test/cpp/api/CMakeLists.txt
index f1fb4b6123..daf0e6735e 100644
--- a/test/cpp/api/CMakeLists.txt
+++ b/test/cpp/api/CMakeLists.txt
@@ -52,7 +52,11 @@ endif()
 
 add_executable(test_api ${TORCH_API_TEST_SOURCES})
 target_include_directories(test_api PRIVATE ${ATen_CPU_INCLUDE})
-target_link_libraries(test_api PRIVATE torch gtest)
+if (${CMAKE_SYSTEM_NAME} MATCHES "Darwin")
+  target_link_libraries(test_api PRIVATE torch gtest -Xpreprocessor -fopenmp -lomp -lgomp)
+else()
+  target_link_libraries(test_api PRIVATE torch gtest)
+endif()
 
 if(USE_CUDA)
   target_link_libraries(test_api PRIVATE
diff --git a/test/cpp/rpc/CMakeLists.txt b/test/cpp/rpc/CMakeLists.txt
index 3997f8753e..50989d129e 100644
--- a/test/cpp/rpc/CMakeLists.txt
+++ b/test/cpp/rpc/CMakeLists.txt
@@ -4,6 +4,7 @@ set(TORCH_RPC_TEST_SOURCES
   ${TORCH_RPC_TEST_DIR}/e2e_test_base.cpp
   ${TORCH_RPC_TEST_DIR}/test_wire_serialization.cpp
 )
+
 set(TORCH_RPC_TEST_DEPENDENCY_LIBS
   torch gtest
 )
diff --git a/torch/csrc/distributed/rpc/testing/init.cpp b/torch/csrc/distributed/rpc/testing/init.cpp
index 0eca2a63d1..5225637883 100644
--- a/torch/csrc/distributed/rpc/testing/init.cpp
+++ b/torch/csrc/distributed/rpc/testing/init.cpp
@@ -33,7 +33,7 @@ PyObject* faulty_agent_init(PyObject* _unused, PyObject* noargs) {
 
   // Import the rpc_module so we can subclass TensorPipeAgent
   py::module rpc_module = py::module::import("torch.distributed.rpc");
-
+#ifdef USE_TENSORPIPE
   shared_ptr_class_<FaultyTensorPipeRpcBackendOptions>(
       module,
       "FaultyTensorPipeRpcBackendOptions",
@@ -125,7 +125,7 @@ PyObject* faulty_agent_init(PyObject* _unused, PyObject* noargs) {
           (std::vector<WorkerInfo>(TensorPipeAgent::*)() const) &
               TensorPipeAgent::getWorkerInfos,
           py::call_guard<py::gil_scoped_release>());
-
+   #endif
   Py_RETURN_TRUE;
 }
 
diff --git a/torch/distributed/rpc/__init__.py b/torch/distributed/rpc/__init__.py
index 42ed41f00b..ee9280320a 100644
--- a/torch/distributed/rpc/__init__.py
+++ b/torch/distributed/rpc/__init__.py
@@ -48,17 +48,21 @@ if is_available():
         get_rpc_timeout,
         enable_gil_profiling,
         RpcBackendOptions,
-        _TensorPipeRpcBackendOptionsBase,
+        #_TensorPipeRpcBackendOptionsBase,
         RpcAgent,
         PyRRef,
-        TensorPipeAgent,
+        #TensorPipeAgent,
         RemoteProfilerManager,
         WorkerInfo,
         _DEFAULT_INIT_METHOD,
-        _DEFAULT_NUM_WORKER_THREADS,
+        #_DEFAULT_NUM_WORKER_THREADS,
         _UNSET_RPC_TIMEOUT,
         _DEFAULT_RPC_TIMEOUT_SEC,
     )  # noqa: F401
+
+    _TensorPipeRpcBackendOptionsBase = object
+    TensorPipeAgent = object
+    _DEFAULT_NUM_WORKER_THREADS = 16
 
     from . import api, backend_registry, functions
     from .api import *  # noqa: F401,F403
diff --git a/torch/distributed/rpc/constants.py b/torch/distributed/rpc/constants.py
index 1ec79b0091..d0fb12c9b1 100644
--- a/torch/distributed/rpc/constants.py
+++ b/torch/distributed/rpc/constants.py
@@ -2,11 +2,12 @@ from datetime import timedelta
 
 from torch._C._distributed_rpc import (
     _DEFAULT_INIT_METHOD,
-    _DEFAULT_NUM_WORKER_THREADS,
+    #_DEFAULT_NUM_WORKER_THREADS,
     _DEFAULT_RPC_TIMEOUT_SEC,
     _UNSET_RPC_TIMEOUT,
 )
 
+_DEFAULT_NUM_WORKER_THREADS = 16
 
 # For any RpcAgent.
 DEFAULT_RPC_TIMEOUT_SEC: float = _DEFAULT_RPC_TIMEOUT_SEC
diff --git a/torch/distributed/rpc/options.py b/torch/distributed/rpc/options.py
index 0c32a57731..fc8ced24a2 100644
--- a/torch/distributed/rpc/options.py
+++ b/torch/distributed/rpc/options.py
@@ -1,4 +1,5 @@
-from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase
+#from torch._C._distributed_rpc import _TensorPipeRpcBackendOptionsBase
+_TensorPipeRpcBackendOptionsBase = object
 from . import constants as rpc_contants
 
 import torch
