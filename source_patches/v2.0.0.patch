diff --git a/.ci/pytorch/win-test-helpers/setup_pytorch_env.bat b/.ci/pytorch/win-test-helpers/setup_pytorch_env.bat
index 043d67f843c..2b197ce065a 100644
--- a/.ci/pytorch/win-test-helpers/setup_pytorch_env.bat
+++ b/.ci/pytorch/win-test-helpers/setup_pytorch_env.bat
@@ -25,6 +25,17 @@ if not errorlevel 0 exit /b
 @echo on
 popd
 
+:: Pin unittest-xml-reporting to freeze printing test summary logic, related: https://github.com/pytorch/pytorch/issues/69014
+
+pip install "ninja==1.10.0.post1" future "hypothesis==5.35.1" "expecttest==0.1.3" "librosa>=0.6.2" "scipy==1.6.3" psutil pillow "unittest-xml-reporting<=3.2.0,>=2.0.0" pytest pytest-xdist pytest-shard pytest-rerunfailures "xdoctest==1.0.2" "pygments==2.12.0" "opt-einsum>=3.3"
+if errorlevel 1 exit /b
+if not errorlevel 0 exit /b
+
+pip uninstall -y sympy
+if errorlevel 1 exit /b
+if not errorlevel 0 exit /b
+
+=======
 set DISTUTILS_USE_SDK=1
 
 if not "%USE_CUDA%"=="1" goto cuda_build_end
diff --git a/.circleci/scripts/cpp_doc_push_script.sh b/.circleci/scripts/cpp_doc_push_script.sh
index 4c22677e94b..6e66514ae93 100755
--- a/.circleci/scripts/cpp_doc_push_script.sh
+++ b/.circleci/scripts/cpp_doc_push_script.sh
@@ -98,6 +98,9 @@ git commit -m "Generate C++ docs from pytorch/pytorch@${GITHUB_SHA}" || true
 git status
 
 if [[ "${WITH_PUSH:-}" == true ]]; then
+  # push to a temp branch first to trigger CLA check and satisfy branch protections
+  git push -u origin HEAD:pytorchbot/temp-branch-cpp -f
+  sleep 30
   git push -u origin
 fi
 
diff --git a/.circleci/scripts/python_doc_push_script.sh b/.circleci/scripts/python_doc_push_script.sh
index e0e6a4ec948..67718d52311 100755
--- a/.circleci/scripts/python_doc_push_script.sh
+++ b/.circleci/scripts/python_doc_push_script.sh
@@ -1,4 +1,9 @@
 # =================== The following code **should** be executed inside Docker container ===================
+###
+ # @Descripttion: C++ Practice
+ # @Author: TomHeaven
+ # @Date: 2023-03-18 01:49:35
+### 
 
 # Install dependencies
 sudo apt-get -y update
diff --git a/.github/scripts/generate_binary_build_matrix.py b/.github/scripts/generate_binary_build_matrix.py
index 456f84c69ea..95bb9af3c76 100644
--- a/.github/scripts/generate_binary_build_matrix.py
+++ b/.github/scripts/generate_binary_build_matrix.py
@@ -151,25 +151,45 @@ def generate_libtorch_matrix(os: str, abi_version: str,
             # ROCm builds without-deps failed even in ROCm runners; skip for now
             if gpu_arch_type == "rocm" and "without-deps" in libtorch_variant:
                 continue
-            ret.append(
-                {
-                    "gpu_arch_type": gpu_arch_type,
-                    "gpu_arch_version": gpu_arch_version,
-                    "desired_cuda": translate_desired_cuda(
-                        gpu_arch_type, gpu_arch_version
-                    ),
-                    "libtorch_variant": libtorch_variant,
-                    "libtorch_config": abi_version if os == "windows" else "",
-                    "devtoolset": abi_version if os != "windows" else "",
-                    "container_image": LIBTORCH_CONTAINER_IMAGES[
-                        (arch_version, abi_version)
-                    ] if os != "windows" else "",
-                    "package_type": "libtorch",
-                    "build_name": f"libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{abi_version}".replace(
-                        ".", "_"
-                    ),
-                }
-            )
+            desired_cuda = translate_desired_cuda(gpu_arch_type, gpu_arch_version)
+
+            if desired_cuda == "rocm5.1.1" and abi_version == PRE_CXX11_ABI:
+                ret.append(
+                    {
+                        "gpu_arch_type": gpu_arch_type,
+                        "gpu_arch_version": gpu_arch_version,
+                        "desired_cuda": desired_cuda,
+                        "libtorch_variant": libtorch_variant,
+                        "libtorch_config": abi_version if os == "windows" else "",
+                        "devtoolset": abi_version if os != "windows" else "",
+                        "container_image": (
+                            "pytorch/manylinux-builder:rocm5.1.1-cd2573d54f9bd9b8f32b4dd7f182923a846597d5"
+                            if os != "windows" else ""
+                        ),
+                        "package_type": "libtorch",
+                        "build_name": f"libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{abi_version}".replace(
+                            ".", "_"
+                        ),
+                    }
+                )
+            else:
+                ret.append(
+                    {
+                        "gpu_arch_type": gpu_arch_type,
+                        "gpu_arch_version": gpu_arch_version,
+                        "desired_cuda": desired_cuda,
+                        "libtorch_variant": libtorch_variant,
+                        "libtorch_config": abi_version if os == "windows" else "",
+                        "devtoolset": abi_version if os != "windows" else "",
+                        "container_image": LIBTORCH_CONTAINER_IMAGES[
+                            (arch_version, abi_version)
+                        ] if os != "windows" else "",
+                        "package_type": "libtorch",
+                        "build_name": f"libtorch-{gpu_arch_type}{gpu_arch_version}-{libtorch_variant}-{abi_version}".replace(
+                            ".", "_"
+                        ),
+                    }
+                )
     return ret
 
 
@@ -201,6 +221,7 @@ def generate_wheels_matrix(os: str,
             if python_version == "3.11" and gpu_arch_type == "rocm":
                 continue
 
+            desired_cuda = translate_desired_cuda(gpu_arch_type, gpu_arch_version)
             # special 11.7 wheels package without dependencies
             # dependency downloaded via pip install
             if arch_version == "11.7" and os == "linux":
@@ -235,21 +256,3 @@ def generate_wheels_matrix(os: str,
                         ),
                     }
                 )
-
-            ret.append(
-                {
-                    "python_version": python_version,
-                    "gpu_arch_type": gpu_arch_type,
-                    "gpu_arch_version": gpu_arch_version,
-                    "desired_cuda": translate_desired_cuda(
-                        gpu_arch_type, gpu_arch_version
-                    ),
-                    "devtoolset": "cxx11-abi" if arch_version == "cpu-cxx11-abi" else "",
-                    "container_image": WHEEL_CONTAINER_IMAGES[arch_version],
-                    "package_type": package_type,
-                    "build_name": f"{package_type}-py{python_version}-{gpu_arch_type}{gpu_arch_version}".replace(
-                        ".", "_"
-                    ),
-                }
-            )
-    return ret
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 471fc8a8d3d..3ea382d16f3 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -147,8 +147,19 @@ endif()
 
 # For non-supported platforms, turn USE_DISTRIBUTED off by default.
 # It is not tested and likely won't work without additional changes.
+# Enable  NCCL and Gloo
+set(USE_NCCL ON)
+set(USE_SYSTEM_NCCL ON)
+set(USE_GLOO ON)
+set(USE_DISTRIBUTED ON)
+set(USE_LIBUV ON)
+set(USE_TENSORPIPE ON)
+set(USE_MPI OFF)
+set(USE_NUMPY ON)
+set(CMAKE_CXX_STANDARD 14)
+
 if(NOT LINUX AND NOT WIN32)
-  set(USE_DISTRIBUTED OFF CACHE STRING "Use distributed")
+  #set(USE_DISTRIBUTED OFF CACHE STRING "Use distributed")
   # On macOS, if USE_DISTRIBUTED is enabled (specified by the user),
   # then make Gloo build with the libuv transport.
   if(APPLE AND USE_DISTRIBUTED)
@@ -232,7 +243,7 @@ cmake_dependent_option(
     "MPS_FOUND" OFF)
 cmake_dependent_option(
     USE_NCCL "Use NCCL" ON
-    "USE_CUDA OR USE_ROCM;UNIX;NOT APPLE" OFF)
+    "USE_CUDA OR USE_ROCM;UNIX" OFF)
 cmake_dependent_option(USE_RCCL "Use RCCL" ON
     USE_NCCL OFF)
 cmake_dependent_option(
@@ -348,7 +359,6 @@ cmake_dependent_option(
 cmake_dependent_option(
     BUILD_FUNCTORCH "Build Functorch" ON "BUILD_PYTHON" OFF)
 
-
 if(USE_CCACHE)
   find_program(CCACHE_PROGRAM ccache)
   if(CCACHE_PROGRAM)
diff --git a/aten/src/ATen/ConjugateFallback.cpp b/aten/src/ATen/ConjugateFallback.cpp
index cb029d97230..49d6d0c3a55 100644
--- a/aten/src/ATen/ConjugateFallback.cpp
+++ b/aten/src/ATen/ConjugateFallback.cpp
@@ -1,7 +1,8 @@
 #include <ATen/native/MathBitsFallback.h>
 #include <ATen/native/MathBitFallThroughLists.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 struct ConjFallback : MathOpFallback {
   ConjFallback() : MathOpFallback(DispatchKey::Conjugate, "conjugate") {}
   bool is_bit_set(const Tensor& tensor) override {
@@ -62,4 +63,5 @@ TORCH_LIBRARY_IMPL(aten, Conjugate, m) {
   TORCH_VIEW_FNS_NATIVE_FN_REGISTRATION(m)
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/NumericUtils.h b/aten/src/ATen/NumericUtils.h
index a26bbe75baf..e6ddf0f6f3a 100644
--- a/aten/src/ATen/NumericUtils.h
+++ b/aten/src/ATen/NumericUtils.h
@@ -32,7 +32,7 @@ inline C10_HOST_DEVICE bool _isnan(T val) {
 #if defined(__CUDACC__) || defined(__HIPCC__)
   return ::isnan(val);
 #else
-  return std::isnan(val);
+  return std::isnan(double(val));
 #endif
 }
 
diff --git a/aten/src/ATen/cuda/Atomic.cuh b/aten/src/ATen/cuda/Atomic.cuh
index 37033eb17fb..3d60b672e97 100644
--- a/aten/src/ATen/cuda/Atomic.cuh
+++ b/aten/src/ATen/cuda/Atomic.cuh
@@ -6,7 +6,7 @@
 
 #include <ATen/NumericUtils.h>
 
-#if !(defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))
+#if !(defined(USE_ROCM) || ((defined(CUDA_VERSION) && CUDA_VERSION < 11000) || (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))
 #include <cuda_bf16.h>
 #endif
 
@@ -212,7 +212,7 @@ static inline __device__ void gpuAtomicAdd(int64_t *address, int64_t val) {
 }
 
 static inline  __device__ at::Half gpuAtomicAdd(at::Half *address, at::Half val) {
-#if defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)))
+#if defined(USE_ROCM) || ((defined(CUDA_VERSION) && CUDA_VERSION < 10000) || (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)))
   return AtomicFPOp<at::Half>()(address, val,
                                 [](at::Half hsum, at::Half val) {
                                   return hsum + val;
@@ -223,7 +223,7 @@ static inline  __device__ at::Half gpuAtomicAdd(at::Half *address, at::Half val)
 }
 
 static inline __device__ at::BFloat16 gpuAtomicAdd(at::BFloat16 *address, at::BFloat16 val) {
-#if defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))
+#if defined(USE_ROCM) || ((defined(CUDA_VERSION) && CUDA_VERSION < 11000) || (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))
 return AtomicFPOp<at::BFloat16>()(address, val,
                                   [](at::BFloat16 bsum, at::BFloat16 val) {
                                     return bsum + val;
@@ -234,7 +234,7 @@ return AtomicFPOp<at::BFloat16>()(address, val,
 #endif
 }
 
-#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 600)
+#if defined(CUDA_VERSION) && defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 600 || CUDA_VERSION < 8000)
 // from CUDA C Programmic Guide
 static inline __device__ double atomicAdd(double* address, double val)
 #if defined(__clang__) && defined(__CUDA__)
@@ -250,7 +250,7 @@ static inline __device__ double atomicAdd(double* address, double val)
                                 return __double_as_longlong(val + __longlong_as_double(assumed));
                               });
 }
-#elif defined(USE_ROCM) || !(defined(__CUDA_ARCH__))
+#elif defined(USE_ROCM) || !(defined(__CUDA_ARCH__) && (defined(CUDA_VERSION) && CUDA_VERSION < 8000))
 
 /* Note [hip-clang differences to hcc]
  * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
diff --git a/aten/src/ATen/cuda/CUDABlas.cpp b/aten/src/ATen/cuda/CUDABlas.cpp
index 659ef114120..304f9054dd8 100644
--- a/aten/src/ATen/cuda/CUDABlas.cpp
+++ b/aten/src/ATen/cuda/CUDABlas.cpp
@@ -11,7 +11,7 @@
 
 // cublasLT was introduced in CUDA 10.1 but we enable only for 11.1 that also
 // added bf16 support
-#if !defined(USE_ROCM) && !defined(_MSC_VER)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)
 #include <cublasLt.h>
 #endif
 
@@ -263,6 +263,12 @@ void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half)) {
                                    (int) num_batches, rocblas_datatype_f32_r, rocblas_gemm_algo_standard,
                                    0, flag));
 #else
+  #if defined(CUDA_VERSION) && CUDA_VERSION < 11000
+    // On CUDA versions prior to 11, users are required to set the math mode to CUBLAS_TENSOR_OP_MATH
+    // manually to be able to use tensor cores for FP16. On CUDA 11, this is no longer required.
+    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
+  #endif  // defined(CUDA_VERSION) && CUDA_VERSION < 11000
+
   cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
   if (prop->major >= 5){
     TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedExFix(
@@ -281,9 +287,15 @@ void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half)) {
         (c + i * stridec), ldc);
     }
   }
+  #if defined(CUDA_VERSION) && CUDA_VERSION < 11000
+    // On CUDA versions prior to 11, users are required to set the math mode to CUBLAS_TENSOR_OP_MATH
+    // manually to be able to use tensor cores for FP16. On CUDA 11, this is no longer required.
+    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
+  #endif  // defined(CUDA_VERSION) && CUDA_VERSION < 11000
 #endif // USE_ROCM
 }
 
+#if defined(USE_ROCM) || defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16)) {
   // See Note [Writing Nondeterministic Operations]
@@ -296,14 +308,14 @@ void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16)) {
   const float fbeta = beta;
   _cublasAdjustLdLevel3(transa, transb, m, n, k, &lda, &ldb, &ldc);
 
-  #if !defined(USE_ROCM)
+  #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
     TORCH_CUDABLAS_CHECK(cublasGemmStridedBatchedExFix(handle,
                                     opa, opb, (int)m, (int)n, (int)k,
                                     (void*)&falpha, a, CUDA_R_16BF, (int)lda, stridea,
                                     b, CUDA_R_16BF, (int)ldb, strideb,
                                     (void*)&fbeta, c, CUDA_R_16BF, (int)ldc, stridec,
                                     (int)num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP));
-  #else
+  #elif defined(USE_ROCM)
     TORCH_CUDABLAS_CHECK(rocblas_gemm_strided_batched_ex(handle, opa, opb, (int)m, (int)n, (int)k,
                                    (void*)&falpha, a, rocblas_datatype_bf16_r, (int)lda, stridea,
                                    b, rocblas_datatype_bf16_r, (int)ldb, strideb,
@@ -311,8 +323,11 @@ void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16)) {
                                    c, rocblas_datatype_bf16_r, (int)ldc, stridec,
                                    (int) num_batches, rocblas_datatype_f32_r, rocblas_gemm_algo_standard,
                                    0, 0, NULL, NULL));
-  #endif // !defined(USE_ROCM)
+  #else
+    TORCH_CHECK(false, "CUDA BFloat16 bgemm requires CUDA 11 or later");
+  #endif // defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 }
+#endif // USE_ROCM
 
 template <>
 void gemm<double>(CUDABLAS_GEMM_ARGTYPES(double)) {
@@ -418,12 +433,18 @@ void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half)) {
 #else
   cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
   if (prop->major >= 5) {
+#if defined(CUDA_VERSION) && CUDA_VERSION < 11000
+    // On CUDA versions prior to 11, users are required to set the math mode to CUBLAS_TENSOR_OP_MATH
+    // manually to be able to use tensor cores for FP16. On CUDA 11, this is no longer required.
+    TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_TENSOR_OP_MATH));
+#else
     cublasMath_t cublas_flags = CUBLAS_DEFAULT_MATH;
     if (!at::globalContext().allowFP16ReductionCuBLAS()) {
       cublas_flags = static_cast<cublasMath_t>(cublas_flags | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION);
     }
     // Disallow fp16 reductions that could lead to unexpected overflow issues.
     TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, cublas_flags));
+#endif  // defined(CUDA_VERSION) && CUDA_VERSION < 11000
     TORCH_CUDABLAS_CHECK(cublasGemmEx(
         handle,
         opa,
@@ -506,7 +527,7 @@ void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
 }
 #endif
 
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
   globalContext().alertCuBLASConfigNotDeterministic();
@@ -544,9 +565,9 @@ void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16)) {
       CUBLAS_GEMM_DFALT_TENSOR_OP));
   TORCH_CUDABLAS_CHECK(cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH));
 }
-#endif // !defined(USE_ROCM)
+#endif // defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 
-#if !defined(USE_ROCM) && !defined(_MSC_VER)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)
 
 namespace {
 // Following the pattern of CuSparseDescriptor
@@ -834,7 +855,7 @@ template void gemm_and_bias(
     at::BFloat16* result_ptr,
     int64_t result_ld,
     GEMMAndBiasActivationEpilogue activation);
-#endif // !defined(USE_ROCM) && !defined(_MSC_VER)
+#endif // defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)
 
 template <>
 void trsm<float>(CUDABLAS_TRSM_ARGTYPES(float)) {
@@ -1062,6 +1083,7 @@ void gemv<at::Half>(CUDABLAS_GEMV_ARGTYPES(at::Half)) {
       'n', trans_flipped, 1, m, n, alpha, x, incx, a, lda, beta, y, incy);
 }
 
+#if defined(USE_ROCM) || defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void gemv<at::BFloat16>(CUDABLAS_GEMV_ARGTYPES(at::BFloat16)) {
   bool trans_bool = (_cublasOpFromChar(trans) != CUBLAS_OP_N);
@@ -1072,6 +1094,7 @@ void gemv<at::BFloat16>(CUDABLAS_GEMV_ARGTYPES(at::BFloat16)) {
   gemm<at::BFloat16>(
       'n', trans_flipped, 1, m, n, alpha, x, incx, a, lda, beta, y, incy);
 }
+#endif
 
 /* LEVEL 1 BLAS FUNCTIONS */
 
@@ -1101,7 +1124,7 @@ void dot<c10::complex<float>>(CUDABLAS_DOT_ARGTYPES(c10::complex<float>)) {
 
 template <>
 void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half)) {
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 8000
   TORCH_CUDABLAS_CHECK(cublasDotEx(
       handle,
       n,
@@ -1130,7 +1153,7 @@ void dot<at::Half>(CUDABLAS_DOT_ARGTYPES(at::Half)) {
 
 template <>
 void dot<at::BFloat16>(CUDABLAS_DOT_ARGTYPES(at::BFloat16)) {
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   TORCH_CUDABLAS_CHECK(cublasDotEx(
       handle,
       n,
diff --git a/aten/src/ATen/cuda/CUDABlas.h b/aten/src/ATen/cuda/CUDABlas.h
index da01bbe3dcf..96c7fc81842 100644
--- a/aten/src/ATen/cuda/CUDABlas.h
+++ b/aten/src/ATen/cuda/CUDABlas.h
@@ -65,10 +65,12 @@ void gemm<float>(CUDABLAS_GEMM_ARGTYPES(float));
 #endif
 template <>
 void gemm<at::Half>(CUDABLAS_GEMM_ARGTYPES(at::Half));
+#if defined(USE_ROCM) || defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void gemm<at::BFloat16>(CUDABLAS_GEMM_ARGTYPES(at::BFloat16));
+#endif
 
-#if !defined(USE_ROCM) && !defined(_MSC_VER)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)
 enum GEMMAndBiasActivationEpilogue {
   None,
   RELU,
@@ -116,8 +118,10 @@ template <>
 void bgemm<c10::complex<float>>(CUDABLAS_BGEMM_ARGTYPES(c10::complex<float>));
 template <>
 void bgemm<at::Half>(CUDABLAS_BGEMM_ARGTYPES(at::Half));
+#if defined(USE_ROCM) || defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void bgemm<at::BFloat16>(CUDABLAS_BGEMM_ARGTYPES(at::BFloat16));
+#endif
 
 #define CUDABLAS_TRSM_ARGTYPES(Dtype)                                  \
   cublasHandle_t handle, cublasSideMode_t side, cublasFillMode_t uplo, \
@@ -184,8 +188,10 @@ void gemv<c10::complex<float>>(CUDABLAS_GEMV_ARGTYPES(c10::complex<float>));
 #endif
 template <>
 void gemv<at::Half>(CUDABLAS_GEMV_ARGTYPES(at::Half));
+#if defined(USE_ROCM) || defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template <>
 void gemv<at::BFloat16>(CUDABLAS_GEMV_ARGTYPES(at::BFloat16));
+#endif
 
 /* LEVEL 1 BLAS FUNCTIONS */
 
diff --git a/aten/src/ATen/cuda/CUDADataType.h b/aten/src/ATen/cuda/CUDADataType.h
index 97dec2e3cda..d25722c080e 100644
--- a/aten/src/ATen/cuda/CUDADataType.h
+++ b/aten/src/ATen/cuda/CUDADataType.h
@@ -45,7 +45,7 @@ template<> inline cudaDataType getCudaDataType<int>() {
 }
 #endif
 
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 template<> inline cudaDataType getCudaDataType<int16_t>() {
   return CUDA_R_16I;
 }
@@ -80,7 +80,7 @@ inline cudaDataType ScalarTypeToCudaDataType(const c10::ScalarType& scalar_type)
       return CUDA_C_32F;
     case c10::ScalarType::ComplexDouble:
       return CUDA_C_64F;
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && defined(CUDA_VERSION) && CUDA_VERSION >= 11000
     case c10::ScalarType::Short:
       return CUDA_R_16I;
     case c10::ScalarType::Long:
diff --git a/aten/src/ATen/cuda/CUDASparseDescriptors.cpp b/aten/src/ATen/cuda/CUDASparseDescriptors.cpp
index 7c505955576..6319e214ac9 100644
--- a/aten/src/ATen/cuda/CUDASparseDescriptors.cpp
+++ b/aten/src/ATen/cuda/CUDASparseDescriptors.cpp
@@ -26,7 +26,7 @@ void check_supported_cuda_type(cudaDataType cuda_type) {
         prop->minor,
         ")");
   }
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   if (cuda_type == CUDA_R_16BF) {
     cudaDeviceProp* prop = at::cuda::getCurrentDeviceProperties();
     TORCH_CHECK(
@@ -73,7 +73,7 @@ CuSparseDnMatDescriptor::CuSparseDnMatDescriptor(const Tensor& input, int64_t ba
   auto leading_dimension =
       is_row_major ? input_strides[ndim - 2] : input_strides[ndim - 1];
 
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   auto order = is_row_major ? CUSPARSE_ORDER_ROW : CUSPARSE_ORDER_COL;
 #else
   TORCH_INTERNAL_ASSERT(is_column_major, "Expected column major input.");
@@ -177,7 +177,7 @@ CuSparseSpMatCsrDescriptor::CuSparseSpMatCsrDescriptor(const Tensor& input, int6
       value_type // data type of values
       ));
 
-#if AT_USE_HIPSPARSE_GENERIC_52_API() || !defined(USE_ROCM)
+#if AT_USE_HIPSPARSE_GENERIC_52_API() || (defined(CUDA_VERSION) && CUDA_VERSION >= 11000)
   if (ndim == 3 && batch_offset == -1) {
     int batch_count =
         at::native::cuda_int_cast(at::native::batchCount(input), "batch_count");
diff --git a/aten/src/ATen/cuda/CUDASparseDescriptors.h b/aten/src/ATen/cuda/CUDASparseDescriptors.h
index f2681d143db..1f6b4e04a74 100644
--- a/aten/src/ATen/cuda/CUDASparseDescriptors.h
+++ b/aten/src/ATen/cuda/CUDASparseDescriptors.h
@@ -175,6 +175,7 @@ class TORCH_CUDA_CPP_API CuSparseSpMatCsrDescriptor
  public:
   explicit CuSparseSpMatCsrDescriptor(const Tensor& input, int64_t batch_offset = -1);
 
+#if defined(USE_ROCM) || (defined(CUDA_VERSION) && CUDA_VERSION >= 11000)
   std::tuple<int64_t, int64_t, int64_t> get_size() {
     int64_t rows, cols, nnz;
     TORCH_CUDASPARSE_CHECK(cusparseSpMatGetSize(
@@ -199,6 +200,7 @@ class TORCH_CUDA_CPP_API CuSparseSpMatCsrDescriptor
         col_indices.data_ptr(),
         values.data_ptr()));
   }
+#endif
 
 #if AT_USE_CUSPARSE_GENERIC_SPSV()
   void set_mat_fill_mode(bool upper) {
@@ -247,7 +249,7 @@ class TORCH_CUDA_CPP_API CuSparseSpSMDescriptor
 };
 #endif
 
-#if (defined(USE_ROCM) && ROCM_VERSION >= 50200) || !defined(USE_ROCM)
+#if (defined(USE_ROCM) && ROCM_VERSION >= 50200) || (defined(CUDA_VERSION) && CUDA_VERSION >= 11000)
 class TORCH_CUDA_CPP_API CuSparseSpGEMMDescriptor
     : public CuSparseDescriptor<cusparseSpGEMMDescr, &cusparseSpGEMM_destroyDescr> {
  public:
diff --git a/aten/src/ATen/cuda/CublasHandlePool.cpp b/aten/src/ATen/cuda/CublasHandlePool.cpp
index 68a4951bcce..685cbacc9ca 100644
--- a/aten/src/ATen/cuda/CublasHandlePool.cpp
+++ b/aten/src/ATen/cuda/CublasHandlePool.cpp
@@ -101,7 +101,7 @@ cublasHandle_t getCurrentCUDABlasHandle() {
   auto handle = myPoolWindow->reserve(device);
   auto stream = c10::cuda::getCurrentCUDAStream();
   TORCH_CUDABLAS_CHECK(cublasSetStream(handle, stream));
-#if !defined(USE_ROCM)
+#if !defined(USE_ROCM) && CUDA_VERSION >= 11000
   // cublasSetWorkspace not available on CUDA 10.2
   cudaStream_t _stream = stream;
   auto key = std::make_tuple(static_cast<void *>(handle), static_cast<void *>(_stream));
@@ -111,7 +111,7 @@ cublasHandle_t getCurrentCUDABlasHandle() {
   }
   TORCH_CUDABLAS_CHECK(cublasSetWorkspace(handle, workspace_it->second.get(), getChosenWorkspaceSize()));
 #endif
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   // On CUDA >= 11, and architecture >= Ampere, cuBLAS can use TF32 to speedup
   // FP32 data type calculations based on the value of the allow_tf32 flag.
   // To enable TF32, set the math mode of the handle to CUBLAS_TF32_TENSOR_OP_MATH.
diff --git a/aten/src/ATen/cuda/detail/LazyNVRTC.cpp b/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
index 0e75c1842cb..88212f5f875 100644
--- a/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
+++ b/aten/src/ATen/cuda/detail/LazyNVRTC.cpp
@@ -12,6 +12,8 @@ namespace _stubs {
 at::DynamicLibrary& getCUDALibrary() {
 #if defined(_WIN32)
   static at::DynamicLibrary lib("nvcuda.dll");
+#elif defined(__APPLE__)
+  static at::DynamicLibrary lib("libcuda.dylib");
 #else
   static at::DynamicLibrary lib("libcuda.so.1");
 #endif
diff --git a/aten/src/ATen/cuda/jiterator_impl.h b/aten/src/ATen/cuda/jiterator_impl.h
index 5ba251055ad..94c85fdb0b4 100644
--- a/aten/src/ATen/cuda/jiterator_impl.h
+++ b/aten/src/ATen/cuda/jiterator_impl.h
@@ -244,7 +244,7 @@ private:
   StoreWithCastPtr v;
 };
 
-}} // namespace at::native
+}} // namespace at  native
 
 
 #endif // AT_USE_JITERATOR()
diff --git a/aten/src/ATen/cudnn/Handle.cpp b/aten/src/ATen/cudnn/Handle.cpp
index a6eb8fd7815..e952cd64629 100644
--- a/aten/src/ATen/cudnn/Handle.cpp
+++ b/aten/src/ATen/cudnn/Handle.cpp
@@ -49,4 +49,4 @@ cudnnHandle_t getCudnnHandle() {
   return handle;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/cudnn/Handle.h b/aten/src/ATen/cudnn/Handle.h
index f7463845581..52e6fd8a4ba 100644
--- a/aten/src/ATen/cudnn/Handle.h
+++ b/aten/src/ATen/cudnn/Handle.h
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:44:05
+ */
 #pragma once
 
 #include <ATen/cudnn/cudnn-wrapper.h>
@@ -6,4 +11,4 @@
 namespace at { namespace native {
 
 TORCH_CUDA_CPP_API cudnnHandle_t getCudnnHandle();
-}} // namespace at::native
+}} // namespace at native
diff --git a/aten/src/ATen/miopen/Exceptions.h b/aten/src/ATen/miopen/Exceptions.h
index f5f0a4785b1..6479d8c234e 100644
--- a/aten/src/ATen/miopen/Exceptions.h
+++ b/aten/src/ATen/miopen/Exceptions.h
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:44:05
+ */
 #pragma once
 
 #include <ATen/miopen/miopen-wrapper.h>
@@ -38,4 +43,4 @@ inline void HIP_CHECK(hipError_t error)
   }
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/miopen/Handle.cpp b/aten/src/ATen/miopen/Handle.cpp
index 6b8c7c6421c..a56b597239c 100644
--- a/aten/src/ATen/miopen/Handle.cpp
+++ b/aten/src/ATen/miopen/Handle.cpp
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:44:05
+ */
 #include <ATen/miopen/Exceptions.h>
 #include <ATen/miopen/Handle.h>
 #include <ATen/hip/detail/DeviceThreadHandles.h>
@@ -50,4 +55,4 @@ miopenHandle_t getMiopenHandle() {
   return handle;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/mkl/Exceptions.h b/aten/src/ATen/mkl/Exceptions.h
index a36d4841357..fb13690801b 100644
--- a/aten/src/ATen/mkl/Exceptions.h
+++ b/aten/src/ATen/mkl/Exceptions.h
@@ -17,7 +17,7 @@ static inline void MKL_DFTI_CHECK(MKL_INT status)
   }
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 namespace at {
 namespace mkl {
diff --git a/aten/src/ATen/native/Activation.cpp b/aten/src/ATen/native/Activation.cpp
index c845e6755f8..534ae977db8 100644
--- a/aten/src/ATen/native/Activation.cpp
+++ b/aten/src/ATen/native/Activation.cpp
@@ -829,4 +829,4 @@ Tensor& log_sigmoid_backward_cpu_out(const Tensor& grad_output,
 DEFINE_DISPATCH(GeluKernel);
 DEFINE_DISPATCH(GeluBackwardKernel);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/AdaptiveMaxPooling3d.cpp b/aten/src/ATen/native/AdaptiveMaxPooling3d.cpp
index 1c906c110eb..a740b89f540 100644
--- a/aten/src/ATen/native/AdaptiveMaxPooling3d.cpp
+++ b/aten/src/ATen/native/AdaptiveMaxPooling3d.cpp
@@ -145,7 +145,7 @@ static void adaptive_max_pool3d_single_out_frame(
                 for(iw = 0; iw < kW; iw++)
                 {
                   scalar_t val = *(ip + it*istrideT + ih*istrideH + iw*istrideW);
-                  if ((val > maxval) || std::isnan(val))
+                  if ((val > maxval) || std::isnan(double(val)))
                   {
                     maxval = val;
                     maxindex = (it+istartT)*isizeH*isizeW + (ih+istartH)*isizeW + (iw+istartW);
diff --git a/aten/src/ATen/native/AdaptivePooling.h b/aten/src/ATen/native/AdaptivePooling.h
index 829a780cf7b..2096489733e 100644
--- a/aten/src/ATen/native/AdaptivePooling.h
+++ b/aten/src/ATen/native/AdaptivePooling.h
@@ -38,4 +38,4 @@ static inline void adaptive_pool_empty_output_check(const Tensor& gradOutput_, c
   }
 }
 
-}} // namespace at::native
+}} // namespace at native
diff --git a/aten/src/ATen/native/AffineGridGenerator.cpp b/aten/src/ATen/native/AffineGridGenerator.cpp
index fe2c2d4aaa2..39451fbeddd 100644
--- a/aten/src/ATen/native/AffineGridGenerator.cpp
+++ b/aten/src/ATen/native/AffineGridGenerator.cpp
@@ -143,4 +143,4 @@ Tensor affine_grid_generator_backward(const Tensor& grad, IntArrayRef size, bool
   }
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/BatchLinearAlgebra.cpp b/aten/src/ATen/native/BatchLinearAlgebra.cpp
index 83613da6550..6d5f5d7180e 100644
--- a/aten/src/ATen/native/BatchLinearAlgebra.cpp
+++ b/aten/src/ATen/native/BatchLinearAlgebra.cpp
@@ -4035,4 +4035,4 @@ Tensor linalg_vander(
   auto ones =  result.new_ones(shape);
   return at::cat({std::move(ones), std::move(result)}, /*dim=*/ -1);
 }
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/BatchLinearAlgebra.h b/aten/src/ATen/native/BatchLinearAlgebra.h
index 955b83b3855..e240768921b 100644
--- a/aten/src/ATen/native/BatchLinearAlgebra.h
+++ b/aten/src/ATen/native/BatchLinearAlgebra.h
@@ -317,4 +317,4 @@ using ldl_solve_fn = void (*)(
     bool /*upper*/,
     bool /*hermitian*/);
 DECLARE_DISPATCH(ldl_solve_fn, ldl_solve_stub);
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp b/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp
index 8f36ae8c3fa..ccc89e214b9 100644
--- a/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp
+++ b/aten/src/ATen/native/BatchLinearAlgebraKernel.cpp
@@ -1224,4 +1224,4 @@ REGISTER_AVX512_DISPATCH(unpack_pivots_stub, &unpack_pivots_cpu_kernel);
 REGISTER_AVX2_DISPATCH(unpack_pivots_stub, &unpack_pivots_cpu_kernel);
 REGISTER_VSX_DISPATCH(unpack_pivots_stub, &unpack_pivots_cpu_kernel);
 REGISTER_ZVECTOR_DISPATCH(unpack_pivots_stub, &unpack_pivots_cpu_kernel);
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/BinaryOps.h b/aten/src/ATen/native/BinaryOps.h
index de1c9c8c0bb..d9e448c786c 100644
--- a/aten/src/ATen/native/BinaryOps.h
+++ b/aten/src/ATen/native/BinaryOps.h
@@ -114,4 +114,4 @@ DECLARE_DISPATCH(structured_binary_fn, shifted_chebyshev_polynomial_u_stub);
 DECLARE_DISPATCH(structured_binary_fn, shifted_chebyshev_polynomial_v_stub);
 DECLARE_DISPATCH(structured_binary_fn, shifted_chebyshev_polynomial_w_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Blas.cpp b/aten/src/ATen/native/Blas.cpp
index deda705d088..d4fa2f8d960 100644
--- a/aten/src/ATen/native/Blas.cpp
+++ b/aten/src/ATen/native/Blas.cpp
@@ -223,4 +223,4 @@ Tensor vdot(const Tensor &self, const Tensor &other){
 
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/BlasKernel.cpp b/aten/src/ATen/native/BlasKernel.cpp
index 87182b3514d..762cccedf0f 100644
--- a/aten/src/ATen/native/BlasKernel.cpp
+++ b/aten/src/ATen/native/BlasKernel.cpp
@@ -365,4 +365,4 @@ INSTANTIATE_VDOT_IMPL(c10::complex<double>);
 
 #undef INSTANTIATE_DOT_IMPL
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Bucketization.cpp b/aten/src/ATen/native/Bucketization.cpp
index 7b53a31c5be..1128c177b63 100644
--- a/aten/src/ATen/native/Bucketization.cpp
+++ b/aten/src/ATen/native/Bucketization.cpp
@@ -230,4 +230,4 @@ Tensor bucketize_cpu(const Scalar& self, const Tensor& boundaries, bool out_int3
   return bucketize_cpu(searchsorted_scalar_tensor(self, boundaries.device()), boundaries, out_int32, right);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/CPUBlas.cpp b/aten/src/ATen/native/CPUBlas.cpp
index b78e57fc63d..daf418b25ed 100644
--- a/aten/src/ATen/native/CPUBlas.cpp
+++ b/aten/src/ATen/native/CPUBlas.cpp
@@ -718,4 +718,4 @@ void copy(int64_t n, const c10::complex<float> *x, int64_t incx, c10::complex<fl
       n, x, incx, y, incy);
 }
 
-}}}  // namespace at::native::cpublas
+}}} // namespace at::native::cpublas
diff --git a/aten/src/ATen/native/CPUBlas.h b/aten/src/ATen/native/CPUBlas.h
index 969bfe2afd9..47f0d308a5f 100644
--- a/aten/src/ATen/native/CPUBlas.h
+++ b/aten/src/ATen/native/CPUBlas.h
@@ -161,4 +161,4 @@ void copy(int64_t n, const float *x, int64_t incx, float *y, int64_t incy);
 void copy(int64_t n, const c10::complex<double> *x, int64_t incx, c10::complex<double> *y, int64_t incy);
 void copy(int64_t n, const c10::complex<float> *x, int64_t incx, c10::complex<float> *y, int64_t incy);
 
-}}}  // namespace at::native::cpublas
+}}} // namespace at::native::cpublas
diff --git a/aten/src/ATen/native/ChanelShuffle.cpp b/aten/src/ATen/native/ChanelShuffle.cpp
index a4f9f2bfe86..f0373ef743c 100644
--- a/aten/src/ATen/native/ChanelShuffle.cpp
+++ b/aten/src/ATen/native/ChanelShuffle.cpp
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:49:32
+ */
 #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
 #include <ATen/NamedTensorUtils.h>
 #if defined(C10_MOBILE) && defined(USE_XNNPACK)
@@ -87,4 +92,4 @@ Tensor math_channel_shuffle(const Tensor& self, int64_t groups) {
 
 DEFINE_DISPATCH(channel_shuffle_kernel);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/ComparisonUtils.cpp b/aten/src/ATen/native/ComparisonUtils.cpp
index c16c361c344..17c8d8b9170 100644
--- a/aten/src/ATen/native/ComparisonUtils.cpp
+++ b/aten/src/ATen/native/ComparisonUtils.cpp
@@ -29,4 +29,4 @@ void _assert_tensor_metadata(at::Tensor const& tensor, at::OptionalIntArrayRef s
 }
 
 }
-}  // namespace at::native
+} // namespace at::native
diff --git a/aten/src/ATen/native/ComplexHelper.h b/aten/src/ATen/native/ComplexHelper.h
index 688b592c7d2..b7b20b030d2 100644
--- a/aten/src/ATen/native/ComplexHelper.h
+++ b/aten/src/ATen/native/ComplexHelper.h
@@ -94,4 +94,4 @@ Tensor view_as_complex(const Tensor& self) {
   return view_tensor(self, complex_type, new_storage_offset, new_sizes, new_strides);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/CompositeRandomAccessor.h b/aten/src/ATen/native/CompositeRandomAccessor.h
index a5984aa61d0..5b8947ed88a 100644
--- a/aten/src/ATen/native/CompositeRandomAccessor.h
+++ b/aten/src/ATen/native/CompositeRandomAccessor.h
@@ -31,4 +31,4 @@ auto get(references_holder<Values, References> rh) -> decltype(std::get<N>(rh.da
   return std::get<N>(rh.data());
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/CompositeRandomAccessorCommon.h b/aten/src/ATen/native/CompositeRandomAccessorCommon.h
index 01a031cdcc2..0364157a9f4 100644
--- a/aten/src/ATen/native/CompositeRandomAccessorCommon.h
+++ b/aten/src/ATen/native/CompositeRandomAccessorCommon.h
@@ -260,4 +260,4 @@ protected:
   ValueAccessor values;
 };
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/ConvUtils.h b/aten/src/ATen/native/ConvUtils.h
index 0d9a0a04962..33475ad84d1 100644
--- a/aten/src/ATen/native/ConvUtils.h
+++ b/aten/src/ATen/native/ConvUtils.h
@@ -402,4 +402,4 @@ static inline bool thnn_conv_use_channels_last(const at::Tensor& input, const at
   return can_use_thnn_channels_last_2d;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/ConvolutionMM3d.h b/aten/src/ATen/native/ConvolutionMM3d.h
index b87674672d1..59e87bbf693 100644
--- a/aten/src/ATen/native/ConvolutionMM3d.h
+++ b/aten/src/ATen/native/ConvolutionMM3d.h
@@ -12,4 +12,4 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv3d_backward_cpu(
     IntArrayRef padding,
     std::array<bool, 3> output_mask);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Cross.cpp b/aten/src/ATen/native/Cross.cpp
index 6c40001703c..8fb0506d48d 100644
--- a/aten/src/ATen/native/Cross.cpp
+++ b/aten/src/ATen/native/Cross.cpp
@@ -79,4 +79,4 @@ TORCH_IMPL_FUNC(linalg_cross_out)
   cross_stub(input.device().type(), out, input_broadcasted, other_broadcasted, dim);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Cross.h b/aten/src/ATen/native/Cross.h
index 9daee7f2d6c..629563fbc3d 100644
--- a/aten/src/ATen/native/Cross.h
+++ b/aten/src/ATen/native/Cross.h
@@ -11,4 +11,4 @@ using cross_fn = void(*)(const Tensor&, const Tensor&, const Tensor&, const int6
 
 DECLARE_DISPATCH(cross_fn, cross_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/DilatedMaxPool3d.cpp b/aten/src/ATen/native/DilatedMaxPool3d.cpp
index dcb1a09d379..d8d5f23a6c0 100644
--- a/aten/src/ATen/native/DilatedMaxPool3d.cpp
+++ b/aten/src/ATen/native/DilatedMaxPool3d.cpp
@@ -93,7 +93,7 @@ static void max_pool3d_with_indices_single_out_frame(
                 {
                   int64_t index = z * iwidth * iheight + y * iwidth + x;
                   scalar_t val = ip[index];
-                  if ((val > maxval) || std::isnan(val))
+                  if ((val > maxval) || std::isnan(double(val)))
                   {
                     maxval = val;
                     maxindex = index;
diff --git a/aten/src/ATen/native/DispatchStub.cpp b/aten/src/ATen/native/DispatchStub.cpp
index 52f73cfce43..65e63bb960a 100644
--- a/aten/src/ATen/native/DispatchStub.cpp
+++ b/aten/src/ATen/native/DispatchStub.cpp
@@ -188,4 +188,4 @@ void* DispatchStubImpl::choose_cpu_impl(
   return DEFAULT;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/DispatchStub.h b/aten/src/ATen/native/DispatchStub.h
index 9394442fe75..f2b58a29082 100644
--- a/aten/src/ATen/native/DispatchStub.h
+++ b/aten/src/ATen/native/DispatchStub.h
@@ -293,7 +293,7 @@ struct RegisterHIPDispatch {
 #endif
 
 
-}} // namespace at::native
+}} // namespace at  native
 
 
 #if defined(__clang__)
diff --git a/aten/src/ATen/native/Distance.cpp b/aten/src/ATen/native/Distance.cpp
index ad6cc0b6080..56b6c915b81 100644
--- a/aten/src/ATen/native/Distance.cpp
+++ b/aten/src/ATen/native/Distance.cpp
@@ -337,4 +337,4 @@ Tensor cosine_similarity(const Tensor& x1_, const Tensor& x2_, int64_t dim, doub
   return cos_sim_value;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/Distance.h b/aten/src/ATen/native/Distance.h
index c2d881ae66f..92c7446d48a 100644
--- a/aten/src/ATen/native/Distance.h
+++ b/aten/src/ATen/native/Distance.h
@@ -17,4 +17,4 @@ DECLARE_DISPATCH(pdist_backward_fn, pdist_backward_stub);
 DECLARE_DISPATCH(cdist_fn, cdist_stub);
 DECLARE_DISPATCH(cdist_backward_fn, cdist_backward_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Distributions.cpp b/aten/src/ATen/native/Distributions.cpp
index 52ee7eb715e..964e24556b9 100644
--- a/aten/src/ATen/native/Distributions.cpp
+++ b/aten/src/ATen/native/Distributions.cpp
@@ -666,4 +666,4 @@ Tensor multinomial(
   return result;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Embedding.cpp b/aten/src/ATen/native/Embedding.cpp
index ca19bf5059b..b583c67bf3b 100644
--- a/aten/src/ATen/native/Embedding.cpp
+++ b/aten/src/ATen/native/Embedding.cpp
@@ -212,4 +212,4 @@ Tensor & embedding_renorm_cpu_(
 }
 
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/EmbeddingBag.cpp b/aten/src/ATen/native/EmbeddingBag.cpp
index 6a0ee75d814..f772498908e 100644
--- a/aten/src/ATen/native/EmbeddingBag.cpp
+++ b/aten/src/ATen/native/EmbeddingBag.cpp
@@ -1787,4 +1787,4 @@ Tensor _embedding_bag_sparse_backward_symint(
                                     scale_grad_by_freq, true);
 }
 }
-} // namespace at::native
+} // namespace at  native
diff --git a/aten/src/ATen/native/Fill.h b/aten/src/ATen/native/Fill.h
index f6de9580ae7..8e65d84da90 100644
--- a/aten/src/ATen/native/Fill.h
+++ b/aten/src/ATen/native/Fill.h
@@ -18,4 +18,4 @@ DECLARE_DISPATCH(void(*)(TensorIterator&, const c10::Scalar&), fill_stub);
 
 Tensor& fill_out(Tensor& self, const Scalar& value);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/ForeachOpsKernels.cpp b/aten/src/ATen/native/ForeachOpsKernels.cpp
index bca5f3e6b38..9437b5830b5 100644
--- a/aten/src/ATen/native/ForeachOpsKernels.cpp
+++ b/aten/src/ATen/native/ForeachOpsKernels.cpp
@@ -321,4 +321,4 @@ std::vector<Tensor> foreach_tensor_norm_slow(TensorList tensors, const Scalar& o
   return result;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/FractionalMaxPool2d.cpp b/aten/src/ATen/native/FractionalMaxPool2d.cpp
index 1e9bf9c3902..02e40c1655c 100644
--- a/aten/src/ATen/native/FractionalMaxPool2d.cpp
+++ b/aten/src/ATen/native/FractionalMaxPool2d.cpp
@@ -175,7 +175,7 @@ static void fractional_max_pool2d_out_single_batch_frame(
 
               int planeIndex = h2 * inputW + w2;
               scalar_t val = inputForPlane[planeIndex];
-              if (val > maxVal || std::isnan(val)) {
+              if (val > maxVal || std::isnan(double(val))) {
                 maxVal = val;
                 maxIndex = planeIndex;
               }
diff --git a/aten/src/ATen/native/FractionalMaxPool3d.cpp b/aten/src/ATen/native/FractionalMaxPool3d.cpp
index c524f054547..ed993f2dc3d 100644
--- a/aten/src/ATen/native/FractionalMaxPool3d.cpp
+++ b/aten/src/ATen/native/FractionalMaxPool3d.cpp
@@ -156,7 +156,7 @@ static void fractional_max_pool3d_out_single_batch_frame(
 
                   int64_t planeIndex = t2 * inputH * inputW + h2 * inputW + w2;
                   scalar_t val = inputForPlane[planeIndex];
-                  if (val > maxVal || std::isnan(val)) {
+                  if (val > maxVal || std::isnan(double(val))) {
                     maxVal = val;
                     maxIndex = planeIndex;
                   }
diff --git a/aten/src/ATen/native/FunctionOfAMatrixUtils.cpp b/aten/src/ATen/native/FunctionOfAMatrixUtils.cpp
index 28abc812f4b..88a72a8179a 100644
--- a/aten/src/ATen/native/FunctionOfAMatrixUtils.cpp
+++ b/aten/src/ATen/native/FunctionOfAMatrixUtils.cpp
@@ -115,4 +115,4 @@ Tensor& _compute_linear_combination_out(const Tensor& input, const Tensor& coeff
   return output;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/FunctionOfAMatrixUtils.h b/aten/src/ATen/native/FunctionOfAMatrixUtils.h
index 68b26ed1381..ef78c87ebdf 100644
--- a/aten/src/ATen/native/FunctionOfAMatrixUtils.h
+++ b/aten/src/ATen/native/FunctionOfAMatrixUtils.h
@@ -17,4 +17,4 @@ using _compute_linear_combination_fn = void(*)(
 
 DECLARE_DISPATCH(_compute_linear_combination_fn, _compute_linear_combination_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/GridSampler.cpp b/aten/src/ATen/native/GridSampler.cpp
index 586c1cab40d..d0708aa4088 100644
--- a/aten/src/ATen/native/GridSampler.cpp
+++ b/aten/src/ATen/native/GridSampler.cpp
@@ -1054,4 +1054,4 @@ Tensor grid_sampler(
   }
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/GridSampler.h b/aten/src/ATen/native/GridSampler.h
index f4a73503243..01615c9b634 100644
--- a/aten/src/ATen/native/GridSampler.h
+++ b/aten/src/ATen/native/GridSampler.h
@@ -295,4 +295,4 @@ static inline void get_cubic_coefficients_grad(
   coeffs[3] = (3 * A * x - 10 * A) * x + 8 * A;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/GridSamplerUtils.h b/aten/src/ATen/native/GridSamplerUtils.h
index 7c22fedfe94..eb24282760d 100644
--- a/aten/src/ATen/native/GridSamplerUtils.h
+++ b/aten/src/ATen/native/GridSamplerUtils.h
@@ -106,4 +106,4 @@ bool cond_cudnn_grid_sampler(
 
 } // anonymous namespace
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Histogram.cpp b/aten/src/ATen/native/Histogram.cpp
index 89ede6bea35..1d229c59881 100644
--- a/aten/src/ATen/native/Histogram.cpp
+++ b/aten/src/ATen/native/Histogram.cpp
@@ -449,4 +449,4 @@ std::tuple<Tensor, std::vector<Tensor>> histogramdd(
   return at::native::histogramdd(self, bins_v, range, weight, density);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Histogram.h b/aten/src/ATen/native/Histogram.h
index 3305cc5e315..c1990cd22a6 100644
--- a/aten/src/ATen/native/Histogram.h
+++ b/aten/src/ATen/native/Histogram.h
@@ -11,4 +11,4 @@ using histogramdd_linear_fn = void(*)(const Tensor&, const c10::optional<Tensor>
 DECLARE_DISPATCH(histogramdd_fn, histogramdd_stub);
 DECLARE_DISPATCH(histogramdd_linear_fn, histogramdd_linear_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/IndexKernel.h b/aten/src/ATen/native/IndexKernel.h
index a54343d510a..c233b58526f 100644
--- a/aten/src/ATen/native/IndexKernel.h
+++ b/aten/src/ATen/native/IndexKernel.h
@@ -38,4 +38,4 @@ DECLARE_DISPATCH(masked_select_fn, masked_select_serial_stub);
 DECLARE_DISPATCH(masked_select_fn, masked_select_stub);
 DECLARE_DISPATCH(masked_scatter_fn, masked_scatter_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/IndexingUtils.cpp b/aten/src/ATen/native/IndexingUtils.cpp
index 2dba1972ce5..6b6a368d5cf 100644
--- a/aten/src/ATen/native/IndexingUtils.cpp
+++ b/aten/src/ATen/native/IndexingUtils.cpp
@@ -31,4 +31,4 @@ bool canUse32BitIndexMath(const TensorBase& t, int64_t max_elem) {
   return true;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Integration.cpp b/aten/src/ATen/native/Integration.cpp
index 09e444476d1..bc87d34729d 100644
--- a/aten/src/ATen/native/Integration.cpp
+++ b/aten/src/ATen/native/Integration.cpp
@@ -169,4 +169,4 @@ Tensor cumulative_trapezoid(const Tensor& y, const Scalar& dx, int64_t dim) {
     return do_cumulative_trapezoid(y, dx.toDouble(), dim);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Linear.cpp b/aten/src/ATen/native/Linear.cpp
index 6a1cabfa8b9..b0d1ada4abb 100644
--- a/aten/src/ATen/native/Linear.cpp
+++ b/aten/src/ATen/native/Linear.cpp
@@ -778,4 +778,4 @@ Tensor &tensordot_out(const Tensor& input1, const Tensor& input2, IntArrayRef di
   return result;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/LinearAlgebra.h b/aten/src/ATen/native/LinearAlgebra.h
index 304fbb8e684..0b052ef24ce 100644
--- a/aten/src/ATen/native/LinearAlgebra.h
+++ b/aten/src/ATen/native/LinearAlgebra.h
@@ -15,4 +15,4 @@ namespace at { namespace native {
 
 using addr_fn = void (*)(TensorIterator &, const Scalar& beta, const Scalar& alpha);
 DECLARE_DISPATCH(addr_fn, addr_stub);
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/LinearAlgebraUtils.h b/aten/src/ATen/native/LinearAlgebraUtils.h
index 00e79601dbf..1e1c40c34d2 100644
--- a/aten/src/ATen/native/LinearAlgebraUtils.h
+++ b/aten/src/ATen/native/LinearAlgebraUtils.h
@@ -621,4 +621,4 @@ static inline bool is_blas_compatible_row_major_order(const Tensor& input) {
       batch_stride_compatible;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/Loss.cpp b/aten/src/ATen/native/Loss.cpp
index 7f3d80212bc..f0b352808de 100644
--- a/aten/src/ATen/native/Loss.cpp
+++ b/aten/src/ATen/native/Loss.cpp
@@ -513,4 +513,4 @@ Tensor& mse_loss_backward_out(const Tensor& grad_output,
 Tensor l1_loss(const Tensor& input, const Tensor& target, int64_t reduction) {
   return apply_loss_reduction((input - target).abs(), reduction);
 }
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/LossMulti.h b/aten/src/ATen/native/LossMulti.h
index 148615e7e14..2d30e71bdf5 100644
--- a/aten/src/ATen/native/LossMulti.h
+++ b/aten/src/ATen/native/LossMulti.h
@@ -69,4 +69,4 @@ namespace {
 
 
 }  // anonymous namespace
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/NamedTensor.cpp b/aten/src/ATen/native/NamedTensor.cpp
index 6ee2f095b6d..54912a9c903 100644
--- a/aten/src/ATen/native/NamedTensor.cpp
+++ b/aten/src/ATen/native/NamedTensor.cpp
@@ -407,4 +407,4 @@ Tensor squeeze(const Tensor& self, Dimname dim) {
 }
 
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/NonEmptyUtils.h b/aten/src/ATen/native/NonEmptyUtils.h
index fdfded039aa..0ec5c23b95c 100644
--- a/aten/src/ATen/native/NonEmptyUtils.h
+++ b/aten/src/ATen/native/NonEmptyUtils.h
@@ -24,4 +24,4 @@ inline IdxVec ensure_nonempty_vec(IdxVec vec) {
   return vec;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/Normalization.h b/aten/src/ATen/native/Normalization.h
index 9500852799e..16652419d4b 100644
--- a/aten/src/ATen/native/Normalization.h
+++ b/aten/src/ATen/native/Normalization.h
@@ -9,4 +9,4 @@ namespace native {
 using renorm_scale_factor_fn = void (*) (TensorIteratorBase& iter, double maxnorm);
 DECLARE_DISPATCH(renorm_scale_factor_fn, renorm_scale_factor_stub);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/PackedSequence.cpp b/aten/src/ATen/native/PackedSequence.cpp
index d8f156f5a5a..805a0f6b028 100644
--- a/aten/src/ATen/native/PackedSequence.cpp
+++ b/aten/src/ATen/native/PackedSequence.cpp
@@ -237,4 +237,4 @@ Tensor pad_sequence(TensorList sequences, bool batch_first, double padding_value
   return out;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/PadNd.cpp b/aten/src/ATen/native/PadNd.cpp
index c3904ad514f..3e24e002280 100644
--- a/aten/src/ATen/native/PadNd.cpp
+++ b/aten/src/ATen/native/PadNd.cpp
@@ -231,4 +231,4 @@ Tensor pad_symint(const Tensor &self, c10::SymIntArrayRef pad, c10::string_view
   return at::native::_pad_enum_symint(self, pad, static_cast<int64_t>(mode_enum), value);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/PixelShuffle.cpp b/aten/src/ATen/native/PixelShuffle.cpp
index e535909a734..d2199623091 100644
--- a/aten/src/ATen/native/PixelShuffle.cpp
+++ b/aten/src/ATen/native/PixelShuffle.cpp
@@ -188,4 +188,4 @@ Tensor math_pixel_unshuffle(const Tensor& self, int64_t downscale_factor) {
 DEFINE_DISPATCH(pixel_shuffle_kernel);
 DEFINE_DISPATCH(pixel_unshuffle_kernel);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/RNN.cpp b/aten/src/ATen/native/RNN.cpp
index ef926b85c27..b74ceb816f1 100644
--- a/aten/src/ATen/native/RNN.cpp
+++ b/aten/src/ATen/native/RNN.cpp
@@ -1975,4 +1975,4 @@ TORCH_LIBRARY_IMPL(quantized, CatchAll, m) {
 }
 
 } // namespace
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/RNN.h b/aten/src/ATen/native/RNN.h
index d59622c0eec..f13bc21eac5 100644
--- a/aten/src/ATen/native/RNN.h
+++ b/aten/src/ATen/native/RNN.h
@@ -50,4 +50,4 @@ inline void check_attributes(const Tensor& input, const TensorList& params, cons
   for (const auto& p : params) check_tensors("parameter", p);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/RangeFactories.cpp b/aten/src/ATen/native/RangeFactories.cpp
index 408bf0a27e6..e287ceac450 100644
--- a/aten/src/ATen/native/RangeFactories.cpp
+++ b/aten/src/ATen/native/RangeFactories.cpp
@@ -221,4 +221,4 @@ Tensor& arange_out(const Scalar& start, const Scalar& end, const Scalar& step, T
 DEFINE_DISPATCH(arange_stub);
 DEFINE_DISPATCH(linspace_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/RangeFactories.h b/aten/src/ATen/native/RangeFactories.h
index df3b43856e0..564a65278a6 100644
--- a/aten/src/ATen/native/RangeFactories.h
+++ b/aten/src/ATen/native/RangeFactories.h
@@ -9,4 +9,4 @@ namespace native {
 DECLARE_DISPATCH(void(*)(TensorIterator&, const Scalar&, const Scalar&, const Scalar&), arange_stub);
 DECLARE_DISPATCH(void(*)(TensorIterator&, const Scalar&, const Scalar&, int64_t), linspace_stub);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/ReduceAllOps.cpp b/aten/src/ATen/native/ReduceAllOps.cpp
index e1d51a1666a..288538036e4 100644
--- a/aten/src/ATen/native/ReduceAllOps.cpp
+++ b/aten/src/ATen/native/ReduceAllOps.cpp
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:49:32
+ */
 #define TORCH_ASSERT_ONLY_METHOD_OPERATORS
 #include <ATen/native/ReduceAllOps.h>
 #include <ATen/native/Resize.h>
@@ -67,4 +72,4 @@ std::tuple<Tensor, Tensor> _aminmax_all(const Tensor &self) {
   return at::aminmax(self);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/ReduceOps.cpp b/aten/src/ATen/native/ReduceOps.cpp
index 91bf3985617..2562942d616 100644
--- a/aten/src/ATen/native/ReduceOps.cpp
+++ b/aten/src/ATen/native/ReduceOps.cpp
@@ -757,6 +757,11 @@ template<typename T>
 inline typename std::enable_if<!std::is_integral<T>::value, bool>::type isnan_(T x) {
   return std::isnan(x);
 }
+#elif __APPLE__
+//template<typename T>
+inline bool isnan_(double x) {
+  return std::isnan(x);
+}
 #else
 template<typename T>
 inline bool isnan_(T x) {
diff --git a/aten/src/ATen/native/ReduceOps.h b/aten/src/ATen/native/ReduceOps.h
index c14033de634..ac1c4920499 100644
--- a/aten/src/ATen/native/ReduceOps.h
+++ b/aten/src/ATen/native/ReduceOps.h
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:44:12
+ */
 #pragma once
 
 #include <ATen/native/DispatchStub.h>
@@ -52,4 +57,4 @@ TORCH_API std::tuple<Tensor&,Tensor&> var_mean_out(
     Tensor &result1, Tensor &result2, const Tensor &self, IntArrayRef dim,
     int64_t correction, bool keepdim);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/RowwisePrune.cpp b/aten/src/ATen/native/RowwisePrune.cpp
index c27707c4d30..df2e61bafad 100644
--- a/aten/src/ATen/native/RowwisePrune.cpp
+++ b/aten/src/ATen/native/RowwisePrune.cpp
@@ -113,4 +113,4 @@ std::tuple<Tensor, Tensor> _rowwise_prune(const Tensor& weights,
                                         compressed_indices_dtype);
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/ScatterGatherChecks.h b/aten/src/ATen/native/ScatterGatherChecks.h
index dea0ca55fd5..b6927a88925 100644
--- a/aten/src/ATen/native/ScatterGatherChecks.h
+++ b/aten/src/ATen/native/ScatterGatherChecks.h
@@ -125,4 +125,4 @@ static C10_UNUSED void scatter_shape_check(
 
 } // anonymous namespace
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/SharedReduceOps.h b/aten/src/ATen/native/SharedReduceOps.h
index 20b1911156c..2b806ae321c 100644
--- a/aten/src/ATen/native/SharedReduceOps.h
+++ b/aten/src/ATen/native/SharedReduceOps.h
@@ -539,7 +539,7 @@ struct MinMaxOps {
 #endif
 };
 
-}} // namespace at::native
+}} // namespace at  native
 
 #undef MAX
 #undef MIN
diff --git a/aten/src/ATen/native/StridedRandomAccessor.h b/aten/src/ATen/native/StridedRandomAccessor.h
index bb7b2155cd3..a41253a3d38 100644
--- a/aten/src/ATen/native/StridedRandomAccessor.h
+++ b/aten/src/ATen/native/StridedRandomAccessor.h
@@ -298,4 +298,4 @@ public:
   // }
 };
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/SummaryOps.cpp b/aten/src/ATen/native/SummaryOps.cpp
index ae0b38c96ef..22737d67920 100644
--- a/aten/src/ATen/native/SummaryOps.cpp
+++ b/aten/src/ATen/native/SummaryOps.cpp
@@ -82,4 +82,4 @@ _bincount_cpu(const Tensor& self, const c10::optional<Tensor>& weights_opt, int6
   });
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/TensorAdvancedIndexing.h b/aten/src/ATen/native/TensorAdvancedIndexing.h
index bfa85fb6221..03515c1c9ff 100644
--- a/aten/src/ATen/native/TensorAdvancedIndexing.h
+++ b/aten/src/ATen/native/TensorAdvancedIndexing.h
@@ -101,4 +101,4 @@ DECLARE_DISPATCH(scatter_add_expanded_index_fn, scatter_add_expanded_index_stub)
 DECLARE_DISPATCH(scatter_reduce_expanded_index_fn, scatter_reduce_expanded_index_stub);
 DECLARE_DISPATCH(gather_expanded_index_fn, gather_expanded_index_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/TensorCompare.h b/aten/src/ATen/native/TensorCompare.h
index f35cd68d480..ac6d3f78a1a 100644
--- a/aten/src/ATen/native/TensorCompare.h
+++ b/aten/src/ATen/native/TensorCompare.h
@@ -46,4 +46,4 @@ DECLARE_DISPATCH(void (*)(TensorIteratorBase &, c10::Scalar), clamp_max_scalar_s
 using isin_default_fn = void (*)(const Tensor&, const Tensor&, bool, const Tensor&);
 DECLARE_DISPATCH(isin_default_fn, isin_default_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/TensorIteratorDynamicCasting.h b/aten/src/ATen/native/TensorIteratorDynamicCasting.h
index 7cf57791230..63232177787 100644
--- a/aten/src/ATen/native/TensorIteratorDynamicCasting.h
+++ b/aten/src/ATen/native/TensorIteratorDynamicCasting.h
@@ -52,4 +52,4 @@ struct needs_dynamic_casting<func_t, 0> {
   }
 };
 
-}} //namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/TensorProperties.h b/aten/src/ATen/native/TensorProperties.h
index fe6e8395c17..2403e4f790d 100644
--- a/aten/src/ATen/native/TensorProperties.h
+++ b/aten/src/ATen/native/TensorProperties.h
@@ -1,3 +1,8 @@
+/*
+ * @Descripttion: C++ Practice
+ * @Author: TomHeaven
+ * @Date: 2023-03-18 01:44:05
+ */
 #pragma once
 
 // See NOTE: [Tensor vs. TensorBase]
@@ -9,4 +14,4 @@ namespace at { namespace native {
 
 TORCH_API bool cudnn_is_acceptable(const TensorBase& self);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/TensorShape.h b/aten/src/ATen/native/TensorShape.h
index 69363ff5c49..74ebaa692ce 100644
--- a/aten/src/ATen/native/TensorShape.h
+++ b/aten/src/ATen/native/TensorShape.h
@@ -56,4 +56,4 @@ inline int64_t get_num_splits(const Tensor& self, int64_t split_size, int64_t di
   return num_splits;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/TensorTransformations.cpp b/aten/src/ATen/native/TensorTransformations.cpp
index 768fb56b6de..46687fad519 100644
--- a/aten/src/ATen/native/TensorTransformations.cpp
+++ b/aten/src/ATen/native/TensorTransformations.cpp
@@ -236,4 +236,4 @@ Tensor chalf(const Tensor& self, c10::optional<MemoryFormat> memory_format) {
 
 DEFINE_DISPATCH(flip_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/TensorTransformations.h b/aten/src/ATen/native/TensorTransformations.h
index f17c96c7bdb..5a780330bb6 100644
--- a/aten/src/ATen/native/TensorTransformations.h
+++ b/aten/src/ATen/native/TensorTransformations.h
@@ -28,4 +28,4 @@ static inline Tensor roll_common(const Tensor& self, IntArrayRef shifts, IntArra
   return at::roll(first_dim_rolled, tail_shifts, tail_dims);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/TypeProperties.cpp b/aten/src/ATen/native/TypeProperties.cpp
index 6e1768fcf47..b36101d558f 100644
--- a/aten/src/ATen/native/TypeProperties.cpp
+++ b/aten/src/ATen/native/TypeProperties.cpp
@@ -201,4 +201,4 @@ ScalarType promote_types(ScalarType type1, ScalarType type2) {
   return ret;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/UnaryOps.h b/aten/src/ATen/native/UnaryOps.h
index 103e522fa35..1c2159a14bf 100644
--- a/aten/src/ATen/native/UnaryOps.h
+++ b/aten/src/ATen/native/UnaryOps.h
@@ -118,4 +118,4 @@ DECLARE_DISPATCH(void (*)(TensorIteratorBase&, int64_t), round_decimals_stub);
 // clone
 // contiguous
 // zero
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/Unfold2d.h b/aten/src/ATen/native/Unfold2d.h
index 2ea27e0cade..930e4fd89a6 100644
--- a/aten/src/ATen/native/Unfold2d.h
+++ b/aten/src/ATen/native/Unfold2d.h
@@ -27,4 +27,4 @@ using unfold2d_fn = void (*)(
 DECLARE_DISPATCH(unfold2d_fn, unfolded2d_copy_stub);
 DECLARE_DISPATCH(unfold2d_fn, unfolded2d_acc_stub);
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/UnfoldBackward.cpp b/aten/src/ATen/native/UnfoldBackward.cpp
index 49414323211..5a490c1e9d2 100644
--- a/aten/src/ATen/native/UnfoldBackward.cpp
+++ b/aten/src/ATen/native/UnfoldBackward.cpp
@@ -38,4 +38,4 @@ Tensor unfold_backward(
   return grad_input;
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/UnfoldBackward.h b/aten/src/ATen/native/UnfoldBackward.h
index f8099167361..fa639492f7a 100644
--- a/aten/src/ATen/native/UnfoldBackward.h
+++ b/aten/src/ATen/native/UnfoldBackward.h
@@ -109,4 +109,4 @@ static C10_UNUSED TensorIterator _make_unfold_backward_iter_over_grad_out(
 
 }
 
-}} // namespace at::native
+}} // namespace at  native
diff --git a/aten/src/ATen/native/cpu/Activation.cpp b/aten/src/ATen/native/cpu/Activation.cpp
index f679cafb67d..064410bfd7c 100644
--- a/aten/src/ATen/native/cpu/Activation.cpp
+++ b/aten/src/ATen/native/cpu/Activation.cpp
@@ -19,7 +19,8 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -1402,4 +1403,5 @@ REGISTER_DISPATCH(mish_backward_stub, &mish_backward_kernel);
 REGISTER_DISPATCH(prelu_stub, &prelu_kernel);
 REGISTER_DISPATCH(prelu_backward_stub, &prelu_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp b/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp
index b1069d2cc2a..500c0a9a49d 100644
--- a/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AdaptiveAvgPoolKernel.cpp
@@ -8,7 +8,8 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -413,4 +414,5 @@ void adapative_avg_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(adaptive_avg_pool2d_kernel, &adaptive_avg_pool2d_kernel_impl);
 REGISTER_DISPATCH(adaptive_avg_pool2d_backward_kernel, &adapative_avg_pool2d_backward_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp b/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
index 2464a6a1441..a4ac41001b8 100644
--- a/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AdaptiveMaxPoolKernel.cpp
@@ -8,7 +8,8 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -56,7 +57,7 @@ void cpu_adaptive_max_pool(
             for (int64_t iw = iw0; iw < iw1; iw ++) {
               int64_t index = ih * input_width + iw;
               scalar_t val = input_ptr[index];
-              if ((val > maxval) || std::isnan(val)) {
+              if ((val > maxval) || std::isnan(double(val))) {
                 maxval = val;
                 maxindex = index;
               }
@@ -173,7 +174,7 @@ void cpu_adaptive_max_pool_channels_last(
             int64_t maxindex = ind[d2];
             scalar_t maxval = out[d2];
 
-            bool mask = (val > maxval) || std::isnan(val);
+            bool mask = (val > maxval) || std::isnan(double(val));
             out[d2] = mask ? val : maxval;
             ind[d2] = mask ? index : maxindex;
           }
@@ -302,7 +303,7 @@ void cpu_adaptive_max_pool_channels_last<BFloat16>(
             int64_t maxindex = ind[d2];
             float maxval = max[d2];
 
-            bool mask = (val > maxval) || std::isnan(val);
+            bool mask = (val > maxval) || std::isnan(double(val));
             max[d2] = mask ? val : maxval;
             ind[d2] = mask ? index : maxindex;
           }
@@ -485,4 +486,5 @@ void adaptive_max_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(adaptive_max_pool2d_kernel, &adaptive_max_pool2d_kernel_impl);
 REGISTER_DISPATCH(adaptive_max_pool2d_backward_kernel, &adaptive_max_pool2d_backward_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/AvgPoolKernel.cpp b/aten/src/ATen/native/cpu/AvgPoolKernel.cpp
index 7a28b977f9d..ebe783c1edd 100644
--- a/aten/src/ATen/native/cpu/AvgPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/AvgPoolKernel.cpp
@@ -7,7 +7,8 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -549,4 +550,5 @@ void avg_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(avg_pool2d_kernel, &avg_pool2d_kernel_impl);
 REGISTER_DISPATCH(avg_pool2d_backward_kernel, &avg_pool2d_backward_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp b/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
index a9e8cf2243f..40a64a279d7 100644
--- a/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/BinaryOpsKernel.cpp
@@ -15,7 +15,8 @@
 #include <c10/util/TypeSafeSignMath.h>
 #include <c10/util/copysign.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -1270,4 +1271,5 @@ REGISTER_DISPATCH(shifted_chebyshev_polynomial_u_stub, &shifted_chebyshev_polyno
 REGISTER_DISPATCH(shifted_chebyshev_polynomial_v_stub, &shifted_chebyshev_polynomial_v_kernel);
 REGISTER_DISPATCH(shifted_chebyshev_polynomial_w_stub, &shifted_chebyshev_polynomial_w_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/BlasKernel.cpp b/aten/src/ATen/native/cpu/BlasKernel.cpp
index e143d8a6546..6ec4d084856 100644
--- a/aten/src/ATen/native/cpu/BlasKernel.cpp
+++ b/aten/src/ATen/native/cpu/BlasKernel.cpp
@@ -4,7 +4,8 @@
 #include <c10/util/irange.h>
 #include <c10/util/Unroll.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace cpublas {
 namespace {
 
@@ -247,4 +248,5 @@ REGISTER_DISPATCH(cpublas::gemm_stub, &cpublas::cpublas_gemm_impl);
 REGISTER_DISPATCH(cpublas::axpy_stub, &cpublas::cpublas_axpy_impl);
 REGISTER_DISPATCH(cpublas::copy_stub, &cpublas::cpublas_copy_impl);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/CatKernel.cpp b/aten/src/ATen/native/cpu/CatKernel.cpp
index 3641b02fbbe..bb460c15f67 100644
--- a/aten/src/ATen/native/cpu/CatKernel.cpp
+++ b/aten/src/ATen/native/cpu/CatKernel.cpp
@@ -7,7 +7,8 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -65,4 +66,5 @@ void cat_serial_kernel(const Tensor& result, const MaterializedITensorListRef& t
 
 REGISTER_DISPATCH(cat_serial_stub, &cat_serial_kernel);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/CatKernel.h b/aten/src/ATen/native/cpu/CatKernel.h
index aedb4aec4f5..ece5d58de54 100644
--- a/aten/src/ATen/native/cpu/CatKernel.h
+++ b/aten/src/ATen/native/cpu/CatKernel.h
@@ -9,4 +9,4 @@ namespace at { namespace native {
 using cat_serial_fn = void(*)(const Tensor &, const MaterializedITensorListRef&, int64_t);
 DECLARE_DISPATCH(cat_serial_fn, cat_serial_stub);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp b/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp
index d2494970c9b..5cf8653b653 100644
--- a/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp
+++ b/aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp
@@ -8,7 +8,8 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -113,4 +114,5 @@ void channel_shuffle_kernel_impl(
 
 REGISTER_DISPATCH(channel_shuffle_kernel, &channel_shuffle_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/ComplexKernel.cpp b/aten/src/ATen/native/cpu/ComplexKernel.cpp
index e9e72ff758d..99dc6134537 100644
--- a/aten/src/ATen/native/cpu/ComplexKernel.cpp
+++ b/aten/src/ATen/native/cpu/ComplexKernel.cpp
@@ -4,7 +4,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void complex_kernel(TensorIterator& iter) {
@@ -28,4 +29,5 @@ void polar_kernel(TensorIterator& iter) {
 REGISTER_DISPATCH(complex_stub, &complex_kernel);
 REGISTER_DISPATCH(polar_stub, &polar_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/CopyKernel.cpp b/aten/src/ATen/native/cpu/CopyKernel.cpp
index adfb550b2a6..c6411efd77c 100644
--- a/aten/src/ATen/native/cpu/CopyKernel.cpp
+++ b/aten/src/ATen/native/cpu/CopyKernel.cpp
@@ -8,7 +8,8 @@
 #include <ATen/TensorIteratorInternal.h>
 #include <ATen/Parallel.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 inline namespace CPU_CAPABILITY {
 void neg_kernel(TensorIteratorBase &iter);
 void conj_kernel(TensorIteratorBase &iter);
@@ -270,4 +271,5 @@ void copy_kernel(TensorIterator& iter, bool /*non_blocking*/) {
 
 REGISTER_DISPATCH(copy_stub, &copy_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/CopyKernel.h b/aten/src/ATen/native/cpu/CopyKernel.h
index 9d2affd6101..297dab742cb 100644
--- a/aten/src/ATen/native/cpu/CopyKernel.h
+++ b/aten/src/ATen/native/cpu/CopyKernel.h
@@ -9,4 +9,4 @@ inline namespace CPU_CAPABILITY {
 void direct_copy_kernel(TensorIteratorBase &iter);
 void copy_kernel(TensorIterator& iter, bool /*non_blocking*/);
 
-}}}  // namespace at::native::CPU_CAPABILITY
+}}} // namespace at::native::CPU_CAPABILITY
diff --git a/aten/src/ATen/native/cpu/CrossKernel.cpp b/aten/src/ATen/native/cpu/CrossKernel.cpp
index 97249807bb8..f75483c8209 100644
--- a/aten/src/ATen/native/cpu/CrossKernel.cpp
+++ b/aten/src/ATen/native/cpu/CrossKernel.cpp
@@ -11,7 +11,8 @@
 #include <ATen/Parallel.h>
 #include <ATen/TensorIterator.h>
 #include <c10/util/irange.h>
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template<typename scalar_t>
@@ -78,4 +79,5 @@ static void cross_kernel_impl(const Tensor& result, const Tensor& a, const Tenso
 
 REGISTER_DISPATCH(cross_stub, &cross_kernel_impl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp b/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp
index 8688bd688b0..a21ff72cf88 100644
--- a/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp
+++ b/aten/src/ATen/native/cpu/DepthwiseConvKernel.cpp
@@ -15,7 +15,8 @@
 #include <arm_neon.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 struct Arguments final {
@@ -312,4 +313,5 @@ Tensor _convolution_depthwise3x3_winograd(
 
 REGISTER_DISPATCH(convolution_depthwise3x3_winograd_stub, &_convolution_depthwise3x3_winograd);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp b/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp
index f2346759cbc..0f0d6e09781 100644
--- a/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp
@@ -10,7 +10,8 @@
 #include <ATen/cpu/vec/functional.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template<typename scalar_t>
@@ -448,4 +449,5 @@ REGISTER_DISPATCH(pdist_backward_stub, &pdist_backward_kernel_impl);
 REGISTER_DISPATCH(cdist_stub, &cdist_kernel_impl);
 REGISTER_DISPATCH(cdist_backward_stub, &cdist_backward_kernel_impl);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/DistributionKernels.cpp b/aten/src/ATen/native/cpu/DistributionKernels.cpp
index 5b9d844b7a3..e52569fba7f 100644
--- a/aten/src/ATen/native/cpu/DistributionKernels.cpp
+++ b/aten/src/ATen/native/cpu/DistributionKernels.cpp
@@ -23,7 +23,8 @@
 #include <cpuinfo.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 static void cauchy_kernel(TensorIteratorBase& iter, double median, double sigma, c10::optional<Generator> gen) {
@@ -239,4 +240,5 @@ REGISTER_DISPATCH(random_from_to_stub, &random_from_to_kernel);
 REGISTER_DISPATCH(random_full_64_bits_range_stub, &random_full_64_bits_range_kernel);
 REGISTER_DISPATCH(random_stub, &random_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/FillKernel.cpp b/aten/src/ATen/native/cpu/FillKernel.cpp
index a04ffdbf904..c915f7adc2a 100644
--- a/aten/src/ATen/native/cpu/FillKernel.cpp
+++ b/aten/src/ATen/native/cpu/FillKernel.cpp
@@ -9,7 +9,8 @@
 #include <ATen/native/Fill.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 
@@ -58,4 +59,5 @@ void fill_kernel(TensorIterator& iter, const Scalar& value_scalar) {
 
 REGISTER_DISPATCH(fill_stub, &fill_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp b/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp
index 92cf41c309e..287de18a1dd 100644
--- a/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp
+++ b/aten/src/ATen/native/cpu/FunctionOfAMatrixUtilsKernel.cpp
@@ -11,7 +11,8 @@
 #define RESTRICT __restrict__
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -54,4 +55,5 @@ void _compute_linear_combination_cpu_kernel(
 
 REGISTER_DISPATCH(_compute_linear_combination_stub, &_compute_linear_combination_cpu_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/GridSamplerKernel.cpp b/aten/src/ATen/native/cpu/GridSamplerKernel.cpp
index c65bdfcd6d6..b59d669b47d 100644
--- a/aten/src/ATen/native/cpu/GridSamplerKernel.cpp
+++ b/aten/src/ATen/native/cpu/GridSamplerKernel.cpp
@@ -14,7 +14,8 @@
 #include <cstring>
 #include <type_traits>
 
-namespace at::native { namespace {
+namespace at {
+namespace native { namespace {
 
 /**  NOTE [ Grid Sample CPU Kernels ]
  *
@@ -1319,4 +1320,5 @@ REGISTER_DISPATCH(grid_sampler_2d_cpu_kernel, &grid_sampler_2d_cpu_kernel_impl);
 REGISTER_DISPATCH(grid_sampler_2d_backward_cpu_kernel, &grid_sampler_2d_backward_cpu_kernel_impl);
 
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/GridSamplerKernel.h b/aten/src/ATen/native/cpu/GridSamplerKernel.h
index b1830fcd391..d5dc111bb76 100644
--- a/aten/src/ATen/native/cpu/GridSamplerKernel.h
+++ b/aten/src/ATen/native/cpu/GridSamplerKernel.h
@@ -31,4 +31,4 @@ using backward_2d_fn = void (*) (
 DECLARE_DISPATCH(forward_2d_fn, grid_sampler_2d_cpu_kernel);
 DECLARE_DISPATCH(backward_2d_fn, grid_sampler_2d_backward_cpu_kernel);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/HistogramKernel.cpp b/aten/src/ATen/native/cpu/HistogramKernel.cpp
index e640aef8dde..948aa4f9d92 100644
--- a/aten/src/ATen/native/cpu/HistogramKernel.cpp
+++ b/aten/src/ATen/native/cpu/HistogramKernel.cpp
@@ -19,7 +19,8 @@
 #include <numeric>
 #include <functional>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -288,4 +289,5 @@ REGISTER_DISPATCH(histogramdd_stub, &histogramdd_kernel_impl);
 
 REGISTER_DISPATCH(histogramdd_linear_stub, &histogramdd_linear_kernel_impl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/IndexKernel.cpp b/aten/src/ATen/native/cpu/IndexKernel.cpp
index 7ac9c3ff607..65b97613d1c 100644
--- a/aten/src/ATen/native/cpu/IndexKernel.cpp
+++ b/aten/src/ATen/native/cpu/IndexKernel.cpp
@@ -15,7 +15,8 @@
 #include <c10/util/irange.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using namespace vec;
@@ -797,4 +798,5 @@ REGISTER_DISPATCH(masked_select_stub, &masked_select_kernel);
 REGISTER_DISPATCH(masked_scatter_stub, &masked_scatter_kernel);
 REGISTER_DISPATCH(flip_stub, &flip_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp b/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp
index f93394bb5eb..6603b85c70a 100644
--- a/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp
+++ b/aten/src/ATen/native/cpu/LinearAlgebraKernel.cpp
@@ -8,7 +8,8 @@
 #include <ATen/native/cpu/Loops.h>
 #include <c10/util/irange.h>
 
-namespace at::native { namespace {
+namespace at {
+namespace native { namespace {
 
 void addr_kernel(TensorIterator &iter,
                  const Scalar& beta, const Scalar& alpha) {
@@ -86,4 +87,5 @@ void addr_kernel(TensorIterator &iter,
 } // anonymous namespace
 
 REGISTER_DISPATCH(addr_stub, &addr_kernel);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/Loops.h b/aten/src/ATen/native/cpu/Loops.h
index 2ca00add995..c769a25a08b 100644
--- a/aten/src/ATen/native/cpu/Loops.h
+++ b/aten/src/ATen/native/cpu/Loops.h
@@ -391,4 +391,4 @@ void cpu_serial_kernel_vec(TensorIteratorBase& iter, func_t&& op, vec_func_t&& v
   cpu_serial_kernel_vec(iter, op, vop, {0, iter.numel()});
 }
 
-}}}  // namespace at::native::<anonymous>
+}}} // namespace at::native::<anonymous>
diff --git a/aten/src/ATen/native/cpu/MaxPoolKernel.cpp b/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
index 90d0993ea03..cdbcecb3f48 100644
--- a/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/MaxPoolKernel.cpp
@@ -9,7 +9,8 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -64,7 +65,7 @@ void cpu_max_pool(
         for (int64_t iw = iw0; iw < iw1; iw += dilationW) {
           int64_t index = ih * input_width + iw;
           accscalar_t val = accscalar_t(input_ptr[index]);
-          if ((val > maxval) || std::isnan(val)) {
+          if ((val > maxval) || std::isnan(double(val))) {
             maxval = val;
             maxindex = index;
           }
@@ -187,7 +188,7 @@ void cpu_max_pool_channels_last(
             int64_t maxindex = ind[d2];
             scalar_t maxval = out[d2];
 
-            bool mask = (val > maxval) || std::isnan(val);
+            bool mask = (val > maxval) || std::isnan(double(val));
             out[d2] = mask ? val : maxval;
             ind[d2] = mask ? index : maxindex;
           }
@@ -320,7 +321,7 @@ void cpu_max_pool_channels_last<BFloat16>(
             int64_t maxindex = ind[d2];
             float maxval = max[d2];
 
-            bool mask = (val > maxval) || std::isnan(val);
+            bool mask = (val > maxval) || std::isnan(double(val));
             max[d2] = mask ? val : maxval;
             ind[d2] = mask ? index : maxindex;
           }
@@ -508,4 +509,5 @@ void max_pool2d_backward_kernel_impl(
 REGISTER_DISPATCH(max_pool2d_kernel, &max_pool2d_kernel_impl);
 REGISTER_DISPATCH(max_pool2d_backward_kernel, &max_pool2d_backward_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/MaxPooling.cpp b/aten/src/ATen/native/cpu/MaxPooling.cpp
index 1f3029456ed..32460db237c 100644
--- a/aten/src/ATen/native/cpu/MaxPooling.cpp
+++ b/aten/src/ATen/native/cpu/MaxPooling.cpp
@@ -6,7 +6,8 @@
 #include <ATen/native/MaxPooling.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -21,7 +22,7 @@ inline void max_pool1d_kernel(
     int64_t ij = p.index(kj, oj);
     for (; oj < oe; ++oj, ij += p.SJ) {
       scalar_t val = ip[ij];
-      bool update_max = std::isnan(val) || op[oj] < val;
+      bool update_max = std::isnan(double(val)) || op[oj] < val;
       op[oj] = update_max ? val : op[oj];
     }
   }
@@ -56,4 +57,5 @@ void max_pool1d_impl(
 
 REGISTER_DISPATCH(max_pool1d_stub, &max_pool1d_impl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp b/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
index 23148589808..37517860150 100644
--- a/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
+++ b/aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp
@@ -9,7 +9,8 @@
 
 #include <c10/util/Optional.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -269,4 +270,5 @@ void max_unpool3d_kernel_impl(
 REGISTER_DISPATCH(max_unpool2d_kernel, &max_unpool2d_kernel_impl);
 REGISTER_DISPATCH(max_unpool3d_kernel, &max_unpool3d_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/MultinomialKernel.cpp b/aten/src/ATen/native/cpu/MultinomialKernel.cpp
index e92b0c73d2b..feda5fe7b3b 100644
--- a/aten/src/ATen/native/cpu/MultinomialKernel.cpp
+++ b/aten/src/ATen/native/cpu/MultinomialKernel.cpp
@@ -15,7 +15,8 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template <typename scalar_t>
@@ -240,4 +241,5 @@ static void multinomial_with_replacement_kernel_impl(
 REGISTER_DISPATCH(
     multinomial_with_replacement_stub,
     &multinomial_with_replacement_kernel_impl);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp b/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp
index 263596faa69..7383236c77f 100644
--- a/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp
+++ b/aten/src/ATen/native/cpu/PixelShuffleKernel.cpp
@@ -8,7 +8,8 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -250,4 +251,5 @@ void pixel_unshuffle_kernel_impl(
 REGISTER_DISPATCH(pixel_shuffle_kernel, &pixel_shuffle_kernel_impl);
 REGISTER_DISPATCH(pixel_unshuffle_kernel, &pixel_unshuffle_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp b/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp
index 4d50fff6f7a..602f8cd41b3 100644
--- a/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp
@@ -6,7 +6,8 @@
 #include <ATen/native/cpu/Loops.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 static void addcmul_cpu_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -241,4 +242,5 @@ REGISTER_DISPATCH(smooth_l1_backward_stub, &smooth_l1_backward_cpu_kernel);
 REGISTER_DISPATCH(huber_backward_stub, &huber_backward_cpu_kernel);
 REGISTER_DISPATCH(mse_backward_stub, &mse_backward_cpu_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/PowKernel.cpp b/aten/src/ATen/native/cpu/PowKernel.cpp
index 74dc944a0f0..0582f5ffacf 100644
--- a/aten/src/ATen/native/cpu/PowKernel.cpp
+++ b/aten/src/ATen/native/cpu/PowKernel.cpp
@@ -9,7 +9,8 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 inline namespace CPU_CAPABILITY {
 
@@ -151,4 +152,5 @@ void pow_tensor_scalar_kernel(
 REGISTER_DISPATCH(pow_tensor_tensor_stub, &CPU_CAPABILITY::pow_tensor_tensor_kernel);
 REGISTER_DISPATCH(pow_tensor_scalar_stub, &CPU_CAPABILITY::pow_tensor_scalar_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp b/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp
index 28adc7040cf..26751339534 100644
--- a/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp
+++ b/aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp
@@ -13,7 +13,8 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using namespace vec;
@@ -74,4 +75,5 @@ static void linspace_kernel(TensorIterator& iter, const Scalar& scalar_start, co
 REGISTER_DISPATCH(arange_stub, &arange_kernel);
 REGISTER_DISPATCH(linspace_stub, &linspace_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/Reduce.h b/aten/src/ATen/native/cpu/Reduce.h
index fdb1c0d1a0f..9c7ac5b8431 100644
--- a/aten/src/ATen/native/cpu/Reduce.h
+++ b/aten/src/ATen/native/cpu/Reduce.h
@@ -310,4 +310,4 @@ void binary_kernel_reduce_lastdim(TensorIteratorBase& iter, reduce_func_t reduce
   sub_iter.for_each(loop, grain_size);
 }
 
-}}}  // namespace at::native::<anonymous>
+}}} // namespace at::native::<anonymous>
diff --git a/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp b/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp
index b3ae7b7b97a..b97bce3e0f2 100644
--- a/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/ReduceAllOpsKernel.cpp
@@ -14,7 +14,8 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using namespace vec;
@@ -228,4 +229,5 @@ REGISTER_DISPATCH(min_all_stub, &min_all_kernel_impl);
 REGISTER_DISPATCH(max_all_stub, &max_all_kernel_impl);
 REGISTER_DISPATCH(aminmax_allreduce_stub, &aminmax_allreduce_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp b/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp
index 7ce3c1506a1..8dee9db78f8 100644
--- a/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp
@@ -23,7 +23,8 @@
 #include <c10/util/irange.h>
 #include <ATen/AccumulateType.h>
 
-namespace at::native { namespace {
+namespace at {
+namespace native { namespace {
 
 using namespace vec;
 
@@ -536,4 +537,5 @@ REGISTER_DISPATCH(cumprod_stub, &cumprod_cpu_kernel);
 REGISTER_DISPATCH(cumsum_stub, &cumsum_cpu_kernel);
 REGISTER_DISPATCH(logcumsumexp_stub, &logcumsumexp_cpu_kernel);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/ReduceUtils.h b/aten/src/ATen/native/cpu/ReduceUtils.h
index 68b19d5b5b9..7917b9a56f6 100644
--- a/aten/src/ATen/native/cpu/ReduceUtils.h
+++ b/aten/src/ATen/native/cpu/ReduceUtils.h
@@ -7,7 +7,8 @@
 #include <ATen/native/ReductionType.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 inline namespace CPU_CAPABILITY {
 
 using namespace vec;
@@ -157,4 +158,5 @@ inline void write(scalar_t* out, int64_t count, int64_t K) {
 }
 
 } // namespace CPU_CAPABILITY
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/RenormKernel.cpp b/aten/src/ATen/native/cpu/RenormKernel.cpp
index f684d59328e..d00beddc476 100644
--- a/aten/src/ATen/native/cpu/RenormKernel.cpp
+++ b/aten/src/ATen/native/cpu/RenormKernel.cpp
@@ -7,7 +7,8 @@
 
 #include <ATen/Dispatch.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
@@ -35,4 +36,5 @@ void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
 
 REGISTER_DISPATCH(renorm_scale_factor_stub, &renorm_scale_factor_impl);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp b/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp
index 731f91c349e..13b003a5a9d 100644
--- a/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp
+++ b/aten/src/ATen/native/cpu/SampledAddmmKernel.cpp
@@ -9,7 +9,8 @@
 #include <ATen/native/cpu/utils.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -96,4 +97,5 @@ void sampled_addmm_sparse_csr_kernel(
 
 REGISTER_DISPATCH(sampled_addmm_sparse_csr_stub, &sampled_addmm_sparse_csr_kernel);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp
index 849ed43bfb5..be9e5722538 100644
--- a/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp
+++ b/aten/src/ATen/native/cpu/ScatterGatherKernel.cpp
@@ -13,7 +13,8 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -857,4 +858,5 @@ REGISTER_DISPATCH(scatter_add_expanded_index_stub, &scatter_add_expanded_index_k
 REGISTER_DISPATCH(scatter_reduce_expanded_index_stub, &scatter_reduce_expanded_index_kernel);
 REGISTER_DISPATCH(gather_expanded_index_stub, &gather_expanded_index_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/SerialStackImpl.h b/aten/src/ATen/native/cpu/SerialStackImpl.h
index ddd914d310d..b250a2b78ec 100644
--- a/aten/src/ATen/native/cpu/SerialStackImpl.h
+++ b/aten/src/ATen/native/cpu/SerialStackImpl.h
@@ -141,4 +141,4 @@ struct CanUseNativeSerialStack<TensorListType, true> {
   }
 };
 
-}}}  // namespace at::native::detail
+}}} // namespace at::native::detail
diff --git a/aten/src/ATen/native/cpu/SoftMaxKernel.cpp b/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
index 337ddb546ff..06261331bf2 100644
--- a/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
+++ b/aten/src/ATen/native/cpu/SoftMaxKernel.cpp
@@ -26,7 +26,8 @@
 //
 // We use a chunk size such that it'd fit in L1D.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 template <typename scalar_t>
@@ -1294,4 +1295,5 @@ REGISTER_DISPATCH(softmax_backward_kernel, &softmax_backward_kernel_impl);
 REGISTER_DISPATCH(
     log_softmax_backward_kernel,
     &log_softmax_backward_kernel_impl);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/SortingKernel.cpp b/aten/src/ATen/native/cpu/SortingKernel.cpp
index 2d98c31d331..bc96499bc73 100644
--- a/aten/src/ATen/native/cpu/SortingKernel.cpp
+++ b/aten/src/ATen/native/cpu/SortingKernel.cpp
@@ -11,7 +11,8 @@
 #include <c10/core/WrapDimMinimal.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -182,4 +183,5 @@ static void topk_kernel(
 REGISTER_DISPATCH(sort_stub, &sort_kernel);
 REGISTER_DISPATCH(topk_stub, &topk_kernel);
 
-} //at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/SparseFactories.cpp b/aten/src/ATen/native/cpu/SparseFactories.cpp
index cbe1abe7716..1fb33c7e371 100644
--- a/aten/src/ATen/native/cpu/SparseFactories.cpp
+++ b/aten/src/ATen/native/cpu/SparseFactories.cpp
@@ -8,7 +8,8 @@
 #include <c10/core/ScalarType.h>
 #include <c10/util/Exception.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 void _spdiags_kernel_cpu(
@@ -62,4 +63,5 @@ void _spdiags_kernel_cpu(
 
 REGISTER_DISPATCH(spdiags_kernel_stub, &_spdiags_kernel_cpu)
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/SpmmReduceKernel.cpp b/aten/src/ATen/native/cpu/SpmmReduceKernel.cpp
index b1a7788e829..0c8def8da5b 100644
--- a/aten/src/ATen/native/cpu/SpmmReduceKernel.cpp
+++ b/aten/src/ATen/native/cpu/SpmmReduceKernel.cpp
@@ -509,4 +509,4 @@ REGISTER_DISPATCH(spmm_reduce_backward_input_arg_stub, &spmm_reduce_backward_inp
 REGISTER_DISPATCH(spmm_reduce_backward_other_stub, &spmm_reduce_backward_other_kernel);
 REGISTER_DISPATCH(spmm_reduce_backward_other_arg_stub, &spmm_reduce_backward_other_arg_kernel);
 
-}} // at::native
+}} // at  native
diff --git a/aten/src/ATen/native/cpu/SpmmReduceKernel.h b/aten/src/ATen/native/cpu/SpmmReduceKernel.h
index cbcbf3c63d9..876bd083000 100644
--- a/aten/src/ATen/native/cpu/SpmmReduceKernel.h
+++ b/aten/src/ATen/native/cpu/SpmmReduceKernel.h
@@ -4,7 +4,8 @@
 #include <ATen/native/DispatchStub.h>
 #include <ATen/native/ReductionType.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using spmm_reduce_fn = void(*)(const Tensor&, const Tensor&, const Tensor&, const Tensor&, const Tensor&, ReductionType op);
 using spmm_reduce_arg_fn = void(*)(const Tensor&, const Tensor&, const Tensor&, const Tensor&, const Tensor&, const Tensor&, ReductionType op);
@@ -19,4 +20,5 @@ DECLARE_DISPATCH(spmm_reduce_backward_input_arg_fn, spmm_reduce_backward_input_a
 DECLARE_DISPATCH(spmm_reduce_backward_other_fn, spmm_reduce_backward_other_stub);
 DECLARE_DISPATCH(spmm_reduce_backward_input_arg_fn, spmm_reduce_backward_other_arg_stub);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/StackKernel.cpp b/aten/src/ATen/native/cpu/StackKernel.cpp
index 999b0d07b9e..6e9248149d8 100644
--- a/aten/src/ATen/native/cpu/StackKernel.cpp
+++ b/aten/src/ATen/native/cpu/StackKernel.cpp
@@ -6,7 +6,8 @@
 #include <ATen/native/cpu/StackKernel.h>
 #include <ATen/native/cpu/SerialStackImpl.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -21,4 +22,5 @@ void stack_serial_kernel(Tensor& result, TensorList tensors, int64_t dim) {
 
 REGISTER_DISPATCH(stack_serial_stub, &stack_serial_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/StackKernel.h b/aten/src/ATen/native/cpu/StackKernel.h
index 4e9a45e4dd1..13b67d31d4f 100644
--- a/aten/src/ATen/native/cpu/StackKernel.h
+++ b/aten/src/ATen/native/cpu/StackKernel.h
@@ -9,4 +9,4 @@ namespace at { namespace native {
 using stack_serial_fn = void(*)(Tensor &, TensorList, int64_t);
 DECLARE_DISPATCH(stack_serial_fn, stack_serial_stub);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/SumKernel.cpp b/aten/src/ATen/native/cpu/SumKernel.cpp
index b5bad819bda..9ea1ba3de68 100644
--- a/aten/src/ATen/native/cpu/SumKernel.cpp
+++ b/aten/src/ATen/native/cpu/SumKernel.cpp
@@ -9,7 +9,8 @@
 
 #include <algorithm>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 // Load vector from a smaller type (more elements) to a larger type (fewer elements),
@@ -639,4 +640,5 @@ REGISTER_DISPATCH(nansum_stub, &nansum_kernel_impl);
 REGISTER_NO_AVX512_DISPATCH(nansum_stub);
 #endif
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/TensorCompareKernel.cpp b/aten/src/ATen/native/cpu/TensorCompareKernel.cpp
index 7449017cfe6..d1c125e73ca 100644
--- a/aten/src/ATen/native/cpu/TensorCompareKernel.cpp
+++ b/aten/src/ATen/native/cpu/TensorCompareKernel.cpp
@@ -26,7 +26,8 @@
 #include <ATen/ops/result_type.h>
 #endif
 
-namespace at::native { namespace {
+namespace at {
+namespace native { namespace {
 
 template <typename scalar_t, typename scalar_t_2 = int64_t, typename loop1d_t>
 static inline void compare_base_kernel_core(
@@ -404,4 +405,5 @@ REGISTER_DISPATCH(clamp_min_scalar_stub, &clamp_min_scalar_kernel_impl);
 REGISTER_DISPATCH(clamp_max_scalar_stub, &clamp_max_scalar_kernel_impl);
 REGISTER_DISPATCH(isin_default_stub, &isin_default_kernel_cpu);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp b/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp
index 292c2e6b7ed..f2e1a93cf75 100644
--- a/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp
+++ b/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp
@@ -28,7 +28,8 @@
 #include <mkl.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 inline namespace CPU_CAPABILITY {
 
@@ -802,4 +803,5 @@ IMPLEMENT_FLOAT_KERNEL(trunc)
 // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables,modernize-avoid-c-arrays,cppcoreguidelines-avoid-c-arrays)
 IMPLEMENT_FLOAT_KERNEL(lgamma)
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/Unfold2d.cpp b/aten/src/ATen/native/cpu/Unfold2d.cpp
index a117a66c5a0..fae56c7ebc2 100644
--- a/aten/src/ATen/native/cpu/Unfold2d.cpp
+++ b/aten/src/ATen/native/cpu/Unfold2d.cpp
@@ -8,7 +8,8 @@
 #include <ATen/native/cpu/utils.h>
 #include <cmath>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -448,4 +449,5 @@ void unfolded2d_copy_kernel(
 REGISTER_DISPATCH(unfolded2d_copy_stub, &unfolded2d_copy_kernel);
 REGISTER_DISPATCH(unfolded2d_acc_stub, &unfolded2d_acc_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp b/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp
index 35049ce21d2..e50fe41045d 100644
--- a/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp
+++ b/aten/src/ATen/native/cpu/UnfoldBackwardKernel.cpp
@@ -53,7 +53,8 @@
 // and then the corresponding value of grad_in[...,i_in_dim,...,i_in_last_dim]
 // gets added up to grad_out[...,i_out_dim,...].
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -149,4 +150,5 @@ void unfold_backward_cpu_kernel(
 
 REGISTER_DISPATCH(unfold_backward_stub, &unfold_backward_cpu_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/UpSampleKernel.cpp b/aten/src/ATen/native/cpu/UpSampleKernel.cpp
index 1f471d495df..a17e3871ec4 100644
--- a/aten/src/ATen/native/cpu/UpSampleKernel.cpp
+++ b/aten/src/ATen/native/cpu/UpSampleKernel.cpp
@@ -18,7 +18,8 @@
 #include <ATen/ops/ones.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using scale_t = std::vector<c10::optional<double>>;
@@ -1981,4 +1982,5 @@ REGISTER_DISPATCH(upsample_trilinear3d_kernel, &upsample_trilinear3d_kernel_impl
 REGISTER_DISPATCH(upsample_bicubic2d_kernel, &upsample_bicubic2d_kernel_impl);
 REGISTER_DISPATCH(_upsample_bicubic2d_aa_kernel, &upsample_bicubic2d_aa_kernel_impl);
 REGISTER_DISPATCH(_upsample_bicubic2d_aa_backward_kernel, &upsample_bicubic2d_aa_backward_kernel_impl);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp b/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp
index 8a2be2738ec..c73e0249dee 100644
--- a/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp
+++ b/aten/src/ATen/native/cpu/UpSampleMoreKernel.cpp
@@ -9,7 +9,8 @@
 #include <c10/util/irange.h>
 #include <ATen/cpu/vec/vec.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using scale_t = std::vector<c10::optional<double>>;
@@ -598,4 +599,5 @@ REGISTER_DISPATCH(upsample_linear1d_backward_kernel, &upsample_linear1d_backward
 REGISTER_DISPATCH(upsample_bilinear2d_backward_kernel, &upsample_bilinear2d_backward_kernel_impl);
 REGISTER_DISPATCH(upsample_trilinear3d_backward_kernel, &upsample_trilinear3d_backward_kernel_impl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/WeightNormKernel.cpp b/aten/src/ATen/native/cpu/WeightNormKernel.cpp
index cace911114e..63c4bcebbb1 100644
--- a/aten/src/ATen/native/cpu/WeightNormKernel.cpp
+++ b/aten/src/ATen/native/cpu/WeightNormKernel.cpp
@@ -10,7 +10,8 @@
 #include <ATen/cpu/vec/vec.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -446,4 +447,5 @@ void weight_norm_backward_kernel(
 REGISTER_DISPATCH(weight_norm_stub, &weight_norm_kernel);
 REGISTER_DISPATCH(weight_norm_backward_stub, &weight_norm_backward_kernel);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/WeightNormKernel.h b/aten/src/ATen/native/cpu/WeightNormKernel.h
index 6e1f3ec3b02..aa90152320c 100644
--- a/aten/src/ATen/native/cpu/WeightNormKernel.h
+++ b/aten/src/ATen/native/cpu/WeightNormKernel.h
@@ -17,4 +17,4 @@ using weight_norm_backward_fn = void(*)(
 DECLARE_DISPATCH(weight_norm_fn, weight_norm_stub);
 DECLARE_DISPATCH(weight_norm_backward_fn, weight_norm_backward_stub);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/airy_ai.cpp b/aten/src/ATen/native/cpu/airy_ai.cpp
index ee75717b8df..6a68302ac79 100644
--- a/aten/src/ATen/native/cpu/airy_ai.cpp
+++ b/aten/src/ATen/native/cpu/airy_ai.cpp
@@ -7,7 +7,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 inline namespace CPU_CAPABILITY {
 static void airy_ai_kernel(TensorIteratorBase& iterator) {
     TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +22,5 @@ static void airy_ai_kernel(TensorIteratorBase& iterator) {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_airy_ai_stub, &CPU_CAPABILITY::airy_ai_kernel);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/batch_norm_kernel.cpp b/aten/src/ATen/native/cpu/batch_norm_kernel.cpp
index 156e98969af..3af9c362b58 100644
--- a/aten/src/ATen/native/cpu/batch_norm_kernel.cpp
+++ b/aten/src/ATen/native/cpu/batch_norm_kernel.cpp
@@ -21,7 +21,8 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using namespace vec;
@@ -1297,4 +1298,5 @@ REGISTER_DISPATCH(batch_norm_cpu_stub, &batch_norm_cpu_kernel);
 REGISTER_DISPATCH(batch_norm_cpu_collect_stats_stub, &batch_norm_cpu_collect_stats_kernel);
 REGISTER_DISPATCH(batch_norm_cpu_backward_stub, &batch_norm_cpu_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/group_norm_kernel.cpp b/aten/src/ATen/native/cpu/group_norm_kernel.cpp
index 5f02ef17d8b..79d168ce4f9 100644
--- a/aten/src/ATen/native/cpu/group_norm_kernel.cpp
+++ b/aten/src/ATen/native/cpu/group_norm_kernel.cpp
@@ -21,7 +21,8 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -1409,4 +1410,5 @@ void GroupNormBackwardKernelImpl(
 REGISTER_DISPATCH(GroupNormKernel, &GroupNormKernelImpl);
 REGISTER_DISPATCH(GroupNormBackwardKernel, &GroupNormBackwardKernelImpl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/layer_norm_kernel.cpp b/aten/src/ATen/native/cpu/layer_norm_kernel.cpp
index 3171f3ff04f..699b104eb4f 100644
--- a/aten/src/ATen/native/cpu/layer_norm_kernel.cpp
+++ b/aten/src/ATen/native/cpu/layer_norm_kernel.cpp
@@ -19,7 +19,8 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -620,4 +621,5 @@ void LayerNormBackwardKernelImpl(
 REGISTER_DISPATCH(LayerNormKernel, &LayerNormKernelImpl);
 REGISTER_DISPATCH(LayerNormBackwardKernel, &LayerNormBackwardKernelImpl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/mixed_data_type.h b/aten/src/ATen/native/cpu/mixed_data_type.h
index 6964dd5fa71..b4e5f0095df 100644
--- a/aten/src/ATen/native/cpu/mixed_data_type.h
+++ b/aten/src/ATen/native/cpu/mixed_data_type.h
@@ -38,4 +38,4 @@ inline ScalarType param_scalar_type(const Tensor& t, bool is_mixed_type) {
   return is_mixed_type ? ScalarType::Float : t.scalar_type();
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cpu/radix_sort.h b/aten/src/ATen/native/cpu/radix_sort.h
index 94faff28ad1..dfd51e43601 100644
--- a/aten/src/ATen/native/cpu/radix_sort.h
+++ b/aten/src/ATen/native/cpu/radix_sort.h
@@ -3,7 +3,8 @@
 
 #if !AT_PARALLEL_OPENMP
 
-namespace at::native {
+namespace at {
+namespace native {
 
 constexpr bool is_radix_sort_available() { return false; }
 
@@ -18,14 +19,16 @@ std::pair<K*, V*> radix_sort_parallel(
   TORCH_CHECK(false, "radix_sort_parallel: ATen is not compiled with OpenMP support");
 }
 
-} // at::native
+} // namespace native
+} // namespace at
 
 #else
 
 #include <omp.h>
 #include <c10/util/llvmMathExtras.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -191,6 +194,7 @@ std::pair<K*, V*> radix_sort_parallel(
                           : std::make_pair(tmp_key_buf, tmp_value_buf));
 }
 
-} // at::native
+} // namespace native
+} // namespace at
 
 #endif
diff --git a/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp b/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp
index c706b225daf..ecf96cab854 100644
--- a/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp
+++ b/aten/src/ATen/native/cpu/scaled_modified_bessel_k0.cpp
@@ -7,7 +7,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 inline namespace CPU_CAPABILITY {
     static void scaled_modified_bessel_k0_kernel(TensorIteratorBase& iterator) {
         TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +22,5 @@ inline namespace CPU_CAPABILITY {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_scaled_modified_bessel_k0_stub, &CPU_CAPABILITY::scaled_modified_bessel_k0_kernel);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp b/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp
index d2d8de71581..bb39f0bd0ec 100644
--- a/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp
+++ b/aten/src/ATen/native/cpu/scaled_modified_bessel_k1.cpp
@@ -7,7 +7,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 inline namespace CPU_CAPABILITY {
     static void scaled_modified_bessel_k1_kernel(TensorIteratorBase& iterator) {
         TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +22,5 @@ inline namespace CPU_CAPABILITY {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_scaled_modified_bessel_k1_stub, &CPU_CAPABILITY::scaled_modified_bessel_k1_kernel);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp b/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp
index 351ab8670be..51cac15840a 100644
--- a/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp
+++ b/aten/src/ATen/native/cpu/spherical_bessel_j0.cpp
@@ -7,7 +7,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cpu/Loops.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 inline namespace CPU_CAPABILITY {
     static void spherical_bessel_j0_kernel(TensorIteratorBase& iterator) {
         TORCH_INTERNAL_ASSERT(iterator.ntensors() == 2);
@@ -21,4 +22,5 @@ inline namespace CPU_CAPABILITY {
 } // namespace CPU_CAPABILITY
 
 REGISTER_DISPATCH(special_spherical_bessel_j0_stub, &CPU_CAPABILITY::spherical_bessel_j0_kernel);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AbsKernel.cu b/aten/src/ATen/native/cuda/AbsKernel.cu
index 980bd663734..29b643a518a 100644
--- a/aten/src/ATen/native/cuda/AbsKernel.cu
+++ b/aten/src/ATen/native/cuda/AbsKernel.cu
@@ -6,7 +6,8 @@
 #include <ATen/native/DispatchStub.h>
 #include <ATen/native/TensorIterator.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename scalar_t>
 struct AbsFunctor {
@@ -48,4 +49,5 @@ void abs_kernel_cuda(TensorIteratorBase& iter) {
 
   REGISTER_DISPATCH(abs_stub, &abs_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Activation.cpp b/aten/src/ATen/native/cuda/Activation.cpp
index 633a5f386a8..842e60f8627 100644
--- a/aten/src/ATen/native/cuda/Activation.cpp
+++ b/aten/src/ATen/native/cuda/Activation.cpp
@@ -20,7 +20,8 @@
 #include <ATen/ops/log_sigmoid_forward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // -----------------------------------
 // glu backward
@@ -105,4 +106,5 @@ TORCH_IMPL_FUNC(gelu_backward_out_cuda) (
   GeluBackwardCUDAKernelImpl(*this, get_gelutype_enum(approximate));
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Activation.h b/aten/src/ATen/native/cuda/Activation.h
index 5fbfe0d2c65..6b57e928a56 100644
--- a/aten/src/ATen/native/cuda/Activation.h
+++ b/aten/src/ATen/native/cuda/Activation.h
@@ -17,4 +17,4 @@ void launch_log_sigmoid_forward_kernel(TensorIteratorBase& iter);
 void GeluCUDAKernelImpl(TensorIteratorBase& it, GeluType approximate);
 void GeluBackwardCUDAKernelImpl(TensorIteratorBase& it, GeluType approximate);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ActivationEluKernel.cu b/aten/src/ATen/native/cuda/ActivationEluKernel.cu
index 3f68b521c00..113e6da10ea 100644
--- a/aten/src/ATen/native/cuda/ActivationEluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationEluKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void elu_kernel(
@@ -83,4 +84,5 @@ void elu_backward_kernel(
 REGISTER_DISPATCH(elu_stub, &elu_kernel);
 REGISTER_DISPATCH(elu_backward_stub, &elu_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationGeluKernel.cu b/aten/src/ATen/native/cuda/ActivationGeluKernel.cu
index 47012f980fc..d3d7879d3b8 100644
--- a/aten/src/ATen/native/cuda/ActivationGeluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationGeluKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void GeluCUDAKernelImpl(TensorIteratorBase& it, GeluType approximate) {
   if (approximate == GeluType::Tanh) {
@@ -85,4 +86,5 @@ void GeluBackwardCUDAKernelImpl(TensorIteratorBase& it, GeluType approximate) {
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationGluKernel.cu b/aten/src/ATen/native/cuda/ActivationGluKernel.cu
index 15ac2a50c91..740edbbf38e 100644
--- a/aten/src/ATen/native/cuda/ActivationGluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationGluKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // -----------------------------------
 // glu forward
@@ -138,4 +139,5 @@ void launch_glu_backward_kernel(
 REGISTER_DISPATCH(glu_stub, &glu_kernel);
 REGISTER_DISPATCH(glu_jvp_stub, &glu_jvp_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu b/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu
index 3e2ca62e274..ae2f6b11b85 100644
--- a/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardshrinkKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void hardshrink_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -36,4 +37,5 @@ void hardshrink_kernel(TensorIteratorBase& iter, const Scalar& value) {
 
 REGISTER_DISPATCH(hardshrink_stub, &hardshrink_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu b/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu
index f69b5c5daed..ceafa53b72f 100644
--- a/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardsigmoidKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void hardsigmoid_kernel(TensorIteratorBase& iter) {
@@ -71,4 +72,5 @@ void hardsigmoid_backward_kernel(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(hardsigmoid_stub, &hardsigmoid_kernel);
 REGISTER_DISPATCH(hardsigmoid_backward_stub, &hardsigmoid_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu b/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu
index 38011e9ed60..7d952043ad8 100644
--- a/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardswishKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void hardswish_kernel(TensorIterator& iter) {
@@ -60,4 +61,5 @@ void hardswish_backward_kernel(TensorIterator& iter) {
 REGISTER_DISPATCH(hardswish_stub, &hardswish_kernel);
 REGISTER_DISPATCH(hardswish_backward_stub, &hardswish_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu b/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu
index 30bb909d58e..0f8338d6911 100644
--- a/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationHardtanhKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void hardtanh_backward_kernel(
@@ -42,4 +43,5 @@ void hardtanh_backward_kernel(
 
 REGISTER_DISPATCH(hardtanh_backward_stub, &hardtanh_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu b/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu
index 6b848df333a..c323aca1ca7 100644
--- a/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationLeakyReluKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void leaky_relu_kernel(TensorIteratorBase& iter, const Scalar& negval_) {
@@ -59,4 +60,5 @@ void leaky_relu_backward_kernel(
 REGISTER_DISPATCH(leaky_relu_stub, &leaky_relu_kernel);
 REGISTER_DISPATCH(leaky_relu_backward_stub, &leaky_relu_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu b/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu
index eb34d9d4633..131462467d3 100644
--- a/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationLogSigmoidKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // -----------------------------------
 // log_sigmoid forward
@@ -61,4 +62,5 @@ void log_sigmoid_backward_kernel(TensorIterator& iter) {
 
 REGISTER_DISPATCH(log_sigmoid_backward_stub, &log_sigmoid_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationMishKernel.cu b/aten/src/ATen/native/cuda/ActivationMishKernel.cu
index e259e64fc08..70c058644f6 100644
--- a/aten/src/ATen/native/cuda/ActivationMishKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationMishKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void mish_kernel(TensorIteratorBase& iter) {
@@ -61,4 +62,5 @@ void mish_backward_kernel(TensorIterator& iter) {
 REGISTER_DISPATCH(mish_stub, &mish_kernel);
 REGISTER_DISPATCH(mish_backward_stub, &mish_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationPreluKernel.cu b/aten/src/ATen/native/cuda/ActivationPreluKernel.cu
index d6b73317738..7ae748599da 100644
--- a/aten/src/ATen/native/cuda/ActivationPreluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationPreluKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // -----------------------------------
 // prelu
@@ -45,4 +46,5 @@ void prelu_backward_kernel(TensorIterator &iter) {
 REGISTER_DISPATCH(prelu_stub, &prelu_kernel);
 REGISTER_DISPATCH(prelu_backward_stub, &prelu_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationSiluKernel.cu b/aten/src/ATen/native/cuda/ActivationSiluKernel.cu
index e9b495ca70b..701b901e4f7 100644
--- a/aten/src/ATen/native/cuda/ActivationSiluKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationSiluKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void silu_kernel(TensorIteratorBase& iter) {
@@ -56,4 +57,5 @@ void silu_backward_kernel(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(silu_stub, &silu_kernel);
 REGISTER_DISPATCH(silu_backward_stub, &silu_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu b/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu
index 054e42139b0..86c04221b24 100644
--- a/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationSoftplusKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void softplus_kernel(
@@ -71,4 +72,5 @@ void softplus_backward_kernel(
 REGISTER_DISPATCH(softplus_stub, &softplus_kernel);
 REGISTER_DISPATCH(softplus_backward_stub, &softplus_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu b/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu
index a07d0d69a38..e21e3b94fac 100644
--- a/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationSoftshrinkKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void softshrink_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -55,4 +56,5 @@ void shrink_backward_kernel(TensorIteratorBase& iter, const Scalar& value) {
 REGISTER_DISPATCH(softshrink_stub, &softshrink_kernel);
 REGISTER_DISPATCH(shrink_backward_stub, &shrink_backward_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu b/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu
index 68baa5133e7..86d8bbd528c 100644
--- a/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu
+++ b/aten/src/ATen/native/cuda/ActivationThresholdKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template <typename scalar_t>
@@ -49,4 +50,5 @@ static void threshold_kernel_cuda(
 
 REGISTER_DISPATCH(threshold_stub, &threshold_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu b/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu
index b25f3872523..e9044e77dca 100644
--- a/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu
@@ -36,7 +36,8 @@
 #define CUDA_MAX_THREADS 1024 // this is safe, in reality 256 is our limit
 #define BLOCK_STRIDE 2 // increasing block_stride to lower # of blocks launched
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -799,7 +800,8 @@ namespace {
     return gradInput;
   }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
 
 #undef BLOCK_STRIDE
 #undef CUDA_MAX_THREADS
diff --git a/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu b/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu
index 4bc45f30569..4e1ac9b5372 100644
--- a/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu
@@ -25,7 +25,8 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -542,4 +543,5 @@ Tensor adaptive_avg_pool3d_backward_cuda(
   return gradInput;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu b/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu
index bf25a098cb2..d37ed621654 100644
--- a/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu
@@ -23,7 +23,8 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -471,4 +472,5 @@ TORCH_IMPL_FUNC(adaptive_max_pool2d_backward_out_cuda)
     gradInput.copy_(gradInput_c);
   }
  }
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu b/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu
index 0b57d9e5101..3c19257036c 100644
--- a/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu
+++ b/aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu
@@ -23,7 +23,8 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -481,4 +482,5 @@ TORCH_IMPL_FUNC(adaptive_max_pool3d_backward_out_cuda)
         });
   }
  }
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AmpKernels.cu b/aten/src/ATen/native/cuda/AmpKernels.cu
index ab2bf66d517..793ce5edb3f 100644
--- a/aten/src/ATen/native/cuda/AmpKernels.cu
+++ b/aten/src/ATen/native/cuda/AmpKernels.cu
@@ -35,7 +35,8 @@ static __host__ __device__ __forceinline__ int isfinite_ensure_cuda_math(float v
 }
 }
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 // Single-tensor fallback for _amp_foreach_non_finite_check_and_unscale_cuda_.
@@ -245,4 +246,5 @@ Tensor& _amp_update_scale_cuda_(Tensor& current_scale,
   return current_scale;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AveragePool2d.cu b/aten/src/ATen/native/cuda/AveragePool2d.cu
index 9a5399b9e00..b330b733edc 100644
--- a/aten/src/ATen/native/cuda/AveragePool2d.cu
+++ b/aten/src/ATen/native/cuda/AveragePool2d.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/avg_pool2d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -455,4 +456,5 @@ TORCH_IMPL_FUNC(avg_pool2d_backward_out_cuda) (
   );
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/AveragePool3d.cu b/aten/src/ATen/native/cuda/AveragePool3d.cu
index a722236ea57..c5bc797c935 100644
--- a/aten/src/ATen/native/cuda/AveragePool3d.cu
+++ b/aten/src/ATen/native/cuda/AveragePool3d.cu
@@ -21,7 +21,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -603,4 +604,5 @@ TORCH_IMPL_FUNC(avg_pool3d_backward_out_cuda) (
   }
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu
index f0a498b0647..0440b522030 100644
--- a/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu
@@ -8,7 +8,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename scalar_t>
 struct BitwiseAndFunctor {
@@ -78,4 +79,5 @@ REGISTER_DISPATCH(bitwise_or_stub, &bitwise_or_kernel_cuda);
 REGISTER_DISPATCH(bitwise_xor_stub, &bitwise_xor_kernel_cuda);
 
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu b/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu
index bd4d243f618..13e9757b5f3 100644
--- a/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryDivFloorKernel.cu
@@ -14,7 +14,8 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace binary_internal {
 
 void div_floor_kernel_cuda(TensorIteratorBase& iter) {
@@ -107,4 +108,5 @@ void div_floor_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(div_floor_stub, &binary_internal::div_floor_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu b/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu
index aa955a9c7e5..05e500c02da 100644
--- a/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryDivTrueKernel.cu
@@ -13,7 +13,8 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace binary_internal {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char div_name[] = "div_kernel";
@@ -58,4 +59,5 @@ void div_true_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(div_true_stub, &binary_internal::div_true_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu b/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu
index 5e906a000b0..01a04b40cbc 100644
--- a/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryDivTruncKernel.cu
@@ -12,7 +12,8 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace binary_internal {
 
 void div_trunc_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +51,5 @@ void div_trunc_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(div_trunc_stub, &binary_internal::div_trunc_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu b/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu
index e734a66e931..1b2c6d7dfbb 100644
--- a/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryGeometricKernels.cu
@@ -8,7 +8,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void atan2_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(
@@ -36,4 +37,5 @@ void hypot_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(atan2_stub, &atan2_kernel_cuda);
 REGISTER_DISPATCH(hypot_stub, &hypot_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu
index eaa01ac1acc..47ff662374d 100644
--- a/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu
@@ -9,7 +9,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char logical_and_name[] = "logical_and_kernel";
 void logical_and_kernel_cuda(TensorIterator& iter) {
@@ -125,4 +126,5 @@ REGISTER_DISPATCH(logical_or_stub, &logical_or_kernel_cuda);
 REGISTER_DISPATCH(logical_xor_stub, &logical_xor_kernel_cuda);
 
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu
index 75d5991f93d..0fd70d9e73c 100644
--- a/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryMiscBackwardOpsKernels.cu
@@ -13,7 +13,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char sigmoid_backward_name[] = "sigmoid_backward";
 void sigmoid_backward_kernel_cuda(TensorIteratorBase& iter) {
@@ -128,4 +129,5 @@ REGISTER_DISPATCH(sigmoid_backward_stub, &sigmoid_backward_kernel_cuda);
 REGISTER_DISPATCH(logit_backward_stub, &logit_backward_kernel_cuda);
 REGISTER_DISPATCH(tanh_backward_stub, &tanh_backward_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu
index 85be724c7a3..6413b98a1ae 100644
--- a/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu
@@ -10,7 +10,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void smooth_l1_kernel_cuda(TensorIteratorBase& iter, double beta) {
   AT_DISPATCH_FLOATING_TYPES_AND_HALF(iter.dtype(), "smooth_l1_cuda", [&iter, beta]() {
@@ -78,4 +79,5 @@ REGISTER_DISPATCH(xlog1py_stub, &xlog1py_kernel_cuda);
 // DO NOT ADD ANY NEW KERNELS HERE
 // CUDA compilation times grow quickly.  It's perfectly acceptable to have a file per kernel.
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryMulKernel.cu b/aten/src/ATen/native/cuda/BinaryMulKernel.cu
index 251221f7adc..4213e77e8d9 100644
--- a/aten/src/ATen/native/cuda/BinaryMulKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryMulKernel.cu
@@ -16,7 +16,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char mul_name[] = "mul_kernel";
 void mul_kernel_cuda(TensorIteratorBase& iter) {
@@ -45,4 +46,5 @@ void mul_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(mul_stub, &mul_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu b/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu
index dfa2f7124b5..285702785d4 100644
--- a/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu
+++ b/aten/src/ATen/native/cuda/BinaryRemainderKernel.cu
@@ -11,7 +11,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void remainder_kernel_cuda(TensorIteratorBase& iter) {
   if (isIntegralType(iter.common_dtype(), /*includeBool*/ false)) {
@@ -58,4 +59,5 @@ void fmod_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(remainder_stub, &remainder_kernel_cuda);
 REGISTER_DISPATCH(fmod_stub, &fmod_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu b/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu
index c2578f8bbc2..c034e98e1e4 100644
--- a/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu
+++ b/aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu
@@ -8,7 +8,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 
 void lshift_kernel_cuda(TensorIteratorBase& iter) {
@@ -32,4 +33,5 @@ void rshift_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(lshift_stub, &lshift_kernel_cuda);
 REGISTER_DISPATCH(rshift_stub, &rshift_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Blas.cpp b/aten/src/ATen/native/cuda/Blas.cpp
index ce78f517a0b..b6cced523ca 100644
--- a/aten/src/ATen/native/cuda/Blas.cpp
+++ b/aten/src/ATen/native/cuda/Blas.cpp
@@ -30,7 +30,8 @@
 #include <ATen/ops/vdot_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -122,7 +123,7 @@ enum class Activation {
   GELU,
 };
 
-#if !defined(USE_ROCM) && !defined(_MSC_VER)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)
 cuda::blas::GEMMAndBiasActivationEpilogue activation_to_gemm_and_blas_arg(Activation a) {
   switch (a) {
     case Activation::None:
@@ -286,7 +287,7 @@ Tensor& addmm_out_cuda_impl(Tensor& result, const Tensor& self, const Tensor& ma
 
   TORCH_INTERNAL_ASSERT_DEBUG_ONLY(!result_->is_conj());
 
-#if !defined(USE_ROCM) && !defined(_MSC_VER)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(_MSC_VER)
   if (useLtInterface) {
     AT_DISPATCH_FLOATING_TYPES_AND2(
         at::ScalarType::Half,
@@ -672,4 +673,5 @@ TORCH_IMPL_FUNC(addmv_out_cuda)(const Tensor &self, const Tensor &mat, const Ten
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Bucketization.cu b/aten/src/ATen/native/cuda/Bucketization.cu
index 7a139ffac7e..6f7f25b8ccb 100644
--- a/aten/src/ATen/native/cuda/Bucketization.cu
+++ b/aten/src/ATen/native/cuda/Bucketization.cu
@@ -15,7 +15,8 @@
 #include <ATen/ops/searchsorted_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // Implement a numpy like searchsorted and a TF like bucketize function running on cuda
 // See details in ATen/nativate/Bucketization.cpp
@@ -218,4 +219,5 @@ Tensor bucketize_cuda(const Scalar& self, const Tensor& boundaries, bool out_int
   return bucketize_cuda(searchsorted_scalar_tensor(self, boundaries.device()), boundaries, out_int32, right);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CUDAJitLoops.cuh b/aten/src/ATen/native/cuda/CUDAJitLoops.cuh
index 830a3024a98..9ade3bf0214 100644
--- a/aten/src/ATen/native/cuda/CUDAJitLoops.cuh
+++ b/aten/src/ATen/native/cuda/CUDAJitLoops.cuh
@@ -287,6 +287,6 @@ static void jitted_gpu_kernel_impl(
     );
 }
 
-}}  // at::native
+}} // at::native
 
 #endif // AT_USE_JITERATOR()
diff --git a/aten/src/ATen/native/cuda/CUDAScalar.cu b/aten/src/ATen/native/cuda/CUDAScalar.cu
index f298058106c..3760257cbdd 100644
--- a/aten/src/ATen/native/cuda/CUDAScalar.cu
+++ b/aten/src/ATen/native/cuda/CUDAScalar.cu
@@ -10,7 +10,8 @@
 
 #include <ATen/cuda/CUDAContext.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Scalar _local_scalar_dense_cuda(const Tensor& self) {
   Scalar r;
@@ -24,4 +25,5 @@ Scalar _local_scalar_dense_cuda(const Tensor& self) {
   return r;
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Col2Im.cu b/aten/src/ATen/native/cuda/Col2Im.cu
index 4445e838990..53eb2df3013 100644
--- a/aten/src/ATen/native/cuda/Col2Im.cu
+++ b/aten/src/ATen/native/cuda/Col2Im.cu
@@ -20,7 +20,8 @@
 #include <ATen/ops/im2col_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void col2im_out_cuda_template(
@@ -168,4 +169,5 @@ Tensor col2im_cuda(
   return output;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CompareEQKernel.cu b/aten/src/ATen/native/cuda/CompareEQKernel.cu
index 088312fb6f1..c3b8689ba3f 100644
--- a/aten/src/ATen/native/cuda/CompareEQKernel.cu
+++ b/aten/src/ATen/native/cuda/CompareEQKernel.cu
@@ -9,7 +9,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native { namespace {
+namespace at {
+namespace native { namespace {
 
 enum class EqOpType {EQ, NE};
 
@@ -47,4 +48,5 @@ void ne_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(eq_stub, &eq_kernel_cuda);
 REGISTER_DISPATCH(ne_stub, &ne_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CompareKernels.cu b/aten/src/ATen/native/cuda/CompareKernels.cu
index 8a1a97759f1..afb79cb2734 100644
--- a/aten/src/ATen/native/cuda/CompareKernels.cu
+++ b/aten/src/ATen/native/cuda/CompareKernels.cu
@@ -9,7 +9,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native { namespace {
+namespace at {
+namespace native { namespace {
 
 enum class OpType {GE, GT, LE, LT};
 
@@ -100,4 +101,5 @@ REGISTER_DISPATCH(gt_stub, &gt_kernel_cuda);
 REGISTER_DISPATCH(le_stub, &le_kernel_cuda);
 REGISTER_DISPATCH(lt_stub, &lt_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ComplexKernel.cu b/aten/src/ATen/native/cuda/ComplexKernel.cu
index 2bf26722fbc..8738c0ab4c8 100644
--- a/aten/src/ATen/native/cuda/ComplexKernel.cu
+++ b/aten/src/ATen/native/cuda/ComplexKernel.cu
@@ -7,7 +7,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void complex_kernel_cuda(TensorIterator& iter) {
@@ -33,4 +34,5 @@ void polar_kernel_cuda(TensorIterator& iter) {
 REGISTER_DISPATCH(complex_stub, &complex_kernel_cuda);
 REGISTER_DISPATCH(polar_stub, &polar_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ConvolutionMM2d.cu b/aten/src/ATen/native/cuda/ConvolutionMM2d.cu
index bca9d38e2ce..153e3009791 100644
--- a/aten/src/ATen/native/cuda/ConvolutionMM2d.cu
+++ b/aten/src/ATen/native/cuda/ConvolutionMM2d.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/sum.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void slow_conv2d_shape_check(
@@ -499,4 +500,5 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv2d_backward_cuda(
       grad_bias);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Copy.cu b/aten/src/ATen/native/cuda/Copy.cu
index 07ead054603..564ecf1c129 100644
--- a/aten/src/ATen/native/cuda/Copy.cu
+++ b/aten/src/ATen/native/cuda/Copy.cu
@@ -19,7 +19,8 @@
 #include <c10/cuda/CUDACachingAllocator.h>
 #include <c10/cuda/CUDAStream.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void neg_kernel_cuda(TensorIteratorBase &iter);
 void conj_kernel_cuda(TensorIteratorBase &iter);
@@ -282,4 +283,5 @@ static void copy_kernel_cuda(TensorIterator& iter, bool non_blocking) {
 
 REGISTER_DISPATCH(copy_stub, &copy_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Copy.h b/aten/src/ATen/native/cuda/Copy.h
index 5639567d666..80504f7a9b0 100644
--- a/aten/src/ATen/native/cuda/Copy.h
+++ b/aten/src/ATen/native/cuda/Copy.h
@@ -7,4 +7,4 @@ namespace native {
 
 void direct_copy_kernel_cuda(TensorIteratorBase &iter);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/CopysignKernel.cu b/aten/src/ATen/native/cuda/CopysignKernel.cu
index 38724d7e299..be46e61d9d7 100644
--- a/aten/src/ATen/native/cuda/CopysignKernel.cu
+++ b/aten/src/ATen/native/cuda/CopysignKernel.cu
@@ -18,7 +18,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void copysign_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(kBFloat16, kHalf, iter.common_dtype(), "copysign_cuda", [&]() {
@@ -30,4 +31,5 @@ void copysign_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(copysign_stub, &copysign_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CrossKernel.cu b/aten/src/ATen/native/cuda/CrossKernel.cu
index 01da1ffc3ba..3ddcf93ed73 100644
--- a/aten/src/ATen/native/cuda/CrossKernel.cu
+++ b/aten/src/ATen/native/cuda/CrossKernel.cu
@@ -5,7 +5,8 @@
 #include <ATen/Dispatch.h>
 #include <ATen/core/Tensor.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename T, typename OffsetCalc, typename StrideType>
 __global__ void cross_kernel(
@@ -89,4 +90,5 @@ void cross_impl(const Tensor& result, const Tensor& x1, const Tensor& x2, int64_
 
 REGISTER_DISPATCH(cross_stub, &cross_impl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CumminmaxKernel.cu b/aten/src/ATen/native/cuda/CumminmaxKernel.cu
index d72f13f559c..be6cdcbbdcc 100644
--- a/aten/src/ATen/native/cuda/CumminmaxKernel.cu
+++ b/aten/src/ATen/native/cuda/CumminmaxKernel.cu
@@ -8,7 +8,8 @@
 #include <limits>
 #include <functional>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void launch_cummax_cuda_kernel(const TensorBase& self, const TensorBase& values, const TensorBase& indices, int64_t dim) {
   AT_DISPATCH_ALL_TYPES_AND3(at::ScalarType::Bool, at::ScalarType::Half, at::ScalarType::BFloat16,
@@ -26,4 +27,5 @@ void launch_cummin_cuda_kernel(const TensorBase& self, const TensorBase& values,
   });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CumprodKernel.cu b/aten/src/ATen/native/cuda/CumprodKernel.cu
index f44ec072b16..20528f31be2 100644
--- a/aten/src/ATen/native/cuda/CumprodKernel.cu
+++ b/aten/src/ATen/native/cuda/CumprodKernel.cu
@@ -5,7 +5,8 @@
 #include <ATen/native/cuda/ScanKernels.h>
 #include <ATen/native/cuda/ScanUtils.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void launch_cumprod_cuda_kernel(const TensorBase& result, const TensorBase& self, int64_t dim) {
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
@@ -20,4 +21,5 @@ void launch_cumprod_cuda_kernel(const TensorBase& result, const TensorBase& self
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/CumsumKernel.cu b/aten/src/ATen/native/cuda/CumsumKernel.cu
index 07db9cd2361..797238e48d2 100644
--- a/aten/src/ATen/native/cuda/CumsumKernel.cu
+++ b/aten/src/ATen/native/cuda/CumsumKernel.cu
@@ -5,7 +5,8 @@
 #include <ATen/native/cuda/ScanKernels.h>
 #include <ATen/native/cuda/ScanUtils.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void launch_cumsum_cuda_kernel(const TensorBase& result, const TensorBase& self, int64_t dim) {
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(
@@ -22,4 +23,5 @@ void launch_cumsum_cuda_kernel(const TensorBase& result, const TensorBase& self,
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DepthwiseConv2d.cu b/aten/src/ATen/native/cuda/DepthwiseConv2d.cu
index 7e2a0796ccb..20748837bba 100644
--- a/aten/src/ATen/native/cuda/DepthwiseConv2d.cu
+++ b/aten/src/ATen/native/cuda/DepthwiseConv2d.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/_conv_depthwise2d_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 using at::cuda::detail::CUDA_NUM_THREADS;
 using at::cuda::detail::GET_BLOCKS;
@@ -632,4 +633,5 @@ std::tuple<Tensor, Tensor> conv_depthwise2d_backward_cuda(
 
 REGISTER_CUDA_DISPATCH(conv_depthwise2d_backward_stub, &conv_depthwise2d_backward_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DepthwiseConv3d.cu b/aten/src/ATen/native/cuda/DepthwiseConv3d.cu
index f11bfc817ab..ce44e2e58d0 100644
--- a/aten/src/ATen/native/cuda/DepthwiseConv3d.cu
+++ b/aten/src/ATen/native/cuda/DepthwiseConv3d.cu
@@ -19,7 +19,8 @@
 #include <tuple>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t,
@@ -697,4 +698,5 @@ REGISTER_CUDA_DISPATCH(conv_depthwise3d_backward_stub, &conv_depthwise3d_backwar
 #undef NODEF_OR_EQUAL_3
 #undef NODEF_OR_EQUAL
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu b/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu
index 8b387585863..b5e14cd51ed 100644
--- a/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu
+++ b/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu
@@ -21,7 +21,8 @@
 #include <ATen/ops/max_pool2d_with_indices_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -564,4 +565,5 @@ const Tensor& gradInput) {
   );
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu b/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu
index 6e2f79466e5..6ac9423f57d 100644
--- a/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu
+++ b/aten/src/ATen/native/cuda/DilatedMaxPool3d.cu
@@ -24,7 +24,8 @@
 #include <ATen/ops/zeros_like.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 __device__ inline int min(int a, int b) {
@@ -650,4 +651,5 @@ Tensor max_pool3d_with_indices_backward_cuda(
   return gradInput;
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistanceKernel.cu b/aten/src/ATen/native/cuda/DistanceKernel.cu
index c1380dd7ff2..609fa3c07db 100644
--- a/aten/src/ATen/native/cuda/DistanceKernel.cu
+++ b/aten/src/ATen/native/cuda/DistanceKernel.cu
@@ -19,7 +19,8 @@
 
 #include <c10/macros/Macros.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -362,4 +363,5 @@ REGISTER_DISPATCH(pdist_backward_stub, &pdist_backward_kernel_impl);
 REGISTER_DISPATCH(cdist_stub, &cdist_kernel_impl);
 REGISTER_DISPATCH(cdist_backward_stub, &cdist_backward_kernel_impl);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionBernoulli.cu b/aten/src/ATen/native/cuda/DistributionBernoulli.cu
index 89a518267d2..261dcf276b9 100644
--- a/aten/src/ATen/native/cuda/DistributionBernoulli.cu
+++ b/aten/src/ATen/native/cuda/DistributionBernoulli.cu
@@ -21,7 +21,8 @@
 #include <utility>
 #include <type_traits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void bernoulli_tensor_kernel(const TensorBase &self, const TensorBase &p_, c10::optional<Generator> gen_) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen_, cuda::detail::getDefaultCUDAGenerator());
@@ -37,4 +38,5 @@ void bernoulli_scalar_kernel(const TensorBase &self, double p, c10::optional<Gen
 REGISTER_DISPATCH(bernoulli_tensor_stub, &bernoulli_tensor_kernel);
 REGISTER_DISPATCH(bernoulli_scalar_stub, &bernoulli_scalar_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu b/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu
index a66d3cf3288..ff3117b3372 100644
--- a/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionCauchyKernel.cu
@@ -3,7 +3,8 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void cauchy_kernel(TensorIteratorBase& iter, double median, double sigma, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +13,5 @@ void cauchy_kernel(TensorIteratorBase& iter, double median, double sigma, c10::o
 
 REGISTER_DISPATCH(cauchy_stub, &cauchy_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu b/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu
index 76cb94f6fd8..7fee338c255 100644
--- a/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionExponentialKernel.cu
@@ -3,7 +3,8 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void exponential_kernel(TensorIteratorBase& iter, double lambda, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +13,5 @@ void exponential_kernel(TensorIteratorBase& iter, double lambda, c10::optional<G
 
 REGISTER_DISPATCH(exponential_stub, &exponential_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu b/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu
index 0fe49d7bbd4..91ddfa1d2fe 100644
--- a/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionGeometricKernel.cu
@@ -3,7 +3,8 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void geometric_kernel(TensorIteratorBase& iter, double p_, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +13,5 @@ void geometric_kernel(TensorIteratorBase& iter, double p_, c10::optional<Generat
 
 REGISTER_DISPATCH(geometric_stub, &geometric_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu b/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu
index f394d4fea39..84667183d6e 100644
--- a/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionLogNormalKernel.cu
@@ -3,7 +3,8 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void log_normal_kernel(TensorIteratorBase& iter, double mean, double std, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +13,5 @@ void log_normal_kernel(TensorIteratorBase& iter, double mean, double std, c10::o
 
 REGISTER_DISPATCH(log_normal_stub, &log_normal_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionNormal.cu b/aten/src/ATen/native/cuda/DistributionNormal.cu
index a17c3e3da05..d0aa59b0f4a 100644
--- a/aten/src/ATen/native/cuda/DistributionNormal.cu
+++ b/aten/src/ATen/native/cuda/DistributionNormal.cu
@@ -3,7 +3,8 @@
 #include <ATen/cuda/CUDAGeneratorImpl.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void normal_kernel(const TensorBase &self, double mean, double std, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +13,5 @@ void normal_kernel(const TensorBase &self, double mean, double std, c10::optiona
 
 REGISTER_DISPATCH(normal_stub, &normal_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionRandomKernel.cu b/aten/src/ATen/native/cuda/DistributionRandomKernel.cu
index 034a19c512f..8f5c653e811 100644
--- a/aten/src/ATen/native/cuda/DistributionRandomKernel.cu
+++ b/aten/src/ATen/native/cuda/DistributionRandomKernel.cu
@@ -3,7 +3,8 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void random_from_to_kernel(TensorIteratorBase& iter, uint64_t range, int64_t base, c10::optional<Generator> gen_) {
   auto gen = get_generator_or_default<CUDAGeneratorImpl>(gen_, cuda::detail::getDefaultCUDAGenerator());
@@ -24,4 +25,5 @@ REGISTER_DISPATCH(random_from_to_stub, &random_from_to_kernel);
 REGISTER_DISPATCH(random_stub, &random_kernel);
 REGISTER_DISPATCH(random_full_64_bits_range_stub, &random_full_64_bits_range_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/DistributionUniform.cu b/aten/src/ATen/native/cuda/DistributionUniform.cu
index 2ebdfa44645..bcd635cba9e 100644
--- a/aten/src/ATen/native/cuda/DistributionUniform.cu
+++ b/aten/src/ATen/native/cuda/DistributionUniform.cu
@@ -3,7 +3,8 @@
 #include <ATen/native/UnaryOps.h>
 #include <ATen/native/cuda/DistributionTemplates.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void uniform_kernel(TensorIteratorBase& iter, double from, double to, c10::optional<Generator> gen) {
   auto generator = get_generator_or_default<CUDAGeneratorImpl>(gen, cuda::detail::getDefaultCUDAGenerator());
@@ -12,4 +13,5 @@ void uniform_kernel(TensorIteratorBase& iter, double from, double to, c10::optio
 
 REGISTER_DISPATCH(uniform_stub, &uniform_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Distributions.cpp b/aten/src/ATen/native/cuda/Distributions.cpp
index c0d5abb49bf..d6a2cc084ab 100644
--- a/aten/src/ATen/native/cuda/Distributions.cpp
+++ b/aten/src/ATen/native/cuda/Distributions.cpp
@@ -16,7 +16,8 @@
 #include <ATen/ops/poisson_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor _s_poisson_cuda(const Tensor& lambda, c10::optional<Generator> gen_) {
   auto gen = get_generator_or_default<CUDAGeneratorImpl>(gen_, cuda::detail::getDefaultCUDAGenerator());
@@ -81,4 +82,5 @@ Tensor _dirichlet_grad_cuda(const Tensor& x, const Tensor& alpha, const Tensor&
   return ret;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Distributions.cu b/aten/src/ATen/native/cuda/Distributions.cu
index 1aac9fe8ba2..5f91aeb8ea0 100644
--- a/aten/src/ATen/native/cuda/Distributions.cu
+++ b/aten/src/ATen/native/cuda/Distributions.cu
@@ -128,7 +128,8 @@ void gamma_cuda_kernel(
 
 } // namespace
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void launch_dirichlet_kernel(at::TensorIteratorBase &iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16,
@@ -205,4 +206,5 @@ void launch_dirichlet_grad_kernel(TensorIteratorBase &iter) {
   });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Distributions.h b/aten/src/ATen/native/cuda/Distributions.h
index 1a34fdfdf31..311375b57d7 100644
--- a/aten/src/ATen/native/cuda/Distributions.h
+++ b/aten/src/ATen/native/cuda/Distributions.h
@@ -22,4 +22,4 @@ void launch_standard_gamma_grad_kernel(TensorIteratorBase &iter);
 
 void launch_dirichlet_grad_kernel(TensorIteratorBase &iter);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Dropout.cu b/aten/src/ATen/native/cuda/Dropout.cu
index 67ea3e4f832..75b87f141e0 100644
--- a/aten/src/ATen/native/cuda/Dropout.cu
+++ b/aten/src/ATen/native/cuda/Dropout.cu
@@ -25,7 +25,8 @@
 #include <ATen/ops/zeros_like.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -413,4 +414,5 @@ Tensor masked_scale_cuda(const Tensor& self, const Tensor& mask, double scale){
   return dropout_backward_cuda<uint8_t>(self, mask, scale);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Embedding.cu b/aten/src/ATen/native/cuda/Embedding.cu
index cd4d8aae544..48c5169a120 100644
--- a/aten/src/ATen/native/cuda/Embedding.cu
+++ b/aten/src/ATen/native/cuda/Embedding.cu
@@ -31,7 +31,8 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -392,4 +393,5 @@ Tensor & embedding_renorm_cuda_(Tensor & self, const Tensor & indices,
 }
 
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu b/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu
index 30ee6abb46a..64742847cfc 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu
@@ -21,7 +21,8 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -361,4 +362,5 @@ Tensor embedding_backward_cuda_kernel(
   return grad_weight;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/EmbeddingBag.cu b/aten/src/ATen/native/cuda/EmbeddingBag.cu
index 7a1e2663b49..419dbeace85 100644
--- a/aten/src/ATen/native/cuda/EmbeddingBag.cu
+++ b/aten/src/ATen/native/cuda/EmbeddingBag.cu
@@ -34,7 +34,8 @@
 #include <thrust/iterator/reverse_iterator.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 #if !CUB_SUPPORTS_SCAN_BY_KEY()
 template<typename index_t>
@@ -177,7 +178,7 @@ Tensor embedding_bag_backward_cuda_sum_avg(
                                    int64_t padding_idx) {
   auto indices = indices_.contiguous();
 
-  ptrdiff_t num_indices = indices.numel();
+  int64_t num_indices = indices.numel();
 
   if (num_indices == 0) {
     // all empty bags
@@ -559,4 +560,5 @@ Tensor _embedding_bag_per_sample_weights_backward_cuda(
   return output;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Equal.cpp b/aten/src/ATen/native/cuda/Equal.cpp
index 206d562efcd..9bcba18dacf 100644
--- a/aten/src/ATen/native/cuda/Equal.cpp
+++ b/aten/src/ATen/native/cuda/Equal.cpp
@@ -10,7 +10,8 @@
 #include <ATen/ops/equal_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 bool cuda_equal(const Tensor& self, const Tensor &src) {
   if (!at::namedinference::are_names_equal(
@@ -29,4 +30,5 @@ bool cuda_equal(const Tensor& self, const Tensor &src) {
   return at::cuda::eq(self, src).all().item().to<bool>();
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/FillKernel.cu b/aten/src/ATen/native/cuda/FillKernel.cu
index 5090979d356..ac64ee11819 100644
--- a/aten/src/ATen/native/cuda/FillKernel.cu
+++ b/aten/src/ATen/native/cuda/FillKernel.cu
@@ -6,7 +6,8 @@
 #include <ATen/native/Fill.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename scalar_t>
 struct FillFunctor {
@@ -26,4 +27,5 @@ void fill_kernel_cuda(TensorIterator& iter, const Scalar& value) {
 
 REGISTER_DISPATCH(fill_stub, &fill_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu b/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu
index f05d0f25783..463842dd9f3 100644
--- a/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu
+++ b/aten/src/ATen/native/cuda/ForeachBinaryOpList.cu
@@ -17,7 +17,8 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename T, template<class> class Op>
 std::vector<Tensor> foreach_tensor_list_op(TensorList tensors1, TensorList tensors2, const Scalar& alpha = 1) {
@@ -133,4 +134,5 @@ FOREACH_BINARY_OP_LIST(all_types_complex_bool_half_bfloat16, div, std::divides,
 FOREACH_BINARY_OP_LIST(all_types_half_bfloat16, clamp_max, minimum, /*division_op*/ false);
 FOREACH_BINARY_OP_LIST(all_types_half_bfloat16, clamp_min, maximum, /*division_op*/ false);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu b/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu
index b1e7d84008c..686075c9db9 100644
--- a/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu
+++ b/aten/src/ATen/native/cuda/ForeachBinaryOpScalar.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename T, template<class> class Op>
 std::vector<Tensor> foreach_binary_op(TensorList tensors, const Scalar& scalar) {
@@ -142,4 +143,5 @@ std::vector<Tensor> foreach_tensor_sub_scalar_kernel_cuda(TensorList tensors, co
 FOREACH_BINARY_OP_SCALAR(all_types_half_bfloat16, clamp_max, minimum, false);
 FOREACH_BINARY_OP_SCALAR(all_types_half_bfloat16, clamp_min, maximum, false);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu b/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu
index f0c7cacd044..dea4fc8c7b3 100644
--- a/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu
+++ b/aten/src/ATen/native/cuda/ForeachBinaryOpScalarList.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename T, template<class> class Op>
 std::vector<Tensor> foreach_binary_op(TensorList tensors, at::ArrayRef<Scalar> scalars) {
@@ -145,4 +146,5 @@ std::vector<Tensor> foreach_tensor_sub_scalarlist_kernel_cuda(TensorList tensors
 FOREACH_BINARY_OP_SCALARLIST(all_types_half_bfloat16, clamp_max, minimum, false);
 FOREACH_BINARY_OP_SCALARLIST(all_types_half_bfloat16, clamp_min, maximum, false);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu b/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu
index 8a95da39697..a84f667a43c 100644
--- a/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachPointwiseOp.cu
@@ -19,7 +19,8 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<template<class> class Op>
 std::vector<Tensor> foreach_pointwise_op(TensorList input, TensorList tensors1, TensorList tensors2, const Scalar& scalar) {
@@ -200,4 +201,5 @@ FOREACH_POINTWISE_OP_SCALARLIST(addcdiv, std::divides);
 FOREACH_POINTWISE_OP_TENSOR(addcdiv, std::divides);
 FOREACH_POINTWISE_OP_TENSOR(addcmul, std::multiplies);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachReduceOp.cu b/aten/src/ATen/native/cuda/ForeachReduceOp.cu
index 27d58e28cf1..175793f5e8f 100644
--- a/aten/src/ATen/native/cuda/ForeachReduceOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachReduceOp.cu
@@ -19,7 +19,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template<typename T, int NormType, int depth=1, int r_args_depth=1, int res_arg_index=0>
 struct LpNormFunctor {
@@ -185,4 +186,5 @@ std::vector<Tensor> foreach_tensor_norm_cuda(TensorList tensors, const Scalar& o
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachTernaryOp.cu b/aten/src/ATen/native/cuda/ForeachTernaryOp.cu
index 26d3ff2160d..e7cd4df78c9 100644
--- a/aten/src/ATen/native/cuda/ForeachTernaryOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachTernaryOp.cu
@@ -13,7 +13,8 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename T>
 struct LerpFunctor {
@@ -115,4 +116,5 @@ void foreach_tensor_lerp_list_cuda_(TensorList tensors1, TensorList tensors2, co
         }
   );
 }
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ForeachUnaryOp.cu b/aten/src/ATen/native/cuda/ForeachUnaryOp.cu
index 693a0d88b62..2468bb472bb 100644
--- a/aten/src/ATen/native/cuda/ForeachUnaryOp.cu
+++ b/aten/src/ATen/native/cuda/ForeachUnaryOp.cu
@@ -39,7 +39,8 @@
 #include <ATen/ops/empty_like_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, template<class> class Op> std::vector<Tensor> foreach_unary_op(TensorList tensors) {
     std::vector<std::vector<at::Tensor>> tensor_lists;
@@ -327,4 +328,5 @@ void foreach_tensor_zero_cuda_(TensorList tensors) {
     });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu b/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu
index df7a1ebdddf..e5fe7c4fe98 100644
--- a/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu
+++ b/aten/src/ATen/native/cuda/FractionalMaxPool2d.cu
@@ -24,7 +24,8 @@
 #include <cfloat>
 #include <cmath>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using namespace at::cuda::detail;
 
@@ -269,4 +270,5 @@ TORCH_IMPL_FUNC(fractional_max_pool2d_backward_cuda)(
   );
 }
 
-}// at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu b/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu
index 4384b07556a..ebfc2ac8f17 100644
--- a/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu
+++ b/aten/src/ATen/native/cuda/FractionalMaxPool3d.cu
@@ -27,7 +27,8 @@
 #include <cfloat>
 #include <cmath>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using namespace at::cuda::detail;
 
@@ -340,4 +341,5 @@ Tensor fractional_max_pool3d_backward_cuda(
     return gradInput;
  }
 
-}// namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu b/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu
index 683c9c058a3..6a1ba886cd1 100644
--- a/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu
+++ b/aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu
@@ -7,7 +7,8 @@
 #include <ATen/cuda/Atomic.cuh>
 #include <ATen/cuda/CUDAContext.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -111,4 +112,5 @@ void _compute_linear_combination_cuda_kernel(
 
 REGISTER_DISPATCH(_compute_linear_combination_stub, &_compute_linear_combination_cuda_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/FusedAdamKernel.cu b/aten/src/ATen/native/cuda/FusedAdamKernel.cu
index b8f514e0f1c..c5a51765dcb 100644
--- a/aten/src/ATen/native/cuda/FusedAdamKernel.cu
+++ b/aten/src/ATen/native/cuda/FusedAdamKernel.cu
@@ -6,7 +6,8 @@
 #include <c10/util/Exception.h>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // note(crcrpar): To observe the CI rules, i.e. 20 minutes per file to compile, defensively split instantiations into _impl files.
 // this is only for CUDA 11.3 for which it took about 20 minutes and 28 minutes in my workstation and CI, respectively.
@@ -42,4 +43,5 @@ void _fused_adam_kernel_cuda_(
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/FusedAdamWKernel.cu b/aten/src/ATen/native/cuda/FusedAdamWKernel.cu
index e11b82fafec..7808b1d9839 100644
--- a/aten/src/ATen/native/cuda/FusedAdamWKernel.cu
+++ b/aten/src/ATen/native/cuda/FusedAdamWKernel.cu
@@ -42,4 +42,4 @@ void _fused_adamw_kernel_cuda_(
   }
 }
 
-}} // namespace at::native
+}} // namespace at native
diff --git a/aten/src/ATen/native/cuda/GcdLcmKernel.cu b/aten/src/ATen/native/cuda/GcdLcmKernel.cu
index c4a8cdfaf1f..bdf39af9a4d 100644
--- a/aten/src/ATen/native/cuda/GcdLcmKernel.cu
+++ b/aten/src/ATen/native/cuda/GcdLcmKernel.cu
@@ -11,7 +11,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // See note [Jiterator]
 CONSTEXPR_EXCEPT_WIN_CUDA char gcd_name[] = "gcd";
@@ -55,4 +56,5 @@ void lcm_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(gcd_stub, &gcd_kernel_cuda);
 REGISTER_DISPATCH(lcm_stub, &lcm_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/GridSampler.cpp b/aten/src/ATen/native/cuda/GridSampler.cpp
index efe19edab44..5b43d998f86 100644
--- a/aten/src/ATen/native/cuda/GridSampler.cpp
+++ b/aten/src/ATen/native/cuda/GridSampler.cpp
@@ -14,7 +14,8 @@
 #include <ATen/ops/zeros_like.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor grid_sampler_2d_cuda(const Tensor& input, const Tensor& grid,
                             int64_t interpolation_mode, int64_t padding_mode,
@@ -79,4 +80,5 @@ grid_sampler_3d_backward_cuda(const Tensor& grad_output, const Tensor& input,
   return std::make_tuple(grad_input, grad_grid);
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/GridSampler.cu b/aten/src/ATen/native/cuda/GridSampler.cu
index e65f6d59c7d..0d7068e2cfb 100644
--- a/aten/src/ATen/native/cuda/GridSampler.cu
+++ b/aten/src/ATen/native/cuda/GridSampler.cu
@@ -13,7 +13,8 @@
 #include <c10/macros/Macros.h>
 #include <cmath>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using namespace at::cuda::detail;
 
@@ -950,4 +951,5 @@ void launch_grid_sampler_3d_backward_kernel(
   }
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/GridSampler.cuh b/aten/src/ATen/native/cuda/GridSampler.cuh
index a0e3b16c3a4..26c508901ba 100644
--- a/aten/src/ATen/native/cuda/GridSampler.cuh
+++ b/aten/src/ATen/native/cuda/GridSampler.cuh
@@ -318,4 +318,4 @@ void get_cubic_coefficients_grad(
 }
 
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/GridSampler.h b/aten/src/ATen/native/cuda/GridSampler.h
index aace9c30b0a..bdf92b82397 100644
--- a/aten/src/ATen/native/cuda/GridSampler.h
+++ b/aten/src/ATen/native/cuda/GridSampler.h
@@ -29,4 +29,4 @@ void launch_grid_sampler_3d_backward_kernel(
     const TensorBase &grid, int64_t interpolation_mode, int64_t padding_mode,
     bool align_corners, std::array<bool, 2> output_mask);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/IGammaKernel.cu b/aten/src/ATen/native/cuda/IGammaKernel.cu
index be3f7fc54a6..12f105093b9 100644
--- a/aten/src/ATen/native/cuda/IGammaKernel.cu
+++ b/aten/src/ATen/native/cuda/IGammaKernel.cu
@@ -531,7 +531,8 @@ struct CalcIgamma{
 
 // end of regularized lower & upper incomplete gamma
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void igamma_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES(iter.common_dtype(), "igamma_cuda", [&]() {
@@ -551,4 +552,5 @@ REGISTER_DISPATCH(igammac_stub, &igammac_kernel_cuda);
 // DO NOT ADD ANY NEW KERNELS HERE
 // CUDA compilation times grow quickly.  It's perfectly acceptable to have a file per kernel.
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Im2Col.cu b/aten/src/ATen/native/cuda/Im2Col.cu
index 1a79db8dcde..a209aa27646 100644
--- a/aten/src/ATen/native/cuda/Im2Col.cu
+++ b/aten/src/ATen/native/cuda/Im2Col.cu
@@ -20,7 +20,8 @@
 #include <ATen/ops/im2col_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 static void im2col_out_cuda_template(
@@ -162,4 +163,5 @@ Tensor im2col_cuda(
   return output;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/IndexKernel.cpp b/aten/src/ATen/native/cuda/IndexKernel.cpp
index b5e92a19770..36c431727c5 100644
--- a/aten/src/ATen/native/cuda/IndexKernel.cpp
+++ b/aten/src/ATen/native/cuda/IndexKernel.cpp
@@ -19,7 +19,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 static Tensor & masked_select_out_cuda_impl(Tensor & result, const Tensor & self, const Tensor & mask) {
   NoNamesGuard guard;
@@ -84,4 +85,5 @@ Tensor & masked_scatter__cuda(Tensor& self, const Tensor& mask, const Tensor& so
   return self;
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/IndexKernel.cu b/aten/src/ATen/native/cuda/IndexKernel.cu
index 670b27ebbab..529b5b9c1d2 100644
--- a/aten/src/ATen/native/cuda/IndexKernel.cu
+++ b/aten/src/ATen/native/cuda/IndexKernel.cu
@@ -16,7 +16,8 @@
 
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 static constexpr int launch_bound2 = 4;
 
@@ -469,4 +470,5 @@ REGISTER_DISPATCH(flip_stub, &flip_kernel);
 
 REGISTER_CUDA_DISPATCH(index_put_kernel_quantized_stub, &index_put_kernel_quantized_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Indexing.cu b/aten/src/ATen/native/cuda/Indexing.cu
index 85ff7c38057..208c1e85b52 100644
--- a/aten/src/ATen/native/cuda/Indexing.cu
+++ b/aten/src/ATen/native/cuda/Indexing.cu
@@ -184,7 +184,8 @@ __global__ void indexing_backward_kernel_quantized(
 }
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -1668,4 +1669,5 @@ Tensor index_select_sparse_cuda(const Tensor& self, int64_t dim, const Tensor& i
 }
 
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/JitLoops.cuh b/aten/src/ATen/native/cuda/JitLoops.cuh
index 6f350c550ce..774effacf6e 100644
--- a/aten/src/ATen/native/cuda/JitLoops.cuh
+++ b/aten/src/ATen/native/cuda/JitLoops.cuh
@@ -182,6 +182,6 @@ void opmath_jitted_gpu_kernel_with_scalars(TensorIteratorBase& iter, const std::
   }
 }
 
-}}  // at::native
+}} // at::native
 
 #endif // AT_USE_JITERATOR()
diff --git a/aten/src/ATen/native/cuda/KernelUtils.cuh b/aten/src/ATen/native/cuda/KernelUtils.cuh
index e1b9f380723..f81214f3dd9 100644
--- a/aten/src/ATen/native/cuda/KernelUtils.cuh
+++ b/aten/src/ATen/native/cuda/KernelUtils.cuh
@@ -1,7 +1,7 @@
 #pragma once
 #include <ATen/cuda/Atomic.cuh>
 
-#if !(defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))
+#if !(defined(USE_ROCM) || ((defined(CUDA_VERSION) && CUDA_VERSION < 11000) || (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800))))
 #include <cuda_bf16.h>
 #endif
 
@@ -38,6 +38,8 @@ __device__ __forceinline__ void fastSpecializedAtomicAdd(
     scalar_t value) {
 #if (                      \
     (defined(USE_ROCM)) || \
+     \
+    (defined(CUDA_VERSION) && (CUDA_VERSION < 10000)) || \
     (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)))
   gpuAtomicAddNoReturn(
       reinterpret_cast<at::Half*>(tensor) + index,
@@ -78,6 +80,7 @@ __device__ __forceinline__ void fastSpecializedAtomicAdd(
     scalar_t value) {
 #if (                      \
     (defined(USE_ROCM)) || \
+    (defined(CUDA_VERSION) && (CUDA_VERSION < 11000)) || \
     (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))
   gpuAtomicAddNoReturn(
       reinterpret_cast<at::BFloat16*>(tensor) + index,
diff --git a/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu b/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
index ffcb5e50339..c6378413e08 100644
--- a/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
+++ b/aten/src/ATen/native/cuda/LegacyThrustHelpers.cu
@@ -17,7 +17,8 @@
 #include <thrust/device_ptr.h>
 #include <thrust/iterator/constant_iterator.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void index_put_with_sort_kernel_thrust_helper(Tensor &linearIndex, Tensor &orig_indices, Tensor &sorted_indices, int64_t num_indices) {
   sorted_indices.copy_(linearIndex);
@@ -110,4 +111,5 @@ int64_t embedding_backward_cuda_kernel_unique_by_key<int>(const Tensor &sorted_i
 template
 int64_t embedding_backward_cuda_kernel_unique_by_key<int64_t>(const Tensor &sorted_indices, Tensor &segment_offsets);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Lerp.cu b/aten/src/ATen/native/cuda/Lerp.cu
index 01053a3beea..b3a54c60823 100644
--- a/aten/src/ATen/native/cuda/Lerp.cu
+++ b/aten/src/ATen/native/cuda/Lerp.cu
@@ -6,7 +6,8 @@
 #include <ATen/native/cuda/JitLoops.cuh>
 #include <ATen/OpMathType.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char lerp_tensor_name[] = "lerp_tensor";
@@ -124,4 +125,5 @@ void lerp_scalar_kernel(at::TensorIteratorBase& iter, const c10::Scalar& weight)
 REGISTER_DISPATCH(lerp_kernel_tensor_weight, &lerp_tensor_kernel);
 REGISTER_DISPATCH(lerp_kernel_scalar_weight, &lerp_scalar_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/LinearAlgebra.cu b/aten/src/ATen/native/cuda/LinearAlgebra.cu
index fb59f976041..78d63d35cd8 100644
--- a/aten/src/ATen/native/cuda/LinearAlgebra.cu
+++ b/aten/src/ATen/native/cuda/LinearAlgebra.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/ReduceOps.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -139,4 +140,5 @@ void unpack_pivots_cuda_kernel(TensorIterator& iter, const int64_t dim_size, con
 
 REGISTER_DISPATCH(unpack_pivots_stub, &unpack_pivots_cuda_kernel);
 REGISTER_DISPATCH(addr_stub, &addr_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp b/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp
index 045bfa8d1f9..e171db8bda3 100644
--- a/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp
+++ b/aten/src/ATen/native/cuda/LinearAlgebraStubs.cpp
@@ -29,7 +29,8 @@ struct MagmaInitializer {
 }  // namespace (anonymous)
 #endif
 #endif
-namespace at::native {
+namespace at {
+namespace native {
 #if defined(BUILD_LAZY_CUDA_LINALG)
 namespace {
 cuda::detail::LinalgDispatch disp = {_cholesky_solve_helper_cuda};
@@ -175,4 +176,5 @@ Tensor _cholesky_solve_helper_cuda(const Tensor& self, const Tensor& A, bool upp
 
 #endif /*defined(BUILD_LAZY_CUDA_LINALG)*/
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/LogAddExpKernel.cu b/aten/src/ATen/native/cuda/LogAddExpKernel.cu
index c167ef698d3..a3c7662efa8 100644
--- a/aten/src/ATen/native/cuda/LogAddExpKernel.cu
+++ b/aten/src/ATen/native/cuda/LogAddExpKernel.cu
@@ -10,7 +10,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void logaddexp_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND2(
@@ -54,4 +55,5 @@ void logaddexp2_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(logaddexp_stub, &logaddexp_kernel_cuda);
 REGISTER_DISPATCH(logaddexp2_stub, &logaddexp2_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu b/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu
index ea4188c970c..d1f284131c3 100644
--- a/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu
+++ b/aten/src/ATen/native/cuda/LogcumsumexpKernel.cu
@@ -9,7 +9,8 @@
 #include <cmath>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // custom min and max to be used in logcumsumexp for complex arguments
 template <typename scalar_t, bool min>
@@ -116,4 +117,5 @@ void launch_logcumsumexp_cuda_kernel(const TensorBase& result, const TensorBase&
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Loops.cuh b/aten/src/ATen/native/cuda/Loops.cuh
index af1eeaa5349..c77b876ee78 100644
--- a/aten/src/ATen/native/cuda/Loops.cuh
+++ b/aten/src/ATen/native/cuda/Loops.cuh
@@ -66,7 +66,7 @@ __device__ inline void elementwise_kernel_helper(func_t f, policy_t policy) {
   policy.store(results, idx);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 // Note:
 // CUDA and ROCm get diverged in this PR:
@@ -310,4 +310,4 @@ void gpu_kernel_multiple_outputs(TensorIteratorBase& iter, const func_t& f) {
   gpu_kernel_multiple_outputs_impl(iter, f);
 }
 
-}} //namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/Loss.cu b/aten/src/ATen/native/cuda/Loss.cu
index c4f1f064614..72d4a5c7c30 100644
--- a/aten/src/ATen/native/cuda/Loss.cu
+++ b/aten/src/ATen/native/cuda/Loss.cu
@@ -58,7 +58,8 @@ void binary_cross_entropy_backward_out_kernel(Tensor& grad_input, const Tensor&
 
 } // namespace
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor binary_cross_entropy_cuda(const Tensor& input, const Tensor& target, const c10::optional<Tensor>& weight_opt, int64_t reduction) {
   // See [Note: hacky wrapper removal for optional tensor]
@@ -611,4 +612,5 @@ TORCH_IMPL_FUNC(nll_loss_backward_out_cuda)
       reduction,
       ignore_index);
 }
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/LossCTC.cu b/aten/src/ATen/native/cuda/LossCTC.cu
index bb70b8313e7..97ae652b629 100644
--- a/aten/src/ATen/native/cuda/LossCTC.cu
+++ b/aten/src/ATen/native/cuda/LossCTC.cu
@@ -36,7 +36,8 @@
 #include <type_traits>
 #include <numeric>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -780,4 +781,5 @@ Tensor ctc_loss_backward_gpu(const Tensor& grad, const Tensor& log_probs, const
     });
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu b/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu
index 51c82e95213..5f22146bdfc 100644
--- a/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu
+++ b/aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu
@@ -9,7 +9,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void maximum_kernel_cuda(TensorIteratorBase& iter) {
   if (iter.dtype() == ScalarType::Bool) {
@@ -95,4 +96,5 @@ REGISTER_DISPATCH(minimum_stub, &minimum_kernel_cuda);
 REGISTER_DISPATCH(fmax_stub, &fmax_kernel_cuda);
 REGISTER_DISPATCH(fmin_stub, &fmin_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/MaxUnpooling.cu b/aten/src/ATen/native/cuda/MaxUnpooling.cu
index 3eedfe6ba9e..ba1a7eb1f5c 100644
--- a/aten/src/ATen/native/cuda/MaxUnpooling.cu
+++ b/aten/src/ATen/native/cuda/MaxUnpooling.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/empty_like.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using namespace at::cuda::detail;
 
@@ -609,4 +610,5 @@ at::Tensor max_unpooling3d_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu b/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu
index debe1bd6180..7f61d9a0b5b 100644
--- a/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu
+++ b/aten/src/ATen/native/cuda/MultiLabelMarginCriterion.cu
@@ -18,7 +18,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 const int MULTILABELMARGIN_THREADS = 128;
@@ -440,4 +441,5 @@ Tensor multilabel_margin_loss_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/MultiMarginLoss.cu b/aten/src/ATen/native/cuda/MultiMarginLoss.cu
index 96f41357f25..b5ca932f2db 100644
--- a/aten/src/ATen/native/cuda/MultiMarginLoss.cu
+++ b/aten/src/ATen/native/cuda/MultiMarginLoss.cu
@@ -16,7 +16,8 @@
 #include <ATen/ops/multi_margin_loss_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 constexpr int MULTIMARGIN_THREADS = 128;
 
@@ -391,4 +392,5 @@ Tensor multi_margin_loss_cuda_backward(
   return grad_input;
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/MultinomialKernel.cu b/aten/src/ATen/native/cuda/MultinomialKernel.cu
index 690b8e44439..5e52d12d2eb 100644
--- a/aten/src/ATen/native/cuda/MultinomialKernel.cu
+++ b/aten/src/ATen/native/cuda/MultinomialKernel.cu
@@ -26,7 +26,8 @@
 #include <curand_kernel.h>
 #include <curand_philox4x32_x.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -457,4 +458,5 @@ void multinomial_with_replacement_kernel_impl(
 REGISTER_DISPATCH(
     multinomial_with_replacement_stub,
     &multinomial_with_replacement_kernel_impl);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/NLLLoss2d.cu b/aten/src/ATen/native/cuda/NLLLoss2d.cu
index ba98f18427d..d3f12846252 100644
--- a/aten/src/ATen/native/cuda/NLLLoss2d.cu
+++ b/aten/src/ATen/native/cuda/NLLLoss2d.cu
@@ -22,7 +22,8 @@
 #include <ATen/ops/nll_loss2d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -525,4 +526,5 @@ Tensor nll_loss2d_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu
index e8b4122092e..75b4e335754 100644
--- a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu
+++ b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu
@@ -23,7 +23,8 @@
 #include <ATen/ops/slow_conv_transpose2d_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 static inline void slow_conv_transpose2d_shape_check(
@@ -830,4 +831,5 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv_transpose2d_backward_cuda(
 
 REGISTER_CUDA_DISPATCH(slow_conv_transpose2d_backward_stub, &slow_conv_transpose2d_backward_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu
index c5067c5b1e1..0ed107f2db1 100644
--- a/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu
+++ b/aten/src/ATen/native/cuda/NaiveConvolutionTranspose3d.cu
@@ -22,7 +22,8 @@
 #include <ATen/ops/slow_conv_transpose3d_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 static inline void slow_conv_transpose3d_shape_check(
@@ -1013,4 +1014,5 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv_transpose3d_backward_cuda(
 
 REGISTER_CUDA_DISPATCH(slow_conv_transpose3d_backward_stub, &slow_conv_transpose3d_backward_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu b/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu
index 1ef3268ddac..6c2942b05de 100644
--- a/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu
+++ b/aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu
@@ -22,7 +22,8 @@
 
 #include <tuple>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -611,4 +612,5 @@ std::tuple<Tensor, Tensor, Tensor> slow_conv_dilated3d_backward_cuda(
 REGISTER_CUDA_DISPATCH(slow_conv_dilated2d_backward_stub, &slow_conv_dilated2d_backward_cuda);
 REGISTER_CUDA_DISPATCH(slow_conv_dilated3d_backward_stub, &slow_conv_dilated3d_backward_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Nonzero.cu b/aten/src/ATen/native/cuda/Nonzero.cu
index 7cd6392bbfd..5f4367d07ad 100644
--- a/aten/src/ATen/native/cuda/Nonzero.cu
+++ b/aten/src/ATen/native/cuda/Nonzero.cu
@@ -16,7 +16,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace{
 template<typename T>
@@ -127,4 +128,5 @@ Tensor nonzero_cuda(const Tensor& self){
   Tensor out = at::detail::empty_cuda({0}, self.options().dtype(kLong));
   return at::native::nonzero_out_cuda(self, out);
 }
-} //namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Normalization.cu b/aten/src/ATen/native/cuda/Normalization.cu
index cd3f4543f29..ff3c54c9c59 100644
--- a/aten/src/ATen/native/cuda/Normalization.cu
+++ b/aten/src/ATen/native/cuda/Normalization.cu
@@ -26,7 +26,8 @@
 #include <ATen/ops/scalar_tensor.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -736,4 +737,5 @@ std::tuple<Tensor, Tensor> batch_norm_update_stats_cuda(
   return std::tuple<Tensor, Tensor>(save_mean, save_var);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu b/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu
index 53b67125222..2b4d6690348 100644
--- a/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu
+++ b/aten/src/ATen/native/cuda/PointwiseOpsKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/PointwiseOps.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char addcmul_name[] = "addcmul";
 void addcmul_cuda_kernel(TensorIteratorBase& iter, const Scalar& value) {
@@ -148,4 +149,5 @@ REGISTER_DISPATCH(addcmul_stub, &addcmul_cuda_kernel);
 REGISTER_DISPATCH(smooth_l1_backward_stub, &smooth_l1_backward_cuda_kernel);
 REGISTER_DISPATCH(huber_backward_stub, &huber_backward_cuda_kernel);
 REGISTER_DISPATCH(mse_backward_stub, &mse_backward_cuda_kernel);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/PowKernel.cu b/aten/src/ATen/native/cuda/PowKernel.cu
index eb56da722fb..dcc70fdb088 100644
--- a/aten/src/ATen/native/cuda/PowKernel.cu
+++ b/aten/src/ATen/native/cuda/PowKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/Pow.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // Forward declare some unary kernels
 void rsqrt_kernel_cuda(TensorIteratorBase& iter);
@@ -206,4 +207,5 @@ void pow_tensor_scalar_kernel(TensorIteratorBase& iter, const Scalar& exp_scalar
 REGISTER_DISPATCH(pow_tensor_tensor_stub, &pow_tensor_tensor_kernel);
 REGISTER_DISPATCH(pow_tensor_scalar_stub, &pow_tensor_scalar_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/RNN.cu b/aten/src/ATen/native/cuda/RNN.cu
index ef384268c81..f3f4c30693c 100644
--- a/aten/src/ATen/native/cuda/RNN.cu
+++ b/aten/src/ATen/native/cuda/RNN.cu
@@ -19,7 +19,8 @@
 #include <ATen/ops/_thnn_fused_gru_cell_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -646,4 +647,5 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor> _thnn_fused_gru_cell_backward
   return std::make_tuple(grad_input_gates, grad_hidden_gates, grad_hx, grad_input_bias, grad_hidden_bias);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Randperm.cu b/aten/src/ATen/native/cuda/Randperm.cu
index 46e14f9f2ea..cfd1fd1ebcc 100644
--- a/aten/src/ATen/native/cuda/Randperm.cu
+++ b/aten/src/ATen/native/cuda/Randperm.cu
@@ -18,7 +18,8 @@
 
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // [Algorithm of randperm]
 //
@@ -130,4 +131,5 @@ Tensor& randperm_out_cuda(int64_t n, c10::optional<Generator> generator, Tensor&
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/RangeFactories.cu b/aten/src/ATen/native/cuda/RangeFactories.cu
index 83a803d2185..4ad424bdd9a 100644
--- a/aten/src/ATen/native/cuda/RangeFactories.cu
+++ b/aten/src/ATen/native/cuda/RangeFactories.cu
@@ -67,7 +67,8 @@ void gpu_kernel_with_index(at::Tensor &output, func_t f) {
 
 }  // namespace
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor& linspace_cuda_out(const Scalar& start, const Scalar& end, int64_t steps, Tensor& result) {
   TORCH_CHECK(steps >= 0, "number of steps must be non-negative");
@@ -271,4 +272,5 @@ Tensor& arange_cuda_out(const Scalar& start, const Scalar& end, const Scalar& st
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/RecordStream.cu b/aten/src/ATen/native/cuda/RecordStream.cu
index cc7fc20d433..81bf789afce 100644
--- a/aten/src/ATen/native/cuda/RecordStream.cu
+++ b/aten/src/ATen/native/cuda/RecordStream.cu
@@ -8,9 +8,11 @@
 #include <ATen/ops/record_stream_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 void record_stream_cuda(Tensor& self, c10::Stream stream) {
   struct c10::StreamData3 data = stream.pack3();
   c10::cuda::CUDACachingAllocator::recordStream(self.storage().data_ptr(), at::cuda::CUDAStream::unpack3(data.stream_id, data.device_index, data.device_type));
 }
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Reduce.cu b/aten/src/ATen/native/cuda/Reduce.cu
index 36a13134882..d6a676a8ac9 100644
--- a/aten/src/ATen/native/cuda/Reduce.cu
+++ b/aten/src/ATen/native/cuda/Reduce.cu
@@ -5,7 +5,8 @@
 #include <iostream>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 static inline std::ostream& operator<<(std::ostream& out, dim3 dim) {
   if (dim.y == 1 && dim.z == 1) {
@@ -53,4 +54,5 @@ std::ostream& operator<<(std::ostream& out, const ReduceConfig& config) {
   return out;
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu b/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu
index cdd5daab2d9..292404cb36a 100644
--- a/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceAMinMaxKernel.cu
@@ -15,7 +15,8 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t>
 void _min_max_values_kernel_cuda_impl(TensorIterator& iter) {
@@ -46,4 +47,5 @@ void aminmax_launch_kernel(TensorIterator& iter) {
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu b/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu
index c5d763f3135..fd8e071cd5c 100644
--- a/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceArgMaxKernel.cu
@@ -15,7 +15,8 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, typename acc_t = scalar_t>
 void argmax_kernel_cuda_impl(TensorIterator& iter) {
@@ -43,4 +44,5 @@ void argmax_kernel_cuda(TensorIterator& iter) {
 
 REGISTER_DISPATCH(argmax_stub, &argmax_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu b/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu
index fc34c11c519..20eb736e494 100644
--- a/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceArgMinKernel.cu
@@ -15,7 +15,8 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, typename acc_t = scalar_t>
 void argmin_kernel_cuda_impl(TensorIterator& iter) {
@@ -43,4 +44,5 @@ void argmin_kernel_cuda(TensorIterator& iter) {
 
 REGISTER_DISPATCH(argmin_stub, &argmin_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceLogicKernel.cu b/aten/src/ATen/native/cuda/ReduceLogicKernel.cu
index 3f65c745d7a..694a3f1a961 100644
--- a/aten/src/ATen/native/cuda/ReduceLogicKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceLogicKernel.cu
@@ -6,7 +6,8 @@
 #include <ATen/native/ReduceOps.h>
 #include <ATen/Dispatch.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void and_kernel_cuda(TensorIterator& iter) {
   AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND3(
@@ -35,4 +36,5 @@ void or_kernel_cuda(TensorIterator& iter) {
 REGISTER_DISPATCH(and_stub, &and_kernel_cuda);
 REGISTER_DISPATCH(or_stub, &or_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu b/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu
index 883e8fe2149..a5363838ee2 100644
--- a/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceMaxValuesKernel.cu
@@ -15,7 +15,8 @@
 #include <ATen/NumericUtils.h>
 #include <ATen/cuda/NumericLimits.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename acc_t>
 struct MaxNanFunctor {
@@ -58,4 +59,5 @@ void max_all_launch_kernel(TensorIterator &iter) {
 
 REGISTER_DISPATCH(max_values_stub, &max_values_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu b/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu
index a0ccf873be0..1af1675672f 100644
--- a/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceMinValuesKernel.cu
@@ -16,7 +16,8 @@
 #include <ATen/cuda/NumericLimits.cuh>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename acc_t>
 struct MinNanFunctor {
@@ -55,4 +56,5 @@ void min_all_launch_kernel(TensorIterator &iter) {
 
 REGISTER_DISPATCH(min_values_stub, &min_values_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceMomentKernel.cu b/aten/src/ATen/native/cuda/ReduceMomentKernel.cu
index 980f7fa5c36..7024f2dd9ec 100644
--- a/aten/src/ATen/native/cuda/ReduceMomentKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceMomentKernel.cu
@@ -8,7 +8,8 @@
 #include <ATen/Dispatch.h>
 #include <ATen/native/ReduceOps.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, typename out_t=scalar_t>
 void std_var_kernel_impl(TensorIterator& iter, int32_t correction, bool take_sqrt) {
@@ -70,4 +71,5 @@ static void mean_kernel_cuda(TensorIterator& iter) {
 REGISTER_DISPATCH(std_var_stub, &std_var_kernel_cuda);
 REGISTER_DISPATCH(mean_stub, &mean_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceNormKernel.cu b/aten/src/ATen/native/cuda/ReduceNormKernel.cu
index 5ad037f6618..ca49185f598 100644
--- a/aten/src/ATen/native/cuda/ReduceNormKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceNormKernel.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/LinearAlgebra.h>
 #include <c10/core/Scalar.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // This reduction accumulates results as the type `acc_t`. By default, when
 // `scalar_t` is complex, `acc_t` is the downgraded real number type.
@@ -48,4 +49,5 @@ void norm_launch_kernel(TensorIterator& iter, double ord) {
   });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceOps.cpp b/aten/src/ATen/native/cuda/ReduceOps.cpp
index d4ed5f7c010..7d3e2c9c49d 100644
--- a/aten/src/ATen/native/cuda/ReduceOps.cpp
+++ b/aten/src/ATen/native/cuda/ReduceOps.cpp
@@ -24,7 +24,8 @@
 #include <ATen/ops/where.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void norm_kernel_cuda(TensorIterator& iter, const Scalar& val) {
@@ -97,4 +98,5 @@ REGISTER_CUDA_DISPATCH(aminmax_stub, &aminmax_kernel_impl);
 
 REGISTER_CUDA_DISPATCH(norm_stub, &norm_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReduceOps.h b/aten/src/ATen/native/cuda/ReduceOps.h
index a67a019ae49..8bac9312f6c 100644
--- a/aten/src/ATen/native/cuda/ReduceOps.h
+++ b/aten/src/ATen/native/cuda/ReduceOps.h
@@ -17,4 +17,4 @@ void min_all_launch_kernel(TensorIterator &iter);
 void max_all_launch_kernel(TensorIterator &iter);
 void aminmax_allreduce_launch_kernel(TensorIterator &iter);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu b/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
index cf2f5064d36..6ec92583373 100644
--- a/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
+++ b/aten/src/ATen/native/cuda/ReduceSumProdKernel.cu
@@ -8,7 +8,8 @@
 #include <ATen/jit_macros.h>
 #include <ATen/OpMathType.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, typename acc_t = scalar_t, typename out_t = scalar_t>
 struct sum_functor {
@@ -184,4 +185,5 @@ REGISTER_DISPATCH(sum_stub, &sum_kernel_cuda);
 REGISTER_DISPATCH(nansum_stub, &nansum_kernel_cuda);
 REGISTER_DISPATCH(prod_stub, &prod_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReflectionPad.cu b/aten/src/ATen/native/cuda/ReflectionPad.cu
index c798d73fe89..5380b0fef5f 100644
--- a/aten/src/ATen/native/cuda/ReflectionPad.cu
+++ b/aten/src/ATen/native/cuda/ReflectionPad.cu
@@ -24,7 +24,8 @@
 
 #include <thrust/pair.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 using at::cuda::detail::canUse32BitIndexMath;
@@ -677,4 +678,5 @@ TORCH_IMPL_FUNC(reflection_pad3d_backward_out_cuda) (
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/RenormKernel.cu b/aten/src/ATen/native/cuda/RenormKernel.cu
index ef133761aed..516360a2e1d 100644
--- a/aten/src/ATen/native/cuda/RenormKernel.cu
+++ b/aten/src/ATen/native/cuda/RenormKernel.cu
@@ -5,7 +5,8 @@
 
 #include <ATen/Dispatch.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
@@ -26,4 +27,5 @@ void renorm_scale_factor_impl(TensorIteratorBase& iter, double maxnorm) {
 
 REGISTER_DISPATCH(renorm_scale_factor_stub, &renorm_scale_factor_impl);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Repeat.cu b/aten/src/ATen/native/cuda/Repeat.cu
index 65c6863745c..1b29dac6690 100644
--- a/aten/src/ATen/native/cuda/Repeat.cu
+++ b/aten/src/ATen/native/cuda/Repeat.cu
@@ -50,7 +50,8 @@ static void compute_cuda(
   C10_CUDA_KERNEL_LAUNCH_CHECK();
 }
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor repeat_interleave_cuda(
     const Tensor& repeat,
@@ -64,4 +65,5 @@ Tensor repeat_interleave_cuda(
   return output;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ReplicationPadding.cu b/aten/src/ATen/native/cuda/ReplicationPadding.cu
index 3ebd7f733b3..73650337147 100644
--- a/aten/src/ATen/native/cuda/ReplicationPadding.cu
+++ b/aten/src/ATen/native/cuda/ReplicationPadding.cu
@@ -27,7 +27,8 @@
 #include <cmath>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 __host__ __device__ __forceinline__ int imin(int a, int b) {
   return a > b ? b : a;
 }
@@ -749,4 +750,5 @@ Tensor replication_pad3d_backward_cuda(
   return gradInput;
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Resize.cpp b/aten/src/ATen/native/cuda/Resize.cpp
index 89e5b286461..43e1cb95157 100644
--- a/aten/src/ATen/native/cuda/Resize.cpp
+++ b/aten/src/ATen/native/cuda/Resize.cpp
@@ -11,7 +11,8 @@
 #include <ATen/ops/resize_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void resize_bytes_cuda(StorageImpl* storage, size_t size_bytes) {
   TORCH_CHECK(storage->resizable(), "Trying to resize storage that is not resizable");
@@ -65,4 +66,5 @@ const Tensor& resize_cuda_(
   }
   return self;
 }
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/RreluWithNoise.cu b/aten/src/ATen/native/cuda/RreluWithNoise.cu
index 5184f29562f..9088122a905 100644
--- a/aten/src/ATen/native/cuda/RreluWithNoise.cu
+++ b/aten/src/ATen/native/cuda/RreluWithNoise.cu
@@ -14,7 +14,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, int unroll_factor, typename F>
 #if __CUDA_ARCH__ >= 350 || defined USE_ROCM
@@ -192,4 +193,5 @@ Tensor& rrelu_with_noise_cuda_(
       self, noise, lower, upper, training, generator, self);
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ScanKernels.cpp b/aten/src/ATen/native/cuda/ScanKernels.cpp
index 463ceb23bad..9b718cec4d5 100644
--- a/aten/src/ATen/native/cuda/ScanKernels.cpp
+++ b/aten/src/ATen/native/cuda/ScanKernels.cpp
@@ -16,7 +16,8 @@
 #include <ATen/ops/empty_like.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 static c10::MaybeOwned<Tensor> contiguous_out_arg(const Tensor &tensor) {
   if (tensor.is_contiguous()) {
@@ -112,4 +113,5 @@ void cumprod_cuda_kernel(const Tensor& result, const Tensor& self, int64_t dim)
 REGISTER_CUDA_DISPATCH(cumsum_stub, &cumsum_cuda_kernel);
 REGISTER_CUDA_DISPATCH(cumprod_stub, &cumprod_cuda_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ScanKernels.h b/aten/src/ATen/native/cuda/ScanKernels.h
index 28e65372511..fe6b4c1a871 100644
--- a/aten/src/ATen/native/cuda/ScanKernels.h
+++ b/aten/src/ATen/native/cuda/ScanKernels.h
@@ -15,4 +15,4 @@ void launch_logcumsumexp_cuda_kernel(const TensorBase& result, const TensorBase&
 void launch_cumsum_cuda_kernel(const TensorBase& result, const TensorBase& self, int64_t dim);
 void launch_cumprod_cuda_kernel(const TensorBase& result, const TensorBase& self, int64_t dim);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ScanUtils.cuh b/aten/src/ATen/native/cuda/ScanUtils.cuh
index ba27a245172..0064c014db0 100644
--- a/aten/src/ATen/native/cuda/ScanUtils.cuh
+++ b/aten/src/ATen/native/cuda/ScanUtils.cuh
@@ -449,4 +449,4 @@ void scan_dim(const TensorBase& self, const TensorBase& result,
   }
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/ScatterGatherKernel.cu b/aten/src/ATen/native/cuda/ScatterGatherKernel.cu
index 55ff12eb0b0..2bdeff11a4e 100644
--- a/aten/src/ATen/native/cuda/ScatterGatherKernel.cu
+++ b/aten/src/ATen/native/cuda/ScatterGatherKernel.cu
@@ -15,7 +15,8 @@
 #include <ATen/cuda/Atomic.cuh>
 #include <ATen/cuda/CUDAContext.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // Implement as functors since lambdas don't get optimized.
 class ReduceMultiply {
@@ -563,4 +564,5 @@ REGISTER_DISPATCH(scatter_reduce_stub, &scatter_reduce_cuda_kernel);
 REGISTER_DISPATCH(scatter_scalar_reduce_stub, &scatter_scalar_reduce_cuda_kernel);
 REGISTER_DISPATCH(scatter_reduce_two_stub, &scatter_reduce_two_cuda_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/SegmentReduce.cu b/aten/src/ATen/native/cuda/SegmentReduce.cu
index 1917666c48b..2bacace1145 100644
--- a/aten/src/ATen/native/cuda/SegmentReduce.cu
+++ b/aten/src/ATen/native/cuda/SegmentReduce.cu
@@ -17,7 +17,8 @@
 #include <ATen/ops/cumsum.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 struct CustomMax {
@@ -619,4 +620,5 @@ REGISTER_DISPATCH(
   _segment_reduce_offsets_backward_stub,
   &_segment_reduce_offsets_backward_cuda_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Shape.cu b/aten/src/ATen/native/cuda/Shape.cu
index feb1fd44a24..389515eac1e 100644
--- a/aten/src/ATen/native/cuda/Shape.cu
+++ b/aten/src/ATen/native/cuda/Shape.cu
@@ -21,7 +21,8 @@
 #include <ATen/ops/narrow.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 constexpr int CAT_ARRAY_BATCH_SIZE = 128;
 constexpr int CAT_ARRAY_MAX_INPUT_DIMS = 4;
@@ -320,4 +321,5 @@ TORCH_IMPL_FUNC(cat_out_cuda)
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/SoftMax.cu b/aten/src/ATen/native/cuda/SoftMax.cu
index 08050fb24f6..603749584d3 100644
--- a/aten/src/ATen/native/cuda/SoftMax.cu
+++ b/aten/src/ATen/native/cuda/SoftMax.cu
@@ -28,7 +28,8 @@
 #include <ATen/ops/_softmax_backward_data.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -1113,4 +1114,5 @@ Tensor masked_softmax_backward_cuda(
   return grad_input;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Sort.cpp b/aten/src/ATen/native/cuda/Sort.cpp
index 87475923e51..a488188506f 100644
--- a/aten/src/ATen/native/cuda/Sort.cpp
+++ b/aten/src/ATen/native/cuda/Sort.cpp
@@ -21,7 +21,8 @@
 
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 std::vector<int64_t> infer_dense_strides_dim_last(const Tensor & self, int64_t dim);
 
@@ -124,4 +125,5 @@ void sort_cuda_kernel(
 // NOLINTNEXTLINE(cppcoreguidelines-avoid-non-const-global-variables)
 REGISTER_CUDA_DISPATCH(sort_stub, &sort_cuda_kernel);
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Sort.cu b/aten/src/ATen/native/cuda/Sort.cu
index cb66b6571c5..94386330c4f 100644
--- a/aten/src/ATen/native/cuda/Sort.cu
+++ b/aten/src/ATen/native/cuda/Sort.cu
@@ -13,7 +13,8 @@
 #include <limits>
 #include <c10/core/DeviceArray.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename T>
 static int minimum_grid_for_occupancy(T kernel, int max_block_size) {
@@ -280,4 +281,5 @@ void sortKeyValueInplace(
   }
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Sort.h b/aten/src/ATen/native/cuda/Sort.h
index 656b4ce2c2b..0baed1ea68c 100644
--- a/aten/src/ATen/native/cuda/Sort.h
+++ b/aten/src/ATen/native/cuda/Sort.h
@@ -14,4 +14,4 @@ void sortKeyValueInplace(
     const TensorBase &key, const TensorBase &value, int dim,
     bool descending, bool stable=false);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SortImpl.cu b/aten/src/ATen/native/cuda/SortImpl.cu
index 5d779d0fd15..79d905e01b3 100644
--- a/aten/src/ATen/native/cuda/SortImpl.cu
+++ b/aten/src/ATen/native/cuda/SortImpl.cu
@@ -3,7 +3,8 @@
 #include <thrust/execution_policy.h>
 #include <thrust/sort.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 std::vector<int64_t> infer_dense_strides_dim_last(const Tensor & self, int64_t dim) {
   int64_t ndim = self.dim();
@@ -34,4 +35,5 @@ std::vector<int64_t> infer_dense_strides_dim_last(const Tensor & self, int64_t d
   return new_strides_unsort;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/SortStable.cu b/aten/src/ATen/native/cuda/SortStable.cu
index 008e1a08f45..7890f44a6e2 100644
--- a/aten/src/ATen/native/cuda/SortStable.cu
+++ b/aten/src/ATen/native/cuda/SortStable.cu
@@ -15,7 +15,8 @@
 #include <c10/core/DeviceArray.h>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -295,4 +296,5 @@ void launch_stable_sort_kernel(
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Sorting.cpp b/aten/src/ATen/native/cuda/Sorting.cpp
index 9381c0e4f02..97b8df55416 100644
--- a/aten/src/ATen/native/cuda/Sorting.cpp
+++ b/aten/src/ATen/native/cuda/Sorting.cpp
@@ -23,7 +23,8 @@
 #include <ATen/ops/where.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 std::tuple<Tensor&, Tensor&> kthvalue_out_impl_cuda(
@@ -204,4 +205,5 @@ Tensor nanmedian_cuda(const Tensor& self) {
   return median_impl(self, /*ignore_nan=*/true);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Sorting.cu b/aten/src/ATen/native/cuda/Sorting.cu
index 313c6d1ea98..52fa2710596 100644
--- a/aten/src/ATen/native/cuda/Sorting.cu
+++ b/aten/src/ATen/native/cuda/Sorting.cu
@@ -15,7 +15,8 @@
 #include <cassert>
 #include <cstdlib>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -277,4 +278,5 @@ void launch_median_kernel(
       });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Sorting.h b/aten/src/ATen/native/cuda/Sorting.h
index bd10ffb1a02..82ba11b95f7 100644
--- a/aten/src/ATen/native/cuda/Sorting.h
+++ b/aten/src/ATen/native/cuda/Sorting.h
@@ -15,4 +15,4 @@ void launch_median_kernel(
     const TensorBase &vals, const TensorBase &inds,
     const TensorBase &in, int64_t dim, bool ignore_nan);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu b/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu
index ead1ff6326e..b365ccd1de3 100644
--- a/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu
+++ b/aten/src/ATen/native/cuda/SparseBinaryOpIntersectionKernel.cu
@@ -5,7 +5,8 @@
 #include <ATen/native/cuda/KernelUtils.cuh>
 #include <ATen/cuda/detail/OffsetCalculator.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -172,4 +173,5 @@ void sparse_mask_intersection_out_cuda_kernel(
 REGISTER_CUDA_DISPATCH(mul_sparse_sparse_out_stub, &mul_sparse_sparse_out_cuda_kernel);
 REGISTER_CUDA_DISPATCH(sparse_mask_intersection_out_stub, &sparse_mask_intersection_out_cuda_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/SparseMM.cu b/aten/src/ATen/native/cuda/SparseMM.cu
index 78bc554b52e..41b5023b9e5 100644
--- a/aten/src/ATen/native/cuda/SparseMM.cu
+++ b/aten/src/ATen/native/cuda/SparseMM.cu
@@ -8,7 +8,8 @@
 #include <ATen/ops/sspaddmm_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 // sparse, sparse, sparse, dense, real, real -> sparse
 Tensor& _sspaddmm_out_only_sparse_cuda(const Tensor& self,
     const Tensor& mat1, const Tensor& mat2, const Scalar& beta, const Scalar& alpha, Tensor& result) {
@@ -18,4 +19,5 @@ Tensor& _sspaddmm_out_cuda(const Tensor& self,
     const Tensor& mat1, const Tensor& mat2, const Scalar& beta, const Scalar& alpha, Tensor& result) {
   AT_ERROR("NYI: CUDA sspaddmm is not implemented");
 }
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/SpectralOps.cpp b/aten/src/ATen/native/cuda/SpectralOps.cpp
index 80bbbcbd89d..28d431dbad6 100644
--- a/aten/src/ATen/native/cuda/SpectralOps.cpp
+++ b/aten/src/ATen/native/cuda/SpectralOps.cpp
@@ -30,7 +30,8 @@
 #include <vector>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using namespace at::native::detail;
 
@@ -534,4 +535,5 @@ Tensor& _fft_c2c_cufft_out(const Tensor& self, IntArrayRef dim,
 }
 
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/SpectralOps.cu b/aten/src/ATen/native/cuda/SpectralOps.cu
index fac39dca008..91255c39638 100644
--- a/aten/src/ATen/native/cuda/SpectralOps.cu
+++ b/aten/src/ATen/native/cuda/SpectralOps.cu
@@ -11,7 +11,8 @@
 #include <vector>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // Offset calculator for indexing in Hermitian mirrored order.
 // In mirrored dims, maps linear index i to (n - i) % n
@@ -121,4 +122,5 @@ void _fft_fill_with_conjugate_symmetry_cuda_(
 
 REGISTER_DISPATCH(fft_fill_with_conjugate_symmetry_stub, &_fft_fill_with_conjugate_symmetry_cuda_);
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/StepKernel.cu b/aten/src/ATen/native/cuda/StepKernel.cu
index 72ad8298287..33348bc98c2 100644
--- a/aten/src/ATen/native/cuda/StepKernel.cu
+++ b/aten/src/ATen/native/cuda/StepKernel.cu
@@ -9,7 +9,8 @@
 // NOTE: CUDA on Windows requires that the enclosing function
 // of a __device__ lambda not have internal linkage.
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void nextafter_kernel_cuda(TensorIteratorBase& iter) {
   AT_DISPATCH_FLOATING_TYPES_AND(kBFloat16, iter.common_dtype(), "nextafter_cuda", [&]() {
@@ -30,4 +31,5 @@ void heaviside_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(nextafter_stub, &nextafter_kernel_cuda);
 REGISTER_DISPATCH(heaviside_stub, &heaviside_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorCompare.cpp b/aten/src/ATen/native/cuda/TensorCompare.cpp
index 1b4d7490b03..963a539f33f 100644
--- a/aten/src/ATen/native/cuda/TensorCompare.cpp
+++ b/aten/src/ATen/native/cuda/TensorCompare.cpp
@@ -2,7 +2,8 @@
 #include <ATen/core/Tensor.h>
 #include <ATen/native/TensorCompare.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -20,4 +21,5 @@ void isin_default_kernel_gpu(
 
 REGISTER_CUDA_DISPATCH(isin_default_stub, &isin_default_kernel_gpu);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorCompare.cu b/aten/src/ATen/native/cuda/TensorCompare.cu
index 0accbf91b9c..331c51933ee 100644
--- a/aten/src/ATen/native/cuda/TensorCompare.cu
+++ b/aten/src/ATen/native/cuda/TensorCompare.cu
@@ -7,7 +7,8 @@
 #include <c10/core/Scalar.h>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -125,4 +126,5 @@ void _assert_async_cuda(const Tensor& self_tensor) {
   });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorFactories.cu b/aten/src/ATen/native/cuda/TensorFactories.cu
index ac0eefeb657..375f1c9ed89 100644
--- a/aten/src/ATen/native/cuda/TensorFactories.cu
+++ b/aten/src/ATen/native/cuda/TensorFactories.cu
@@ -28,7 +28,8 @@
 #include <cmath>
 #include <cstddef>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor& eye_out_cuda(int64_t n, Tensor& result) {
   // the default value of `m` equals to `n`
@@ -383,4 +384,5 @@ Tensor triu_indices_cuda(
   return tensor;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorModeKernel.cpp b/aten/src/ATen/native/cuda/TensorModeKernel.cpp
index db15c9b76e3..c04693bb72e 100644
--- a/aten/src/ATen/native/cuda/TensorModeKernel.cpp
+++ b/aten/src/ATen/native/cuda/TensorModeKernel.cpp
@@ -11,7 +11,8 @@ constexpr int MAX_BLOCK_SIZE = AT_ROCM_ENABLED() ? 256 : 1024;
 // Maximum size per grid dimension that we assume (compute capability >= 2.0)
 constexpr int64_t MAX_GRID_SIZE = 65535LL;
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void mode_kernel_impl(
     Tensor& values,
@@ -97,4 +98,5 @@ void mode_kernel_impl(
 }
 
 REGISTER_CUDA_DISPATCH(mode_stub, &mode_kernel_impl);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorModeKernel.cu b/aten/src/ATen/native/cuda/TensorModeKernel.cu
index 77f88f984d4..7d2371cba55 100644
--- a/aten/src/ATen/native/cuda/TensorModeKernel.cu
+++ b/aten/src/ATen/native/cuda/TensorModeKernel.cu
@@ -18,7 +18,8 @@
 #include <thrust/sequence.h>
 #include <thrust/sort.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t>
 struct ModeImpl {
@@ -282,4 +283,5 @@ void launch_apply_mode_kernel(const TensorBase &values, const TensorBase &indice
   });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorModeKernel.h b/aten/src/ATen/native/cuda/TensorModeKernel.h
index b5660747997..49b2f9535d7 100644
--- a/aten/src/ATen/native/cuda/TensorModeKernel.h
+++ b/aten/src/ATen/native/cuda/TensorModeKernel.h
@@ -16,4 +16,4 @@ void launch_apply_mode_kernel(
     const TensorBase &values, const TensorBase &indices,
     const TensorBase &self, int64_t dim, int64_t ndim);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp b/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp
index 8edfa1a6744..2cffa8b25a8 100644
--- a/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp
+++ b/aten/src/ATen/native/cuda/TensorShapeCUDA.cpp
@@ -10,7 +10,8 @@
 #include <ATen/ops/set_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // this needs to be split along CPU/CUDA lines because we don't have a consistent
 // way of getting the allocator to use for a device (c10::GetAllocator is not
@@ -38,4 +39,5 @@ Tensor& set_storage_cuda_(Tensor& result, Storage storage, int64_t storage_offse
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorTopK.cpp b/aten/src/ATen/native/cuda/TensorTopK.cpp
index 36e45d4dae2..0e28c7a9cb2 100644
--- a/aten/src/ATen/native/cuda/TensorTopK.cpp
+++ b/aten/src/ATen/native/cuda/TensorTopK.cpp
@@ -17,7 +17,8 @@
 #include <ATen/ops/topk_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // TODO: remove this when CUDA <11.6 is no longer supported
 void topk_out_with_sort(
@@ -94,4 +95,5 @@ TORCH_IMPL_FUNC(topk_out_cuda)
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorTopK.cu b/aten/src/ATen/native/cuda/TensorTopK.cu
index bd48c9b0580..eb9808d3e55 100644
--- a/aten/src/ATen/native/cuda/TensorTopK.cu
+++ b/aten/src/ATen/native/cuda/TensorTopK.cu
@@ -19,7 +19,8 @@
 
 using namespace at::native;
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // TODO: remove this when CUDA <11.6 is no longer supported
 bool disable_sort_for_topk() {
@@ -617,7 +618,7 @@ int get_items_per_thread(uint64_t num_slices, uint64_t slice_size) {
   int max_blocks_per_mp = 32;
 #else
   int regs_per_mp = prop->regsPerMultiprocessor;
-#if !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   int max_blocks_per_mp = prop->maxBlocksPerMultiProcessor;
 #else
   int max_blocks_per_mp = 32;
@@ -904,4 +905,5 @@ void launch_gather_topk_kernel(
 #undef RUN_K
 }
 
-} // at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TensorTransformations.cu b/aten/src/ATen/native/cuda/TensorTransformations.cu
index 4fda9d7a880..3bbe87ec592 100644
--- a/aten/src/ATen/native/cuda/TensorTransformations.cu
+++ b/aten/src/ATen/native/cuda/TensorTransformations.cu
@@ -18,7 +18,8 @@
 #include <cstddef>
 #include <vector>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 template <typename scalar_t, typename IndexType>
 #if __CUDA_ARCH__ >= 350 || defined(USE_ROCM)
@@ -151,4 +152,5 @@ Tensor roll_cuda(const Tensor& self, IntArrayRef shifts, IntArrayRef dims) {
   return out_tensor;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/TriangularOps.cu b/aten/src/ATen/native/cuda/TriangularOps.cu
index 549ef15293e..a079ec68498 100644
--- a/aten/src/ATen/native/cuda/TriangularOps.cu
+++ b/aten/src/ATen/native/cuda/TriangularOps.cu
@@ -19,7 +19,8 @@
 
 #include <ATen/cuda/CUDAApplyUtils.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ triu/tril ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 
@@ -106,4 +107,5 @@ Tensor trace_cuda(const Tensor& self) {
   return self.diagonal().sum();
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryComplexKernels.cu b/aten/src/ATen/native/cuda/UnaryComplexKernels.cu
index 688974db517..5e3de1fc13f 100644
--- a/aten/src/ATen/native/cuda/UnaryComplexKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryComplexKernels.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/DispatchStub.h>
 #include <ATen/native/TensorIterator.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // We manually overload angle because std::arg does not work with types other than c10::complex.
 template<typename scalar_t>
@@ -96,4 +97,5 @@ void conj_kernel_cuda(TensorIteratorBase& iter) {
 REGISTER_DISPATCH(angle_stub, &angle_kernel_cuda);
 REGISTER_DISPATCH(conj_physical_stub, &conj_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryFractionKernels.cu b/aten/src/ATen/native/cuda/UnaryFractionKernels.cu
index 2cc6b1b5581..0d84292e33d 100644
--- a/aten/src/ATen/native/cuda/UnaryFractionKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryFractionKernels.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cuda/Math.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // We manually overload ceil because std::ceil does not work with std::complex types.
 template <typename scalar_t>
@@ -196,4 +197,5 @@ REGISTER_DISPATCH(round_stub, &round_kernel_cuda);
 REGISTER_DISPATCH(round_decimals_stub, &round_decimals_kernel_cuda);
 REGISTER_DISPATCH(trunc_stub, &trunc_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGammaKernels.cu b/aten/src/ATen/native/cuda/UnaryGammaKernels.cu
index f4a540fcf93..5f30272196a 100644
--- a/aten/src/ATen/native/cuda/UnaryGammaKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryGammaKernels.cu
@@ -10,7 +10,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/Math.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // See note [Jiterator]
 CONSTEXPR_EXCEPT_WIN_CUDA char digamma_name[] = "digamma";
@@ -105,4 +106,5 @@ REGISTER_DISPATCH(digamma_stub, &digamma_kernel_cuda);
 REGISTER_DISPATCH(polygamma_stub, &polygamma_kernel_cuda);
 REGISTER_DISPATCH(lgamma_stub, &lgamma_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu
index 329fd465d2f..9bafd930762 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAcosKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char acos_name[] = "acos";
 void acos_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void acos_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(acos_stub, &acos_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu
index ad48e51af3c..3745b4169c3 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAcoshKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char acosh_name[] = "acosh";
 void acosh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void acosh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(acosh_stub, &acosh_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu
index 6b3cec3b96c..c94fdf0a09c 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAsinKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char asin_name[] = "asin";
 void asin_kernel_cuda(TensorIteratorBase& iter) {
@@ -47,4 +48,5 @@ void asin_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(asin_stub, &asin_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu
index 7ffe938181d..e2c596b3df5 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAsinhKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char asinh_name[] = "asinh";
 void asinh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void asinh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(asinh_stub, &asinh_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu
index d56f75efd4e..a29190dd29b 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAtanKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char atan_name[] = "atan";
 void atan_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void atan_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(atan_stub, &atan_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu
index 55c9919c2ca..c0973b55812 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricAtanhKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char atanh_name[] = "atanh";
 void atanh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void atanh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(atanh_stub, &atanh_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu
index 1359d0a16ae..d23dffcfa2f 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricCosKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char cos_name[] = "cos";
 void cos_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +51,5 @@ void cos_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(cos_stub, &cos_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu
index c9608a1ba2a..5d78479ab43 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricCoshKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char cosh_name[] = "cosh";
 void cosh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void cosh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(cosh_stub, &cosh_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu
index f7d6d5e3b42..ec57bab04b8 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricSinKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char sin_name[] = "sin";
 void sin_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +51,5 @@ void sin_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(sin_stub, &sin_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu
index 22dd2bf2ab2..e92f06c8146 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricSinhKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char sinh_name[] = "sinh";
 void sinh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void sinh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(sinh_stub, &sinh_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu
index 91208b69e48..8a009180ef1 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricTanKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char tan_name[] = "tan";
 void tan_kernel_cuda(TensorIteratorBase& iter) {
@@ -50,4 +51,5 @@ void tan_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(tan_stub, &tan_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu b/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu
index 9e6184f7a3f..81ecca7e288 100644
--- a/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryGeometricTanhKernel.cu
@@ -9,7 +9,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char tanh_name[] = "tanh";
 void tanh_kernel_cuda(TensorIteratorBase& iter) {
@@ -51,4 +52,5 @@ void tanh_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(tanh_stub, &tanh_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryLogKernels.cu b/aten/src/ATen/native/cuda/UnaryLogKernels.cu
index fb3d19baca3..e9585f57ac6 100644
--- a/aten/src/ATen/native/cuda/UnaryLogKernels.cu
+++ b/aten/src/ATen/native/cuda/UnaryLogKernels.cu
@@ -10,7 +10,8 @@
 #include <ATen/native/TensorIterator.h>
 #include <ATen/native/cuda/Math.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char log_name[] = "log_kernel";
 void log_kernel_cuda(TensorIteratorBase& iter) {
@@ -115,4 +116,5 @@ REGISTER_DISPATCH(log10_stub, &log10_kernel_cuda);
 REGISTER_DISPATCH(log2_stub, &log2_kernel_cuda);
 REGISTER_DISPATCH(log1p_stub, &log1p_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnaryOpsKernel.cu b/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
index 07d5527e87d..0cdc295632e 100644
--- a/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
+++ b/aten/src/ATen/native/cuda/UnaryOpsKernel.cu
@@ -18,7 +18,8 @@
 #include <c10/core/Scalar.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void bitwise_not_kernel_cuda(TensorIteratorBase& iter) {
   if (iter.dtype() == ScalarType::Bool) {
@@ -256,4 +257,5 @@ REGISTER_DISPATCH(sqrt_stub, &sqrt_kernel_cuda);
 REGISTER_DISPATCH(nan_to_num_stub, &nan_to_num_kernel_cuda);
 REGISTER_DISPATCH(frexp_stub, &frexp_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnarySignKernels.cu b/aten/src/ATen/native/cuda/UnarySignKernels.cu
index 83233f3143c..0565f032283 100644
--- a/aten/src/ATen/native/cuda/UnarySignKernels.cu
+++ b/aten/src/ATen/native/cuda/UnarySignKernels.cu
@@ -12,7 +12,8 @@
 
 #include <type_traits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void logical_not_kernel_cuda(TensorIteratorBase& iter) {
   // error check -- this is just ensuring we don't dispatch on types that aren't in ALL_TYPES_AND_COMPLEX_AND3(...)
@@ -134,4 +135,5 @@ REGISTER_DISPATCH(sign_stub, &sign_kernel_cuda);
 REGISTER_DISPATCH(signbit_stub, &signbit_kernel_cuda);
 REGISTER_DISPATCH(sgn_stub, &sgn_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu b/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu
index cd62641a80d..1eaf5e08604 100644
--- a/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu
+++ b/aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu
@@ -17,7 +17,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 CONSTEXPR_EXCEPT_WIN_CUDA char exp2_name[] = "exp2_kernel";
 void exp2_kernel_cuda(TensorIteratorBase& iter) {
@@ -394,4 +395,5 @@ REGISTER_DISPATCH(special_ndtri_stub, &ndtri_kernel_cuda);
 REGISTER_DISPATCH(special_log_ndtr_stub, &log_ndtr_kernel_cuda);
 REGISTER_DISPATCH(special_erfcx_stub, &erfcx_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu b/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu
index 2f48d4fc014..c8fd1915467 100644
--- a/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu
+++ b/aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu
@@ -15,7 +15,8 @@
 // unfold_backward, the algorithm is described in
 // /native/cpu/UnfoldBackwardKernel.cpp
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -159,4 +160,5 @@ void unfold_backward_cuda_kernel(
 
 REGISTER_DISPATCH(unfold_backward_stub, &unfold_backward_cuda_kernel);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/Unique.cu b/aten/src/ATen/native/cuda/Unique.cu
index fd1dc2f5236..38117e47bdc 100644
--- a/aten/src/ATen/native/cuda/Unique.cu
+++ b/aten/src/ATen/native/cuda/Unique.cu
@@ -29,7 +29,8 @@
 
 #include <ATen/native/cuda/UniqueCub.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -232,4 +233,5 @@ unique_consecutive_cuda(const Tensor& self, const bool return_inverse, const boo
   return unique_dim_consecutive_cuda(self, dim.value(), return_inverse, return_counts);
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UniqueCub.cu b/aten/src/ATen/native/cuda/UniqueCub.cu
index 5e259048fe8..500038ffaa1 100644
--- a/aten/src/ATen/native/cuda/UniqueCub.cu
+++ b/aten/src/ATen/native/cuda/UniqueCub.cu
@@ -16,7 +16,9 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native::internal {
+namespace at {
+namespace native {
+namespace internal {
 
 namespace {
 
@@ -338,4 +340,6 @@ INSTANTIATE_UNIQUE_CUDA_TEMPLATE(at::Half);
 
 #undef INSTANTIATE
 
-} // namespace at::native::internal
+} // internal
+} // native
+} // at
diff --git a/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu b/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu
index 3589e06b52f..1214955b06d 100644
--- a/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleBicubic2d.cu
@@ -16,7 +16,8 @@
 #include <ATen/ops/upsample_bicubic2d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t>
@@ -294,4 +295,5 @@ TORCH_IMPL_FUNC(upsample_bicubic2d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu b/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu
index e7d1bb02eeb..d76e2783207 100644
--- a/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleBilinear2d.cu
@@ -27,7 +27,8 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t>
@@ -914,4 +915,5 @@ TORCH_IMPL_FUNC(_upsample_bicubic2d_aa_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales_h, scales_w);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UpSampleLinear1d.cu b/aten/src/ATen/native/cuda/UpSampleLinear1d.cu
index fd29c2ec855..af9edca2280 100644
--- a/aten/src/ATen/native/cuda/UpSampleLinear1d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleLinear1d.cu
@@ -19,7 +19,8 @@
 #include <ATen/ops/upsample_linear1d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 template <typename scalar_t, typename accscalar_t>
@@ -227,4 +228,5 @@ TORCH_IMPL_FUNC(upsample_linear1d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UpSampleNearest1d.cu b/aten/src/ATen/native/cuda/UpSampleNearest1d.cu
index 26048202a45..decdfca30d7 100644
--- a/aten/src/ATen/native/cuda/UpSampleNearest1d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleNearest1d.cu
@@ -18,7 +18,8 @@
 #include <ATen/ops/_upsample_nearest_exact1d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 #define MAX_THREADS 512
@@ -235,4 +236,5 @@ TORCH_IMPL_FUNC(_upsample_nearest_exact1d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, scales);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UpSampleNearest2d.cu b/aten/src/ATen/native/cuda/UpSampleNearest2d.cu
index 5f4f4100da5..f223655daca 100644
--- a/aten/src/ATen/native/cuda/UpSampleNearest2d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleNearest2d.cu
@@ -22,7 +22,8 @@
 #include <ATen/ops/upsample_nearest2d_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 #define MAX_THREADS 512
@@ -484,4 +485,5 @@ TORCH_IMPL_FUNC(_upsample_nearest_exact2d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, scales_h, scales_w);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UpSampleNearest3d.cu b/aten/src/ATen/native/cuda/UpSampleNearest3d.cu
index d06fc571a2d..58f14ad491a 100644
--- a/aten/src/ATen/native/cuda/UpSampleNearest3d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleNearest3d.cu
@@ -24,7 +24,8 @@
 #include <ATen/ops/_upsample_nearest_exact3d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 #define MAX_THREADS 512
@@ -336,4 +337,5 @@ TORCH_IMPL_FUNC(_upsample_nearest_exact3d_backward_out_cuda) (
 using at::native::upsample::compute_output_size;
 using at::native::upsample_cuda::get_scale_value;
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu b/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu
index d443082c4a0..b19bf4858ac 100644
--- a/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu
+++ b/aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu
@@ -21,7 +21,8 @@
 #include <ATen/ops/upsample_trilinear3d_backward_native.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 __device__ __forceinline__ size_t
@@ -393,4 +394,5 @@ TORCH_IMPL_FUNC(upsample_trilinear3d_backward_out_cuda) (
       grad_input, grad_output, output_size, input_size, align_corners, scales_d, scales_h, scales_w);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu b/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu
index 9f7fb1cbd0a..d644c40b8ca 100644
--- a/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu
+++ b/aten/src/ATen/native/cuda/ValidateCompressedIndicesKernel.cu
@@ -2,7 +2,8 @@
 #include <ATen/native/sparse/ValidateCompressedIndicesCommon.h>
 #include <ATen/native/cuda/Loops.cuh>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -26,4 +27,5 @@ void _validate_compressed_sparse_indices_cuda(
       is_crow, cidx, idx, cdim, dim, nnz);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/WeightNorm.cu b/aten/src/ATen/native/cuda/WeightNorm.cu
index d492633a1bb..e25a1b40775 100644
--- a/aten/src/ATen/native/cuda/WeightNorm.cu
+++ b/aten/src/ATen/native/cuda/WeightNorm.cu
@@ -19,7 +19,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 // Block size for weight_norm_*_first_dim_kernel.
@@ -522,4 +523,5 @@ std::tuple<Tensor, Tensor> weight_norm_backward_cuda
 #undef TILE_W
 #undef TILE_H
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/ZetaKernel.cu b/aten/src/ATen/native/cuda/ZetaKernel.cu
index 7459504f508..528212795d4 100644
--- a/aten/src/ATen/native/cuda/ZetaKernel.cu
+++ b/aten/src/ATen/native/cuda/ZetaKernel.cu
@@ -7,7 +7,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 
 /*
@@ -36,4 +37,5 @@ void zeta_kernel_cuda(TensorIteratorBase& iter) {
 
 REGISTER_DISPATCH(zeta_stub, &zeta_kernel_cuda);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/airy_ai.cu b/aten/src/ATen/native/cuda/airy_ai.cu
index 35e6b002260..5b6ba49ea7c 100644
--- a/aten/src/ATen/native/cuda/airy_ai.cu
+++ b/aten/src/ATen/native/cuda/airy_ai.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char airy_ai_name[] = "airy_ai_forward";
 
@@ -39,4 +40,5 @@ void airy_ai_kernel_cuda(TensorIteratorBase& iterator) {
 } // anonymous namespace
 
 REGISTER_DISPATCH(special_airy_ai_stub, &airy_ai_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/bessel_j0.cu b/aten/src/ATen/native/cuda/bessel_j0.cu
index 2ebfe676e50..875ef776bc5 100644
--- a/aten/src/ATen/native/cuda/bessel_j0.cu
+++ b/aten/src/ATen/native/cuda/bessel_j0.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char bessel_j0_name[] = "bessel_j0_forward";
 
@@ -39,4 +40,5 @@ void bessel_j0_kernel_cuda(TensorIteratorBase& iterator) {
 } // anonymous namespace
 
 REGISTER_DISPATCH(special_bessel_j0_stub, &bessel_j0_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/bessel_j1.cu b/aten/src/ATen/native/cuda/bessel_j1.cu
index 42bd43321f4..a7acfbc9ed3 100644
--- a/aten/src/ATen/native/cuda/bessel_j1.cu
+++ b/aten/src/ATen/native/cuda/bessel_j1.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char bessel_j1_name[] = "bessel_j1_forward";
 
@@ -39,4 +40,5 @@ void bessel_j1_kernel_cuda(TensorIteratorBase& iterator) {
 } // anonymous namespace
 
 REGISTER_DISPATCH(special_bessel_j1_stub, &bessel_j1_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/bessel_y0.cu b/aten/src/ATen/native/cuda/bessel_y0.cu
index 631031d4e26..8b8cd37d612 100644
--- a/aten/src/ATen/native/cuda/bessel_y0.cu
+++ b/aten/src/ATen/native/cuda/bessel_y0.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char bessel_y0_name[] = "bessel_y0_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_bessel_y0_stub, &bessel_y0_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/bessel_y1.cu b/aten/src/ATen/native/cuda/bessel_y1.cu
index 1375061e43e..41bf2af92b2 100644
--- a/aten/src/ATen/native/cuda/bessel_y1.cu
+++ b/aten/src/ATen/native/cuda/bessel_y1.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char bessel_y1_name[] = "bessel_y1_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_bessel_y1_stub, &bessel_y1_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu
index 7736d20e018..65c7cb026af 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_t.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_t_name[] = "chebyshev_polynomial_t_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_t_stub, &chebyshev_polynomial_t_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu
index 412479e11f4..8756b397010 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_u.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_u_name[] = "chebyshev_polynomial_u_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_u_stub, &chebyshev_polynomial_u_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu
index ca2e534e641..2eb14c1ce52 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_v.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_v_name[] = "chebyshev_polynomial_v_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_v_stub, &chebyshev_polynomial_v_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu b/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu
index 9d5a0e3a7bd..ac0d3eaf2fc 100644
--- a/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu
+++ b/aten/src/ATen/native/cuda/chebyshev_polynomial_w.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char chebyshev_polynomial_w_name[] = "chebyshev_polynomial_w_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(chebyshev_polynomial_w_stub, &chebyshev_polynomial_w_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu b/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu
index ec8ac6b4f26..817b50ffe9a 100644
--- a/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu
+++ b/aten/src/ATen/native/cuda/fused_adam_amsgrad_impl.cu
@@ -6,7 +6,8 @@
 #include <ATen/native/cuda/MultiTensorApply.cuh>
 #include <vector>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void _fused_adam_amsgrad_cuda_impl_(
     at::TensorList params,
@@ -49,4 +50,5 @@ void _fused_adam_amsgrad_cuda_impl_(
         });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/fused_adam_impl.cu b/aten/src/ATen/native/cuda/fused_adam_impl.cu
index d91be6bfc99..75b70c0caf6 100644
--- a/aten/src/ATen/native/cuda/fused_adam_impl.cu
+++ b/aten/src/ATen/native/cuda/fused_adam_impl.cu
@@ -6,7 +6,8 @@
 #include <ATen/native/cuda/MultiTensorApply.cuh>
 #include <vector>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void _fused_adam_cuda_impl_(
     at::TensorList params,
@@ -48,4 +49,5 @@ void _fused_adam_cuda_impl_(
         });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/fused_adamw_amsgrad_impl.cu b/aten/src/ATen/native/cuda/fused_adamw_amsgrad_impl.cu
index b82db1d7763..0570bb6a416 100644
--- a/aten/src/ATen/native/cuda/fused_adamw_amsgrad_impl.cu
+++ b/aten/src/ATen/native/cuda/fused_adamw_amsgrad_impl.cu
@@ -49,4 +49,4 @@ void _fused_adamw_amsgrad_cuda_impl_(
         });
 }
 
-} } // namespace at::native
+} } // namespace at  native
diff --git a/aten/src/ATen/native/cuda/fused_adamw_impl.cu b/aten/src/ATen/native/cuda/fused_adamw_impl.cu
index fff29afd7b4..2312ab21025 100644
--- a/aten/src/ATen/native/cuda/fused_adamw_impl.cu
+++ b/aten/src/ATen/native/cuda/fused_adamw_impl.cu
@@ -48,4 +48,4 @@ void _fused_adamw_cuda_impl_(
         });
 }
 
-} } // namespace at::native
+} } // namespace at  native
diff --git a/aten/src/ATen/native/cuda/group_norm_kernel.cu b/aten/src/ATen/native/cuda/group_norm_kernel.cu
index 04bdca8ad11..11ea67e2a79 100644
--- a/aten/src/ATen/native/cuda/group_norm_kernel.cu
+++ b/aten/src/ATen/native/cuda/group_norm_kernel.cu
@@ -21,7 +21,8 @@
 #include <ATen/ops/empty.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -993,4 +994,5 @@ void GroupNormBackwardKernelImpl(
 REGISTER_DISPATCH(GroupNormKernel, &GroupNormKernelImpl);
 REGISTER_DISPATCH(GroupNormBackwardKernel, &GroupNormBackwardKernelImpl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/hermite_polynomial_h.cu b/aten/src/ATen/native/cuda/hermite_polynomial_h.cu
index f53253bcd09..671b5b4dcdb 100644
--- a/aten/src/ATen/native/cuda/hermite_polynomial_h.cu
+++ b/aten/src/ATen/native/cuda/hermite_polynomial_h.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char hermite_polynomial_h_name[] = "hermite_polynomial_h_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(hermite_polynomial_h_stub, &hermite_polynomial_h_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/hermite_polynomial_he.cu b/aten/src/ATen/native/cuda/hermite_polynomial_he.cu
index bab37656585..0ce4b63a45a 100644
--- a/aten/src/ATen/native/cuda/hermite_polynomial_he.cu
+++ b/aten/src/ATen/native/cuda/hermite_polynomial_he.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char hermite_polynomial_he_name[] = "hermite_polynomial_he_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(hermite_polynomial_he_stub, &hermite_polynomial_he_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu b/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu
index a98336dfcb6..1489ba3f114 100644
--- a/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu
+++ b/aten/src/ATen/native/cuda/laguerre_polynomial_l.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char laguerre_polynomial_l_name[] = "laguerre_polynomial_l_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(laguerre_polynomial_l_stub, &laguerre_polynomial_l_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/layer_norm_kernel.cu b/aten/src/ATen/native/cuda/layer_norm_kernel.cu
index 6d8008230f8..be0358f57c0 100644
--- a/aten/src/ATen/native/cuda/layer_norm_kernel.cu
+++ b/aten/src/ATen/native/cuda/layer_norm_kernel.cu
@@ -28,7 +28,8 @@
 #include <c10/util/env.h>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 
@@ -1446,4 +1447,5 @@ std::tuple<Tensor, Tensor, Tensor> layer_norm_backward_cuda(
 
 REGISTER_DISPATCH(LayerNormKernel, &LayerNormKernelImpl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/legendre_polynomial_p.cu b/aten/src/ATen/native/cuda/legendre_polynomial_p.cu
index 9f5efc9b451..bfcdee91b9e 100644
--- a/aten/src/ATen/native/cuda/legendre_polynomial_p.cu
+++ b/aten/src/ATen/native/cuda/legendre_polynomial_p.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             const char legendre_polynomial_p_name[] = "legendre_polynomial_p_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(legendre_polynomial_p_stub, &legendre_polynomial_p_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp
index 87260196a40..d0bc52805bc 100644
--- a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp
+++ b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebra.cpp
@@ -66,7 +66,8 @@ const bool use_magma_ = false;
 
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 #if defined(BUILD_LAZY_CUDA_LINALG)
 // All registrations with PyTorch runtime should be done dynamically
 // so if library is lazy loaded it must not export anything, otherwise
@@ -2767,6 +2768,7 @@ struct DispatchInitializer {
 
 }  // namespace lazy_linalg
 #endif
-}  // namespace at::native
+} // namespace native
+} // namespace at
 
 #undef ALLOCATE_ARRAY
diff --git a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp
index 823c20b1d48..ded0ebf849d 100644
--- a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp
+++ b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.cpp
@@ -27,7 +27,8 @@
 #include <ATen/ops/zeros.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 cublasOperation_t to_cublas(TransposeType trans) {
   switch (trans) {
@@ -1743,4 +1744,5 @@ void lu_solve_looped_cusolver(const Tensor& LU, const Tensor& pivots, const Tens
 
 #endif  // USE_CUSOLVER
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h
index 3fdf3ebf7af..d2bc5154b72 100644
--- a/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h
+++ b/aten/src/ATen/native/cuda/linalg/BatchLinearAlgebraLib.h
@@ -90,4 +90,4 @@ C10_EXPORT void registerLinalgDispatch(const LinalgDispatch&);
 }} // namespace cuda::detail
 #endif
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/cuda/modified_bessel_i0.cu b/aten/src/ATen/native/cuda/modified_bessel_i0.cu
index 9f1f3ba98c6..6c1bc231634 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_i0.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_i0.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_i0_name[] = "modified_bessel_i0_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_i0_stub, &modified_bessel_i0_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/modified_bessel_i1.cu b/aten/src/ATen/native/cuda/modified_bessel_i1.cu
index d51e7fefb0e..f2eca12b8d8 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_i1.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_i1.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_i1_name[] = "modified_bessel_i1_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_i1_stub, &modified_bessel_i1_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/modified_bessel_k0.cu b/aten/src/ATen/native/cuda/modified_bessel_k0.cu
index 574268456c8..5139f2e6291 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_k0.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_k0.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_k0_name[] = "modified_bessel_k0_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_k0_stub, &modified_bessel_k0_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/modified_bessel_k1.cu b/aten/src/ATen/native/cuda/modified_bessel_k1.cu
index b3720d8e1ba..a7980b40b89 100644
--- a/aten/src/ATen/native/cuda/modified_bessel_k1.cu
+++ b/aten/src/ATen/native/cuda/modified_bessel_k1.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char modified_bessel_k1_name[] = "modified_bessel_k1_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_modified_bessel_k1_stub, &modified_bessel_k1_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu b/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu
index ac2355e409a..e7cc7854fe5 100644
--- a/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu
+++ b/aten/src/ATen/native/cuda/scaled_modified_bessel_k0.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char scaled_modified_bessel_k0_name[] = "scaled_modified_bessel_k0_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_scaled_modified_bessel_k0_stub, &scaled_modified_bessel_k0_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu b/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu
index b1d8d2a41b6..a65ed384501 100644
--- a/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu
+++ b/aten/src/ATen/native/cuda/scaled_modified_bessel_k1.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char scaled_modified_bessel_k1_name[] = "scaled_modified_bessel_k1_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_scaled_modified_bessel_k1_stub, &scaled_modified_bessel_k1_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu
index d86042030cd..05961263077 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_t.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_t_name[] = "shifted_chebyshev_polynomial_t_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(shifted_chebyshev_polynomial_t_stub, &shifted_chebyshev_polynomial_t_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu
index a2e2cd485fd..911145eeca8 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_u.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_u_name[] = "shifted_chebyshev_polynomial_u_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(shifted_chebyshev_polynomial_u_stub, &shifted_chebyshev_polynomial_u_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu
index 6e5404179ab..42a59bfff84 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_v.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace {
 CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_v_name[] = "shifted_chebyshev_polynomial_v_forward";
 
@@ -29,4 +30,5 @@ void shifted_chebyshev_polynomial_v_kernel_cuda(TensorIteratorBase& iterator) {
 } // namespace (anonymous)
 
 REGISTER_DISPATCH(shifted_chebyshev_polynomial_v_stub, &shifted_chebyshev_polynomial_v_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu
index 3bfee57d14e..52cfe8a2c0f 100644
--- a/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu
+++ b/aten/src/ATen/native/cuda/shifted_chebyshev_polynomial_w.cu
@@ -8,7 +8,8 @@
 #include <ATen/native/cuda/Math.cuh>
 #include <ATen/native/cuda/jit_utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char shifted_chebyshev_polynomial_w_name[] = "shifted_chebyshev_polynomial_w_forward";
 
@@ -28,4 +29,5 @@ namespace at::native {
         } // namespace (anonymous)
 
         REGISTER_DISPATCH(shifted_chebyshev_polynomial_w_stub, &shifted_chebyshev_polynomial_w_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cuda/spherical_bessel_j0.cu b/aten/src/ATen/native/cuda/spherical_bessel_j0.cu
index d0bf46e6539..532e5920514 100644
--- a/aten/src/ATen/native/cuda/spherical_bessel_j0.cu
+++ b/aten/src/ATen/native/cuda/spherical_bessel_j0.cu
@@ -18,7 +18,8 @@
 #include <c10/cuda/CUDAMathCompat.h>
 #include <c10/util/complex.h>
 
-namespace at::native {
+namespace at {
+namespace native {
         namespace {
             CONSTEXPR_EXCEPT_WIN_CUDA char spherical_bessel_j0_name[] = "spherical_bessel_j0_forward";
 
@@ -38,4 +39,5 @@ namespace at::native {
         }
 
         REGISTER_DISPATCH(special_spherical_bessel_j0_stub, &spherical_bessel_j0_kernel_cuda);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp b/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp
index bfc7184e930..7fbfbf71d03 100644
--- a/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp
+++ b/aten/src/ATen/native/cudnn/AffineGridGenerator.cpp
@@ -101,6 +101,6 @@ Tensor cudnn_affine_grid_generator_backward(
   return grad_theta_t;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 #endif // AT_CUDNN_ENABLED()
diff --git a/aten/src/ATen/native/cudnn/BatchNorm.cpp b/aten/src/ATen/native/cudnn/BatchNorm.cpp
index f1f275e6388..a80eb0e3010 100644
--- a/aten/src/ATen/native/cudnn/BatchNorm.cpp
+++ b/aten/src/ATen/native/cudnn/BatchNorm.cpp
@@ -21,7 +21,7 @@ std::tuple<Tensor, Tensor, Tensor> cudnn_batch_norm_backward(
   AT_ERROR("cudnn_batch_norm_backward: ATen not compiled with cuDNN support");
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 #else // AT_CUDNN_ENABLED
 
diff --git a/aten/src/ATen/native/cudnn/Conv_v7.cpp b/aten/src/ATen/native/cudnn/Conv_v7.cpp
index f5c7af79a74..b4ed5f19d4e 100644
--- a/aten/src/ATen/native/cudnn/Conv_v7.cpp
+++ b/aten/src/ATen/native/cudnn/Conv_v7.cpp
@@ -978,6 +978,6 @@ void raw_cudnn_convolution_add_relu_fallback_out(
   output.relu_();
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 #endif
diff --git a/aten/src/ATen/native/cudnn/LossCTC.cpp b/aten/src/ATen/native/cudnn/LossCTC.cpp
index 7737e91d441..9e7ec81de96 100644
--- a/aten/src/ATen/native/cudnn/LossCTC.cpp
+++ b/aten/src/ATen/native/cudnn/LossCTC.cpp
@@ -200,6 +200,6 @@ std::tuple<Tensor, Tensor> _cudnn_ctc_loss_tensor(
       log_probs, targets, il, tl, BLANK, deterministic, zero_infinity);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 #endif
diff --git a/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp b/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp
index 91f3f01764d..11ddd7fdfe4 100644
--- a/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp
+++ b/aten/src/ATen/native/miopen/BatchNorm_miopen.cpp
@@ -33,7 +33,7 @@ std::tuple<Tensor, Tensor, Tensor> miopen_batch_norm_backward(
   AT_ERROR("miopen_batch_norm_backward: ATen not compiled with MIOpen support");
 }
 
-}}  // namespace at::native
+}} // namespace at::native
 
 #else // AT_ROCM_ENABLED
 
diff --git a/aten/src/ATen/native/miopen/RNN_miopen.cpp b/aten/src/ATen/native/miopen/RNN_miopen.cpp
index b61794487a4..22d194fe05f 100644
--- a/aten/src/ATen/native/miopen/RNN_miopen.cpp
+++ b/aten/src/ATen/native/miopen/RNN_miopen.cpp
@@ -46,7 +46,7 @@ namespace at { namespace native {
         AT_ERROR("miopen_rnn_backward: ATen not compiled with MIOpen support.");
     }
 
-}} //namespace at::native
+}} // namespace at::native
 
 #else // AT_ROCM_ENABLED()
 
diff --git a/aten/src/ATen/native/mkl/LinearAlgebra.h b/aten/src/ATen/native/mkl/LinearAlgebra.h
index a3bbd828532..32bf96c0dfb 100644
--- a/aten/src/ATen/native/mkl/LinearAlgebra.h
+++ b/aten/src/ATen/native/mkl/LinearAlgebra.h
@@ -29,4 +29,4 @@ void mkl_gemm_batched(
     const c10::complex<double>** A, int lda, const c10::complex<double>** B, int ldb,
     c10::complex<double> beta, c10::complex<double>** C, int ldc);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/mkldnn/Conv.cpp b/aten/src/ATen/native/mkldnn/Conv.cpp
index 7ba6b320ad7..9970449186d 100644
--- a/aten/src/ATen/native/mkldnn/Conv.cpp
+++ b/aten/src/ATen/native/mkldnn/Conv.cpp
@@ -1137,6 +1137,6 @@ TORCH_LIBRARY_IMPL(mkldnn, Meta, m) {
       TORCH_SELECTIVE_NAME("mkldnn::_convolution_transpose_pointwise"),
       TORCH_FN(mkldnn_convolution_transpose_pointwise_meta));
 }
-}}  // namespace at::native
+}} // namespace at::native
 
 #endif
diff --git a/aten/src/ATen/native/mkldnn/RNN.cpp b/aten/src/ATen/native/mkldnn/RNN.cpp
index bb0251bc9d6..e9f6ce71ea0 100644
--- a/aten/src/ATen/native/mkldnn/RNN.cpp
+++ b/aten/src/ATen/native/mkldnn/RNN.cpp
@@ -16,7 +16,8 @@
 
 #if !AT_MKLDNN_ENABLED()
 
-namespace at::native {
+namespace at {
+namespace native {
 
 
 std::tuple<Tensor, Tensor, Tensor, Tensor> mkldnn_rnn_layer(
@@ -68,14 +69,16 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> mkldnn_rnn_la
 
 REGISTER_NO_CPU_DISPATCH(lstm_mkldnn_stub);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
 
 #else // AT_MKLDNN_EBABLED
 
 #include <ATen/native/mkldnn/MKLDNNCommon.h>
 #include <ATen/native/mkldnn/Utils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 struct RNNParams {
   ideep::rnn_kind mode;
@@ -571,6 +574,7 @@ void lstm_mkldnn(Tensor& output, Tensor& hy, Tensor& cy,
 
 REGISTER_ALL_CPU_DISPATCH(lstm_mkldnn_stub, &lstm_mkldnn);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
 
 #endif // AT_MKLDNN_EBABLED
diff --git a/aten/src/ATen/native/mps/OperationUtils.mm b/aten/src/ATen/native/mps/OperationUtils.mm
index c5e8b5d1fc1..64ab16e0c32 100644
--- a/aten/src/ATen/native/mps/OperationUtils.mm
+++ b/aten/src/ATen/native/mps/OperationUtils.mm
@@ -438,4 +438,5 @@ private:
 
 REGISTER_MPS_ALLOCATOR_CALLBACK("mps_graph_cache_callback", MPSGraphCacheCallback);
 
-} // namespace at::native::mps
+} // namespace native
+} // namespace at::mps
diff --git a/aten/src/ATen/native/mps/TensorFactory.cpp b/aten/src/ATen/native/mps/TensorFactory.cpp
index 4639546e726..811c1a055ec 100644
--- a/aten/src/ATen/native/mps/TensorFactory.cpp
+++ b/aten/src/ATen/native/mps/TensorFactory.cpp
@@ -9,7 +9,8 @@
 #include <ATen/native/Resize.h>
 #include <ATen/native/mps/Copy.h>
 #include <ATen/native/mps/TensorFactory.h>
-namespace at::native {
+namespace at {
+namespace native {
 
 static inline void maybe_resize_storage_mps(TensorImpl* self, uint64_t new_size) {
   if (new_size == 0) {
@@ -134,4 +135,5 @@ Tensor& set_storage_mps_(Tensor& result, Storage storage, int64_t storage_offset
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Activation.mm b/aten/src/ATen/native/mps/operations/Activation.mm
index 198c13f3330..e583ce1bf7a 100644
--- a/aten/src/ATen/native/mps/operations/Activation.mm
+++ b/aten/src/ATen/native/mps/operations/Activation.mm
@@ -13,7 +13,8 @@
 
 using namespace at::mps;
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor relu_mps(const Tensor& self) {
   using namespace mps;
@@ -2470,4 +2471,5 @@ Tensor hardswish_backward_mps(const Tensor& grad_output, const Tensor& self) {
   }
   return grad_input;
 }
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/AdaptivePooling.mm b/aten/src/ATen/native/mps/operations/AdaptivePooling.mm
index d90545147e3..b65a6ec96e6 100644
--- a/aten/src/ATen/native/mps/operations/AdaptivePooling.mm
+++ b/aten/src/ATen/native/mps/operations/AdaptivePooling.mm
@@ -3,7 +3,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/Pool.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void set_kernel_params
   (int64_t isizeH, int64_t isizeW,
@@ -241,4 +242,5 @@ TORCH_IMPL_FUNC(adaptive_max_pool2d_backward_out_mps)
                                            indices);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/BinaryOps.mm b/aten/src/ATen/native/mps/operations/BinaryOps.mm
index 80113706746..143fa3659a3 100644
--- a/aten/src/ATen/native/mps/operations/BinaryOps.mm
+++ b/aten/src/ATen/native/mps/operations/BinaryOps.mm
@@ -9,7 +9,8 @@
 #include <c10/util/Optional.h>
 #include <ATen/native/BinaryOps.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 struct BinaryOpCachedGraph : public MPSCachedGraph
@@ -390,4 +391,5 @@ TORCH_IMPL_FUNC(logaddexp2_out_mps) (const Tensor& self, const Tensor& other, co
   mps::binaryOpTensor(self, other, Scalar(1.0), output, "logaddexp2_out_mps", logaddexp2_op_block);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Blas.mm b/aten/src/ATen/native/mps/operations/Blas.mm
index a5768d0d13a..31b05926200 100644
--- a/aten/src/ATen/native/mps/operations/Blas.mm
+++ b/aten/src/ATen/native/mps/operations/Blas.mm
@@ -13,7 +13,8 @@
 #endif
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 
 Tensor dot_mps(
@@ -217,4 +218,5 @@ TORCH_IMPL_FUNC(addmv_out_mps)(const Tensor &self, const Tensor &mat, const Tens
   addmv_out_mps_impl(self, mat, vec, beta_, alpha_, const_cast<Tensor&>(result));
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/ConstantOps.mm b/aten/src/ATen/native/mps/operations/ConstantOps.mm
index 12e86e14c63..6c75eb15059 100644
--- a/aten/src/ATen/native/mps/operations/ConstantOps.mm
+++ b/aten/src/ATen/native/mps/operations/ConstantOps.mm
@@ -2,7 +2,8 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor& fill_scalar_mps_impl(Tensor& self, const Scalar& value) {
   using namespace mps;
@@ -116,4 +117,5 @@ Tensor& fill_tensor_mps_(Tensor& self, const Tensor& value) {
   return fill_scalar_mps_impl(self, scalar_value);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Convolution.mm b/aten/src/ATen/native/mps/operations/Convolution.mm
index 601cbaec965..dcba542c084 100644
--- a/aten/src/ATen/native/mps/operations/Convolution.mm
+++ b/aten/src/ATen/native/mps/operations/Convolution.mm
@@ -9,7 +9,8 @@
 #include <ATen/native/ConvUtils.h>
 #include <torch/library.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void fill_depthwise_conv_desc(MPSGraphDepthwiseConvolution3DOpDescriptor* descriptor_,
                     NSUInteger strideInX, NSUInteger strideInY,
@@ -618,4 +619,5 @@ std::tuple<Tensor,Tensor> mps_convolution_transpose_backward(
 }
 
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Copy.mm b/aten/src/ATen/native/mps/operations/Copy.mm
index 16f5718dd29..cf0b9db9965 100644
--- a/aten/src/ATen/native/mps/operations/Copy.mm
+++ b/aten/src/ATen/native/mps/operations/Copy.mm
@@ -3,7 +3,8 @@
 #include <ATen/native/mps/Copy.h>
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 void* pageAlignedBlockPtr(
@@ -339,4 +340,5 @@ Tensor _copy_from_mps(const at::Tensor& self, const at::Tensor& dst, bool non_bl
   return mps::mps_copy_(const_cast<Tensor&>(dst), self, non_blocking);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/CrossKernel.mm b/aten/src/ATen/native/mps/operations/CrossKernel.mm
index f140bcb8f9b..22b715f5f91 100644
--- a/aten/src/ATen/native/mps/operations/CrossKernel.mm
+++ b/aten/src/ATen/native/mps/operations/CrossKernel.mm
@@ -3,7 +3,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/Cross.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 static const char* METAL_CROSS = R"CROSS_METAL(
 
@@ -202,4 +203,5 @@ void cross_mps_impl(const Tensor& out, const Tensor& input, const Tensor& other,
 
 REGISTER_DISPATCH(cross_stub, &cross_mps_impl);
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Distributions.mm b/aten/src/ATen/native/mps/operations/Distributions.mm
index f047b9e524c..1b395a3b907 100644
--- a/aten/src/ATen/native/mps/operations/Distributions.mm
+++ b/aten/src/ATen/native/mps/operations/Distributions.mm
@@ -7,7 +7,8 @@
 #include <ATen/mps/MPSGeneratorImpl.h>
 #include <ATen/native/TensorFactories.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 struct RandomCachedGraph : public MPSCachedGraph
@@ -635,4 +636,5 @@ Tensor multinomial_mps(
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Eye.mm b/aten/src/ATen/native/mps/operations/Eye.mm
index 88edc9b1aa3..605f9204e08 100644
--- a/aten/src/ATen/native/mps/operations/Eye.mm
+++ b/aten/src/ATen/native/mps/operations/Eye.mm
@@ -30,7 +30,8 @@
 //
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor& eye_out_mps(int64_t n, Tensor& result) {
   // the default value of `m` equals to `n`
@@ -114,4 +115,5 @@ Tensor& eye_out_mps(int64_t n, int64_t m, Tensor& result) {
 }
 
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Indexing.mm b/aten/src/ATen/native/mps/operations/Indexing.mm
index 8522ac92027..a2b8e634cba 100644
--- a/aten/src/ATen/native/mps/operations/Indexing.mm
+++ b/aten/src/ATen/native/mps/operations/Indexing.mm
@@ -27,7 +27,8 @@
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 static
 bool dispatchIndexKernel(TensorIteratorBase& iter,
@@ -944,4 +945,5 @@ Tensor & masked_fill__mps(Tensor& self, const Tensor & mask, const Tensor & valu
 
 REGISTER_DISPATCH(index_stub, &index_kernel_mps);
 REGISTER_DISPATCH(index_put_stub, &index_put_kernel_mps);
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Inverse.mm b/aten/src/ATen/native/mps/operations/Inverse.mm
index 519de6afa3b..d75e839acf6 100644
--- a/aten/src/ATen/native/mps/operations/Inverse.mm
+++ b/aten/src/ATen/native/mps/operations/Inverse.mm
@@ -5,7 +5,8 @@
 #include <c10/util/Optional.h>
 
 
-namespace at::native {
+namespace at {
+namespace native {
 
 TORCH_IMPL_FUNC(linalg_inv_ex_out_mps)(const Tensor& A, bool check_errors, const Tensor& result, const Tensor& info)
 {
@@ -87,4 +88,5 @@ TORCH_IMPL_FUNC(linalg_inv_ex_out_mps)(const Tensor& A, bool check_errors, const
     }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Linear.mm b/aten/src/ATen/native/mps/operations/Linear.mm
index 529c26ded00..639e41c68ca 100644
--- a/aten/src/ATen/native/mps/operations/Linear.mm
+++ b/aten/src/ATen/native/mps/operations/Linear.mm
@@ -2,7 +2,8 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 using namespace mps;
 
@@ -356,4 +357,5 @@ std::tuple<Tensor, Tensor, Tensor> mps_linear_backward(
   return std::tuple<Tensor, Tensor, Tensor>{grad_input, grad_weight, grad_bias};
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/LinearAlgebra.mm b/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
index 6e3f1bc594a..180c81effb2 100644
--- a/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
+++ b/aten/src/ATen/native/mps/operations/LinearAlgebra.mm
@@ -4,7 +4,8 @@
 #include <ATen/native/LinearAlgebraUtils.h>
 #include <ATen/native/Resize.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 /*
  * Helper functions to be used for mm/addmm for detecting the Transpositions
@@ -842,4 +843,5 @@ TORCH_IMPL_FUNC(triangular_solve_mps_out)(const Tensor& self, const Tensor& A, b
   result.copy_(out);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/LossOps.mm b/aten/src/ATen/native/mps/operations/LossOps.mm
index 1a8c689003b..06269c27bf7 100644
--- a/aten/src/ATen/native/mps/operations/LossOps.mm
+++ b/aten/src/ATen/native/mps/operations/LossOps.mm
@@ -2,7 +2,8 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 string reductionToString(int64_t reduction)
@@ -1552,4 +1553,5 @@ Tensor nll_loss2d_backward_mps(
 }
 
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Normalization.mm b/aten/src/ATen/native/mps/operations/Normalization.mm
index 34dd5f75211..5742688b326 100644
--- a/aten/src/ATen/native/mps/operations/Normalization.mm
+++ b/aten/src/ATen/native/mps/operations/Normalization.mm
@@ -10,7 +10,8 @@
 #include <ATen/native/layer_norm.h>
 #include <torch/library.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void get_shapes(MPSShape* input_shape_readonly,
                 NSMutableArray<NSNumber*>* &input_shape,
@@ -1308,4 +1309,5 @@ std::tuple<Tensor, Tensor, Tensor> layer_norm_backward_mps(
 
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Pad.mm b/aten/src/ATen/native/mps/operations/Pad.mm
index d152dbe5eef..c6029e3d7b2 100644
--- a/aten/src/ATen/native/mps/operations/Pad.mm
+++ b/aten/src/ATen/native/mps/operations/Pad.mm
@@ -3,7 +3,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <c10/util/Optional.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 // Pad operations (1D/2D/3D forward and backward)
@@ -397,4 +398,5 @@ Tensor constant_pad_nd_mps(const Tensor& self, IntArrayRef pad, const Scalar& va
   return mps::pad_out_template(output, self, pad, c10::nullopt, MPSGraphPaddingModeConstant, value.toDouble(), __func__);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/PointwiseOps.mm b/aten/src/ATen/native/mps/operations/PointwiseOps.mm
index 92109c64caf..9ed62983687 100644
--- a/aten/src/ATen/native/mps/operations/PointwiseOps.mm
+++ b/aten/src/ATen/native/mps/operations/PointwiseOps.mm
@@ -2,7 +2,8 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 // scope the MPS's internal methods to not expose them to at::native
 namespace mps {
 
@@ -114,4 +115,5 @@ TORCH_IMPL_FUNC(addcdiv_out_mps)
   mps::addc_mul_div_out_mps(self, tensor1, tensor2, value, const_cast<Tensor&>(output), true, "addcdiv_out_mps");
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Pooling.mm b/aten/src/ATen/native/mps/operations/Pooling.mm
index ff26ff83518..0a471308514 100644
--- a/aten/src/ATen/native/mps/operations/Pooling.mm
+++ b/aten/src/ATen/native/mps/operations/Pooling.mm
@@ -3,7 +3,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/Pool.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 struct PoolingCachedGraph : public MPSCachedGraph
@@ -434,4 +435,5 @@ TORCH_IMPL_FUNC(avg_pool2d_backward_out_mps) (
                            {1, 1}, ceil_mode, count_include_pad, divisor_override, "avg_pool2d_backward");
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/RangeFactories.mm b/aten/src/ATen/native/mps/operations/RangeFactories.mm
index 9cfd1423621..73a639e5130 100644
--- a/aten/src/ATen/native/mps/operations/RangeFactories.mm
+++ b/aten/src/ATen/native/mps/operations/RangeFactories.mm
@@ -10,7 +10,8 @@
 #include <cmath>
 #include <limits>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 namespace {
 struct RangeCachedGraph : public mps::MPSCachedGraph {
@@ -268,4 +269,5 @@ Tensor& linspace_out_mps(const Scalar& start, const Scalar& end, int64_t steps,
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/ReduceOps.mm b/aten/src/ATen/native/mps/operations/ReduceOps.mm
index 5f8c0f64d8b..487e2be1311 100644
--- a/aten/src/ATen/native/mps/operations/ReduceOps.mm
+++ b/aten/src/ATen/native/mps/operations/ReduceOps.mm
@@ -12,7 +12,8 @@
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 #include <c10/util/irange.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 typedef MPSGraphTensor* (^NormOpBlock)(mps::MPSBinaryCachedGraph*, MPSGraphTensor*, MPSGraphTensor*);
 #define NormOpFn(graph, primary, secondary) MPSGraphTensor* (mps::MPSBinaryCachedGraph* graph, MPSGraphTensor* primary, MPSGraphTensor* secondary)
@@ -2056,4 +2057,5 @@ TORCH_API ::std::tuple<at::Tensor &,at::Tensor &> median_out_mps(
   return std::tuple<Tensor&, Tensor&>{values, indices};
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Repeat.mm b/aten/src/ATen/native/mps/operations/Repeat.mm
index d2155d2e7fe..bf00da8369a 100644
--- a/aten/src/ATen/native/mps/operations/Repeat.mm
+++ b/aten/src/ATen/native/mps/operations/Repeat.mm
@@ -15,7 +15,8 @@
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor permute_mps(const Tensor& self, IntArrayRef dims) {
   auto nDims = self.dim();
@@ -243,4 +244,5 @@ Tensor repeat_interleave_mps(const Tensor& repeat_, c10::optional<int64_t> outpu
   return output;
 }
 
-}  // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/RnnOps.mm b/aten/src/ATen/native/mps/operations/RnnOps.mm
index 9e59a6cf702..a09d1679016 100644
--- a/aten/src/ATen/native/mps/operations/RnnOps.mm
+++ b/aten/src/ATen/native/mps/operations/RnnOps.mm
@@ -12,7 +12,8 @@
 #import <MetalPerformanceShadersGraph/MPSGraphRNNOps.h>
 #include <torch/library.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 std::vector<long long> getTensorShape(MPSGraphTensor* mpsTensor) {
     std::vector<long long> output_dimensions = {};
@@ -584,4 +585,5 @@ std::tuple<Tensor, std::vector<Tensor>, std::vector<Tensor>> lstm_mps_backward(c
     }
 }
 
-} //namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Scalar.mm b/aten/src/ATen/native/mps/operations/Scalar.mm
index 73e099d1476..2a5d7fd700c 100644
--- a/aten/src/ATen/native/mps/operations/Scalar.mm
+++ b/aten/src/ATen/native/mps/operations/Scalar.mm
@@ -16,7 +16,8 @@
 
 using namespace at::mps;
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Scalar _local_scalar_dense_mps(const Tensor& self) {
   Scalar r;
@@ -34,4 +35,5 @@ Scalar _local_scalar_dense_mps(const Tensor& self) {
 }
 
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/ScatterGather.mm b/aten/src/ATen/native/mps/operations/ScatterGather.mm
index 62ae308cc25..5d145072971 100644
--- a/aten/src/ATen/native/mps/operations/ScatterGather.mm
+++ b/aten/src/ATen/native/mps/operations/ScatterGather.mm
@@ -2,7 +2,8 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 TORCH_IMPL_FUNC(gather_out_mps)
 (const Tensor & self_arg,
@@ -420,4 +421,5 @@ TORCH_IMPL_FUNC(scatter_add_mps_out)
   scatter_mps_general(self, dim, index, src, output, "scatter_add_mps_out", "add");
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Shape.mm b/aten/src/ATen/native/mps/operations/Shape.mm
index a4f70fe68ff..388b04f5450 100644
--- a/aten/src/ATen/native/mps/operations/Shape.mm
+++ b/aten/src/ATen/native/mps/operations/Shape.mm
@@ -7,7 +7,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // Produces a shape with the `dim` dimension set to 0.
 std::vector<int64_t> getTopK0Shape(IntArrayRef sizes, const int64_t dim_) {
@@ -445,4 +446,5 @@ TORCH_IMPL_FUNC(cat_out_mps)
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/SoftMax.mm b/aten/src/ATen/native/mps/operations/SoftMax.mm
index 59d3fd61eea..e96d5ed2481 100644
--- a/aten/src/ATen/native/mps/operations/SoftMax.mm
+++ b/aten/src/ATen/native/mps/operations/SoftMax.mm
@@ -13,7 +13,8 @@
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 #endif
 
-namespace at::native {
+namespace at {
+namespace native {
 
 void get_shapes(MPSShape* input_shape_readonly,
                 NSMutableArray<NSNumber*>* &input_shape,
@@ -271,4 +272,5 @@ TORCH_IMPL_FUNC(softmax_backward_mps_out)
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Sort.mm b/aten/src/ATen/native/mps/operations/Sort.mm
index 4b3bb692ac0..5eb1fd63ece 100644
--- a/aten/src/ATen/native/mps/operations/Sort.mm
+++ b/aten/src/ATen/native/mps/operations/Sort.mm
@@ -7,7 +7,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 // sort
 TORCH_IMPL_FUNC(sort_stable_out_mps)
diff --git a/aten/src/ATen/native/mps/operations/SummaryOps.mm b/aten/src/ATen/native/mps/operations/SummaryOps.mm
index 8677e14b8b7..c05c5079454 100644
--- a/aten/src/ATen/native/mps/operations/SummaryOps.mm
+++ b/aten/src/ATen/native/mps/operations/SummaryOps.mm
@@ -2,7 +2,8 @@
 
 #include <ATen/native/mps/OperationUtils.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 Tensor& bincount_mps_impl(const Tensor& self,
                           const Tensor& weights,
@@ -150,4 +151,5 @@ Tensor _bincount_mps(const Tensor& self, const c10::optional<Tensor>& weights_op
   return bincount_mps_impl(self, weights_, output);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/TensorCompare.mm b/aten/src/ATen/native/mps/operations/TensorCompare.mm
index 4f8def1cbb7..df0dab82d94 100644
--- a/aten/src/ATen/native/mps/operations/TensorCompare.mm
+++ b/aten/src/ATen/native/mps/operations/TensorCompare.mm
@@ -4,7 +4,8 @@
 #include <ATen/native/TensorCompare.h>
 #include <ATen/native/Resize.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 struct CachedGraph : public MPSCachedGraph
@@ -541,4 +542,5 @@ Tensor& nan_to_num_out_mps(const Tensor& self,
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/TriangularOps.mm b/aten/src/ATen/native/mps/operations/TriangularOps.mm
index a4b0db98b0f..22c69b20cb1 100644
--- a/aten/src/ATen/native/mps/operations/TriangularOps.mm
+++ b/aten/src/ATen/native/mps/operations/TriangularOps.mm
@@ -11,7 +11,8 @@
 
 #include <MetalPerformanceShaders/MetalPerformanceShaders.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 
 TORCH_IMPL_FUNC(triu_mps_out)
 (const Tensor& self,
@@ -179,4 +180,5 @@ TORCH_IMPL_FUNC(tril_mps_out)
 
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/UnaryOps.mm b/aten/src/ATen/native/mps/operations/UnaryOps.mm
index 210c214834a..ca24c7545db 100644
--- a/aten/src/ATen/native/mps/operations/UnaryOps.mm
+++ b/aten/src/ATen/native/mps/operations/UnaryOps.mm
@@ -3,7 +3,8 @@
 #include <ATen/native/mps/OperationUtils.h>
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 typedef MPSGraphTensor* (^UnaryOpBlock)(MPSGraph*, MPSGraphTensor*);
@@ -285,4 +286,5 @@ TORCH_IMPL_FUNC(cumsum_out_mps)
     });
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/Unique.mm b/aten/src/ATen/native/mps/operations/Unique.mm
index eac16a74564..a2a573f2c0c 100644
--- a/aten/src/ATen/native/mps/operations/Unique.mm
+++ b/aten/src/ATen/native/mps/operations/Unique.mm
@@ -4,7 +4,8 @@
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 #include <ATen/native/Resize.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 struct UniqueCachedGraph : public MPSCachedGraph
@@ -358,4 +359,5 @@ _unique2_mps(const Tensor& self, const bool sorted, const bool return_inverse, c
   return _unique_impl_mps(self, return_inverse, return_counts, false, c10::nullopt);
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/UpSample.mm b/aten/src/ATen/native/mps/operations/UpSample.mm
index 3b781dea08f..fa8b8a5cfd5 100644
--- a/aten/src/ATen/native/mps/operations/UpSample.mm
+++ b/aten/src/ATen/native/mps/operations/UpSample.mm
@@ -4,7 +4,8 @@
 #include <ATen/native/mps/MPSGraphVenturaOps.h>
 #include <ATen/native/UpSample.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 // Upsampling operations (1D/2D forward and backward)
@@ -397,4 +398,5 @@ TORCH_IMPL_FUNC(upsample_bilinear2d_backward_out_mps) (
   }
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/mps/operations/View.mm b/aten/src/ATen/native/mps/operations/View.mm
index b6df6e3f654..d0e9850e4c8 100644
--- a/aten/src/ATen/native/mps/operations/View.mm
+++ b/aten/src/ATen/native/mps/operations/View.mm
@@ -7,7 +7,8 @@
 #include <torch/library.h>
 #include <ATen/mps/IndexKernels.h>
 
-namespace at::native {
+namespace at {
+namespace native {
 namespace mps {
 
 struct ViewCachedGraph : public MPSCachedGraph
@@ -948,4 +949,5 @@ Tensor as_strided_tensorimpl_mps(const Tensor& self, IntArrayRef size, IntArrayR
   return result;
 }
 
-} // namespace at::native
+} // namespace native
+} // namespace at
diff --git a/aten/src/ATen/native/quantized/README.md b/aten/src/ATen/native/quantized/README.md
index f042881a8ce..09417d81bcc 100644
--- a/aten/src/ATen/native/quantized/README.md
+++ b/aten/src/ATen/native/quantized/README.md
@@ -130,7 +130,7 @@ namespace at {
   TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
     m.impl("xand", TORCH_FN(quantized_xand));
   }
-}}  // namespace at::native
+}} // namespace at::native
 ```
 
 ### Step 3. Administrative stuff
diff --git a/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp b/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp
index c6312e033bf..83ae98ae375 100644
--- a/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp
+++ b/aten/src/ATen/native/quantized/cpu/BinaryOps.cpp
@@ -487,4 +487,4 @@ Tensor quantized_add(Tensor qa, Tensor qb, double scale, int64_t zero_point){
   return qadd<false>(std::move(qa), std::move(qb), scale, zero_point);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/Sorting.cpp b/aten/src/ATen/native/quantized/cpu/Sorting.cpp
index 9389261ac1e..d03bea56742 100644
--- a/aten/src/ATen/native/quantized/cpu/Sorting.cpp
+++ b/aten/src/ATen/native/quantized/cpu/Sorting.cpp
@@ -64,4 +64,4 @@ std::tuple<Tensor, Tensor> topk_quantized_cpu(
 
 DEFINE_DISPATCH(qtopk_stub);
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qdropout.cpp b/aten/src/ATen/native/quantized/cpu/qdropout.cpp
index 1331f5ed1ee..6f5c289210e 100644
--- a/aten/src/ATen/native/quantized/cpu/qdropout.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qdropout.cpp
@@ -18,4 +18,4 @@ TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
   m.impl(TORCH_SELECTIVE_NAME("quantized::dropout"), quantized_dropout);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qelu.cpp b/aten/src/ATen/native/quantized/cpu/qelu.cpp
index f8b66781f2e..d1353be56e8 100644
--- a/aten/src/ATen/native/quantized/cpu/qelu.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qelu.cpp
@@ -34,4 +34,4 @@ TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
   m.impl(TORCH_SELECTIVE_NAME("quantized::celu"), quantized_celu);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qgelu.cpp b/aten/src/ATen/native/quantized/cpu/qgelu.cpp
index f9a3c32343d..679bc6e68c2 100644
--- a/aten/src/ATen/native/quantized/cpu/qgelu.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qgelu.cpp
@@ -18,4 +18,4 @@ Tensor gelu_quantized_cpu(const Tensor& qx, c10::string_view approximate) {
   qgelu_stub(qx.device().type(), qx, qy, get_gelutype_enum(approximate));
   return qy;
 }
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp b/aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp
index aa37e51e7ea..d6c5f818b50 100644
--- a/aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qhardsigmoid.cpp
@@ -109,4 +109,4 @@ Tensor& hardsigmoid_out_quantized_cpu(const Tensor& qx, Tensor& result) {
   return result;
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qhardswish.cpp b/aten/src/ATen/native/quantized/cpu/qhardswish.cpp
index bf4e0d98829..0aa07a9a98f 100644
--- a/aten/src/ATen/native/quantized/cpu/qhardswish.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qhardswish.cpp
@@ -103,4 +103,4 @@ TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
   m.impl(TORCH_SELECTIVE_NAME("quantized::hardswish"), TORCH_FN(quantized_hardswish));
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qmul.cpp b/aten/src/ATen/native/quantized/cpu/qmul.cpp
index aa6ad0e724f..0e514b488b6 100644
--- a/aten/src/ATen/native/quantized/cpu/qmul.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qmul.cpp
@@ -358,4 +358,4 @@ TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
 }
 
 }  // namespace
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qrelu.cpp b/aten/src/ATen/native/quantized/cpu/qrelu.cpp
index 739b5c5af30..82c99a650a6 100644
--- a/aten/src/ATen/native/quantized/cpu/qrelu.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qrelu.cpp
@@ -232,4 +232,4 @@ TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
 
 } // namespace
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qsigmoid.cpp b/aten/src/ATen/native/quantized/cpu/qsigmoid.cpp
index ed4308adb8f..a4617b0ce15 100644
--- a/aten/src/ATen/native/quantized/cpu/qsigmoid.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qsigmoid.cpp
@@ -149,4 +149,4 @@ TORCH_LIBRARY_IMPL(quantized, QuantizedCPU, m) {
 }
 } // namespace
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cpu/qtanh.cpp b/aten/src/ATen/native/quantized/cpu/qtanh.cpp
index 5dc3e759ede..a039123a374 100644
--- a/aten/src/ATen/native/quantized/cpu/qtanh.cpp
+++ b/aten/src/ATen/native/quantized/cpu/qtanh.cpp
@@ -100,4 +100,4 @@ Tensor tanh_quantized_cpu(const Tensor& qx) {
   qtanh_stub(qx.device().type(), qx, qy);
   return qy;
 }
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/quantized/cuda/Activation.cpp b/aten/src/ATen/native/quantized/cuda/Activation.cpp
index 3a9e400fa81..091ce0aeff8 100644
--- a/aten/src/ATen/native/quantized/cuda/Activation.cpp
+++ b/aten/src/ATen/native/quantized/cuda/Activation.cpp
@@ -26,5 +26,5 @@ Tensor relu_quantized_cuda(const Tensor& self) {
   return at::_make_per_tensor_quantized_tensor(relu_int_repr, self.q_scale(), zero_point);
 }
 
-}  // namespace at::native
+} // namespace at::native
 }  // namespace at
diff --git a/aten/src/ATen/native/quantized/cuda/Activation.cu b/aten/src/ATen/native/quantized/cuda/Activation.cu
index 9e3e3ba13ea..0190f0146ee 100644
--- a/aten/src/ATen/native/quantized/cuda/Activation.cu
+++ b/aten/src/ATen/native/quantized/cuda/Activation.cu
@@ -17,5 +17,5 @@ Tensor& relu_quantized_cuda_(Tensor& self) {
   return self;
 }
 
-}  // namespace at::native
+} // namespace at::native
 }  // namespace at
diff --git a/aten/src/ATen/native/sparse/SparseUnaryOps.cpp b/aten/src/ATen/native/sparse/SparseUnaryOps.cpp
index 9e0503337b5..573f3a9b3f4 100644
--- a/aten/src/ATen/native/sparse/SparseUnaryOps.cpp
+++ b/aten/src/ATen/native/sparse/SparseUnaryOps.cpp
@@ -261,4 +261,4 @@ Tensor& nan_to_num_sparse_(
   return nan_to_num_sparse_out(self, nan, posinf, neginf, self);
 }
 
-}}  // namespace at::native
+}} // namespace at::native
diff --git a/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp b/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp
index 5dccb15060f..833fd41eb6a 100644
--- a/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp
+++ b/aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp
@@ -42,7 +42,7 @@ c10::MaybeOwned<Tensor> prepare_column_major_matrix_for_cusparse(
 
 c10::MaybeOwned<Tensor> inline prepare_dense_matrix_for_cusparse(
     const Tensor& tensor) {
-#if defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION < 11000
   // CUDA < 11.0 doesn't support row-major layout, return column-major in this case
   return prepare_column_major_matrix_for_cusparse(tensor);
 #else
@@ -612,7 +612,7 @@ void spmm(
 
   // CUDA < 11.0 doesn't support 64-bit indices and doesn't raise an error about this
   // silently returning incorrect results
-#if defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION < 11000
   auto mat1_32 = at::native::_sparse_csr_tensor_unsafe(
       mat1.crow_indices().to(kInt),
       mat1.col_indices().to(kInt),
@@ -689,7 +689,13 @@ void spgemm(
     const Scalar& beta,
     const Scalar& alpha,
     const at::sparse_csr::SparseCsrTensor& C) {
-#if defined(USE_ROCM) && ROCM_VERSION < 50200
+#if (!defined(USE_ROCM)) && (defined(CUDA_VERSION) && CUDA_VERSION < 11000)
+  TORCH_CHECK(
+      false,
+      "Calling addmm with sparse GPU tensors requires compiling ",
+      "PyTorch with CUDA 11+. ",
+      "Please use PyTorch built with newer CUDA version.");
+#elif defined(USE_ROCM) && ROCM_VERSION < 50200
   TORCH_CHECK(
       false,
       "Calling addmm with sparse GPU tensors requires compiling ",
diff --git a/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu b/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu
index 91466770a92..d388864f0b0 100644
--- a/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu
+++ b/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu
@@ -832,7 +832,7 @@ Tensor& bmm_out_sparse_cuda(const SparseTensor& self, const Tensor& mat2, Tensor
 
   // See Note [Enabling Deterministic Operations]
   bool deterministic =  globalContext().deterministicAlgorithms();
-  cusparseSpMMAlg_t mm_alg = deterministic ? CUSPARSE_SPMM_COO_ALG2 : CUSPARSE_SPMM_COO_ALG1;
+  cusparseSpMMAlg_t mm_alg = deterministic ? CUSPARSE_COOMM_ALG2 : CUSPARSE_COOMM_ALG1;
 
   // Iterate through each set of 2D matrices within the 3D
   // tensor inputs, performing a matrix multiply with each
diff --git a/aten/src/ATen/native/ufunc/add.h b/aten/src/ATen/native/ufunc/add.h
index 94a776728ea..3d858e3bcf0 100644
--- a/aten/src/ATen/native/ufunc/add.h
+++ b/aten/src/ATen/native/ufunc/add.h
@@ -24,4 +24,4 @@ C10_ALWAYS_INLINE Vectorized<T> add(Vectorized<T> self, Vectorized<T> other, Vec
 }
 #endif
 
-}}}  // namespace at::native::ufunc
+}}} // namespace at::native::ufunc
diff --git a/c10/util/BFloat16-inl.h b/c10/util/BFloat16-inl.h
index c2ad4d20377..210743acfab 100644
--- a/c10/util/BFloat16-inl.h
+++ b/c10/util/BFloat16-inl.h
@@ -22,7 +22,7 @@ namespace c10 {
 /// Constructors
 inline C10_HOST_DEVICE BFloat16::BFloat16(float value)
     :
-#if defined(__CUDACC__) && !defined(USE_ROCM) && defined(__CUDA_ARCH__) && \
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && defined(__CUDA_ARCH__) && \
     __CUDA_ARCH__ >= 800
       x(__bfloat16_as_ushort(__float2bfloat16(value)))
 #elif defined(__SYCL_DEVICE_ONLY__) && \
@@ -37,7 +37,7 @@ inline C10_HOST_DEVICE BFloat16::BFloat16(float value)
 
 /// Implicit conversions
 inline C10_HOST_DEVICE BFloat16::operator float() const {
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   return __bfloat162float(*reinterpret_cast<const __nv_bfloat16*>(&x));
 #elif defined(__SYCL_DEVICE_ONLY__) && \
     defined(SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS)
@@ -47,7 +47,7 @@ inline C10_HOST_DEVICE BFloat16::operator float() const {
 #endif
 }
 
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 inline C10_HOST_DEVICE BFloat16::BFloat16(const __nv_bfloat16& value) {
   x = *reinterpret_cast<const unsigned short*>(&value);
 }
diff --git a/c10/util/BFloat16.h b/c10/util/BFloat16.h
index cf20f7a3b90..220c121674d 100644
--- a/c10/util/BFloat16.h
+++ b/c10/util/BFloat16.h
@@ -7,7 +7,7 @@
 #include <cmath>
 #include <cstring>
 
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
 #include <cuda_bf16.h>
 #endif
 
@@ -99,7 +99,7 @@ struct alignas(2) BFloat16 {
   inline C10_HOST_DEVICE BFloat16(float value);
   inline C10_HOST_DEVICE operator float() const;
 
-#if defined(__CUDACC__) && !defined(USE_ROCM)
+#if defined(CUDA_VERSION) && CUDA_VERSION >= 11000
   inline C10_HOST_DEVICE BFloat16(const __nv_bfloat16& value);
   explicit inline C10_HOST_DEVICE operator __nv_bfloat16() const;
 #endif
diff --git a/c10/util/safe_numerics.h b/c10/util/safe_numerics.h
index e5c249dd1d2..865d747ea30 100644
--- a/c10/util/safe_numerics.h
+++ b/c10/util/safe_numerics.h
@@ -12,7 +12,7 @@
 #include <c10/util/llvmMathExtras.h>
 #include <intrin.h>
 #else
-#define C10_HAS_BUILTIN_OVERFLOW() (1)
+#define C10_HAS_BUILTIN_OVERFLOW() (0)
 #endif
 
 namespace c10 {
@@ -21,14 +21,9 @@ C10_ALWAYS_INLINE bool add_overflows(uint64_t a, uint64_t b, uint64_t* out) {
 #if C10_HAS_BUILTIN_OVERFLOW()
   return __builtin_add_overflow(a, b, out);
 #else
-  unsigned long long tmp;
-#if defined(_M_IX86) || defined(_M_X64)
-  auto carry = _addcarry_u64(0, a, b, &tmp);
-#else
-  tmp = a + b;
+  unsigned long long tmp = a + b;
   unsigned long long vector = (a & b) ^ ((a ^ b) & ~tmp);
   auto carry = vector >> 63;
-#endif
   *out = tmp;
   return carry;
 #endif
diff --git a/cmake/Dependencies.cmake b/cmake/Dependencies.cmake
index 8c0e3c24bc5..1a6ea26c5b8 100644
--- a/cmake/Dependencies.cmake
+++ b/cmake/Dependencies.cmake
@@ -1089,9 +1089,9 @@ if(BUILD_PYTHON)
     message(FATAL_ERROR
       "Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python 2 has reached end-of-life and is no longer supported by PyTorch.")
   endif()
-  if(${PYTHONLIBS_VERSION_STRING} VERSION_LESS 3.8)
+  if(${PYTHONLIBS_VERSION_STRING} VERSION_LESS 3.7)
     message(FATAL_ERROR
-      "Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python < 3.8 is no longer supported by PyTorch.")
+      "Found Python libraries version ${PYTHONLIBS_VERSION_STRING}. Python < 3.7 is no longer supported by PyTorch.")
   endif()
 
   # When building pytorch, we pass this in directly from setup.py, and
@@ -1350,8 +1350,8 @@ if(USE_NCCL)
         "Not using CUDA/ROCM, so disabling USE_NCCL. Suppress this warning with "
         "-DUSE_NCCL=OFF.")
     caffe2_update_option(USE_NCCL OFF)
-  elseif(NOT CMAKE_SYSTEM_NAME STREQUAL "Linux")
-    message(WARNING "NCCL is currently only supported under Linux.")
+  elseif(NOT (CMAKE_SYSTEM_NAME STREQUAL "Linux" OR CMAKE_SYSTEM_NAME STREQUAL "Darwin"))
+    message(WARNING "NCCL is currently only supported under Linux and macOS.")
     caffe2_update_option(USE_NCCL OFF)
   elseif(USE_CUDA)
     include(${CMAKE_CURRENT_LIST_DIR}/External/nccl.cmake)
diff --git a/cmake/public/cuda.cmake b/cmake/public/cuda.cmake
index df40ff7d2da..85c3a03cecd 100644
--- a/cmake/public/cuda.cmake
+++ b/cmake/public/cuda.cmake
@@ -51,8 +51,8 @@ set(CMAKE_CUDA_STANDARD_REQUIRED ON)
 message(STATUS "Caffe2: CUDA detected: " ${CUDA_VERSION})
 message(STATUS "Caffe2: CUDA nvcc is: " ${CUDA_NVCC_EXECUTABLE})
 message(STATUS "Caffe2: CUDA toolkit directory: " ${CUDA_TOOLKIT_ROOT_DIR})
-if(CUDA_VERSION VERSION_LESS 11.0)
-  message(FATAL_ERROR "PyTorch requires CUDA 11.0 or above.")
+if(CUDA_VERSION VERSION_LESS 10.1)
+  message(FATAL_ERROR "PyTorch requires CUDA 10.1 or above.")
 endif()
 
 if(CUDA_FOUND)
@@ -273,8 +273,8 @@ if(CAFFE2_USE_CUDNN)
       "Cannot find cuDNN library. Turning the option off")
     set(CAFFE2_USE_CUDNN OFF)
   else()
-    if(CUDNN_VERSION VERSION_LESS "8.0.0")
-      message(FATAL_ERROR "PyTorch requires cuDNN 8 and above.")
+    if(CUDNN_VERSION VERSION_LESS "7.0.0")
+      message(FATAL_ERROR "PyTorch requires cuDNN 7 and above.")
     endif()
   endif()
 
diff --git a/docs/source/torch.rst b/docs/source/torch.rst
index a4f0a2c721e..3f7649790a5 100644
--- a/docs/source/torch.rst
+++ b/docs/source/torch.rst
@@ -682,7 +682,6 @@ Utilities
     get_float32_matmul_precision
     set_warn_always
     is_warn_always_enabled
-    vmap
     _assert
 
 Symbolic Numbers
diff --git a/functorch/docs/source/conf.py b/functorch/docs/source/conf.py
index 7224273616d..f6c1a6beee3 100644
--- a/functorch/docs/source/conf.py
+++ b/functorch/docs/source/conf.py
@@ -90,7 +90,7 @@ master_doc = 'index'
 project = 'functorch'
 copyright = 'PyTorch Contributors'
 author = 'PyTorch Contributors'
-functorch_version = str(functorch.__version__)
+functorch_version = '1.13'
 
 # The version info for the project you're documenting, acts as replacement for
 # |version| and |release|, also used in various other places throughout the
@@ -98,10 +98,10 @@ functorch_version = str(functorch.__version__)
 #
 # The short X.Y version.
 # TODO: change to [:2] at v1.0
-version = 'nightly (' + functorch_version + ')'
+version = functorch_version
 # The full version, including alpha/beta/rc tags.
 # TODO: verify this works as expected
-release = 'nightly'
+release = functorch_version
 
 # Customized html_title here.
 # Default is " ".join(project, release, "documentation") if not set
diff --git a/functorch/notebooks/aot_autograd_optimizations.ipynb b/functorch/notebooks/aot_autograd_optimizations.ipynb
index 5f896f8223f..3b23b0a0a18 100644
--- a/functorch/notebooks/aot_autograd_optimizations.ipynb
+++ b/functorch/notebooks/aot_autograd_optimizations.ipynb
@@ -6,7 +6,7 @@
    "source": [
     "# AOT Autograd - How to use and optimize?\n",
     "\n",
-    "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/master/functorch/notebooks/aot_autograd_optimizations.ipynb\">\n",
+    "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/release/1.13/functorch/notebooks/aot_autograd_optimizations.ipynb\">\n",
     "  <img style=\"width: auto\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
     "</a>\n",
     "\n",
diff --git a/functorch/notebooks/ensembling.ipynb b/functorch/notebooks/ensembling.ipynb
index 1ecc8738b0b..ea280dc55a2 100644
--- a/functorch/notebooks/ensembling.ipynb
+++ b/functorch/notebooks/ensembling.ipynb
@@ -11,7 +11,7 @@
         "\n",
         "This example illustrates how to vectorize model ensembling using vmap.\n",
         "\n",
-        "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/master/functorch/notebooks/ensembling.ipynb\">\n",
+        "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/release/1.13/functorch/notebooks/ensembling.ipynb\">\n",
         "  <img style=\"width: auto\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
         "</a>\n",
         "\n",
diff --git a/functorch/notebooks/jacobians_hessians.ipynb b/functorch/notebooks/jacobians_hessians.ipynb
index 5b986a592b7..aa612978d29 100644
--- a/functorch/notebooks/jacobians_hessians.ipynb
+++ b/functorch/notebooks/jacobians_hessians.ipynb
@@ -5,7 +5,7 @@
       "source": [
         "# Jacobians, Hessians, hvp, vhp, and more: composing functorch transforms\n",
         "\n",
-        "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/master/functorch/notebooks/jacobians_hessians.ipynb\">\n",
+        "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/release/1.13/functorch/notebooks/jacobians_hessians.ipynb\">\n",
         "  <img style=\"width: auto\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
         "</a>\n",
         "\n",
diff --git a/functorch/notebooks/neural_tangent_kernels.ipynb b/functorch/notebooks/neural_tangent_kernels.ipynb
index 9d041be9092..6d590302912 100644
--- a/functorch/notebooks/neural_tangent_kernels.ipynb
+++ b/functorch/notebooks/neural_tangent_kernels.ipynb
@@ -7,7 +7,7 @@
    "source": [
     "# Neural Tangent Kernels\n",
     "\n",
-    "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/master/functorch/notebooks/neural_tangent_kernels.ipynb\">\n",
+    "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/release/1.13/functorch/notebooks/neural_tangent_kernels.ipynb\">\n",
     "  <img style=\"width: auto\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
     "</a>\n",
     "\n",
diff --git a/functorch/notebooks/per_sample_grads.ipynb b/functorch/notebooks/per_sample_grads.ipynb
index 5f7ad23880b..61da7128cce 100644
--- a/functorch/notebooks/per_sample_grads.ipynb
+++ b/functorch/notebooks/per_sample_grads.ipynb
@@ -9,7 +9,7 @@
       "source": [
         "# Per-sample-gradients\n",
         "\n",
-        "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/master/functorch/notebooks/per_sample_grads.ipynb\">\n",
+        "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/release/1.13/functorch/notebooks/per_sample_grads.ipynb\">\n",
         "  <img style=\"width: auto\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
         "</a>\n",
         "\n",
diff --git a/functorch/notebooks/whirlwind_tour.ipynb b/functorch/notebooks/whirlwind_tour.ipynb
index deae3418966..b0264e392f3 100644
--- a/functorch/notebooks/whirlwind_tour.ipynb
+++ b/functorch/notebooks/whirlwind_tour.ipynb
@@ -7,7 +7,7 @@
    "source": [
     "# Whirlwind Tour\n",
     "\n",
-    "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/master/functorch/notebooks/whirlwind_tour.ipynb\">\n",
+    "<a href=\"https://colab.research.google.com/github/pytorch/pytorch/blob/release/1.13/functorch/notebooks/whirlwind_tour.ipynb\">\n",
     "  <img style=\"width: auto\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
     "</a>\n",
     "\n",
diff --git a/functorch/packaging/pkg_helpers.bash b/functorch/packaging/pkg_helpers.bash
new file mode 100644
index 00000000000..5286914a65a
--- /dev/null
+++ b/functorch/packaging/pkg_helpers.bash
@@ -0,0 +1,414 @@
+# A set of useful bash functions for common functionality we need to do in
+# many build scripts
+
+
+# Setup CUDA environment variables, based on CU_VERSION
+#
+# Inputs:
+#   CU_VERSION (cpu, cu92, cu100)
+#   NO_CUDA_PACKAGE (bool)
+#   BUILD_TYPE (conda, wheel)
+#
+# Outputs:
+#   VERSION_SUFFIX (e.g., "")
+#   PYTORCH_VERSION_SUFFIX (e.g., +cpu)
+#   WHEEL_DIR (e.g., cu100/)
+#   CUDA_HOME (e.g., /usr/local/cuda-9.2, respected by torch.utils.cpp_extension)
+#   FORCE_CUDA (respected by torchvision setup.py)
+#   NVCC_FLAGS (respected by torchvision setup.py)
+#
+# Precondition: CUDA versions are installed in their conventional locations in
+# /usr/local/cuda-*
+#
+# NOTE: Why VERSION_SUFFIX versus PYTORCH_VERSION_SUFFIX?  If you're building
+# a package with CUDA on a platform we support CUDA on, VERSION_SUFFIX ==
+# PYTORCH_VERSION_SUFFIX and everyone is happy.  However, if you are building a
+# package with only CPU bits (e.g., torchaudio), then VERSION_SUFFIX is always
+# empty, but PYTORCH_VERSION_SUFFIX is +cpu (because that's how you get a CPU
+# version of a Python package.  But that doesn't apply if you're on OS X,
+# since the default CU_VERSION on OS X is cpu.
+setup_cuda() {
+
+  # First, compute version suffixes.  By default, assume no version suffixes
+  export VERSION_SUFFIX=""
+  export PYTORCH_VERSION_SUFFIX=""
+  export WHEEL_DIR=""
+  # Wheel builds need suffixes (but not if they're on OS X, which never has suffix)
+  if [[ "$BUILD_TYPE" == "wheel" ]] && [[ "$(uname)" != Darwin ]]; then
+    export PYTORCH_VERSION_SUFFIX="+$CU_VERSION"
+    # Match the suffix scheme of pytorch, unless this package does not have
+    # CUDA builds (in which case, use default)
+    if [[ -z "$NO_CUDA_PACKAGE" ]]; then
+      export VERSION_SUFFIX="$PYTORCH_VERSION_SUFFIX"
+      export WHEEL_DIR="$CU_VERSION/"
+    fi
+  fi
+
+  # Now work out the CUDA settings
+  case "$CU_VERSION" in
+    cu115)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.5"
+      else
+        export CUDA_HOME=/usr/local/cuda-11.5/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5;8.0;8.6"
+      ;;
+    cu113)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3"
+      else
+        export CUDA_HOME=/usr/local/cuda-11.3/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5;8.0;8.6"
+      ;;
+    cu112)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2"
+      else
+        export CUDA_HOME=/usr/local/cuda-11.2/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5;8.0;8.6"
+      ;;
+    cu111)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.1"
+      else
+        export CUDA_HOME=/usr/local/cuda-11.1/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5;8.0;8.6"
+      ;;
+    cu110)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0"
+      else
+        export CUDA_HOME=/usr/local/cuda-11.0/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5;8.0"
+      ;;
+    cu102)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.2"
+      else
+        export CUDA_HOME=/usr/local/cuda-10.2/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5"
+      ;;
+    cu101)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.1"
+      else
+        export CUDA_HOME=/usr/local/cuda-10.1/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5"
+      ;;
+    cu100)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v10.0"
+      else
+        export CUDA_HOME=/usr/local/cuda-10.0/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0;7.5"
+      ;;
+    cu92)
+      if [[ "$OSTYPE" == "msys" ]]; then
+        export CUDA_HOME="C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.2"
+      else
+        export CUDA_HOME=/usr/local/cuda-9.2/
+      fi
+      export TORCH_CUDA_ARCH_LIST="3.5;5.0+PTX;6.0;7.0"
+      ;;
+    cpu)
+      ;;
+    rocm*)
+      export FORCE_CUDA=1
+      ;;
+    *)
+      echo "Unrecognized CU_VERSION=$CU_VERSION"
+      exit 1
+      ;;
+  esac
+  if [[ -n "$CUDA_HOME" ]]; then
+    # Adds nvcc binary to the search path so that CMake's `find_package(CUDA)` will pick the right one
+    export PATH="$CUDA_HOME/bin:$PATH"
+    export FORCE_CUDA=1
+  fi
+}
+
+# Populate build version if necessary, and add version suffix
+#
+# Inputs:
+#   BUILD_VERSION (e.g., 0.2.0 or empty)
+#   VERSION_SUFFIX (e.g., +cpu)
+#
+# Outputs:
+#   BUILD_VERSION (e.g., 0.2.0.dev20190807+cpu)
+#
+# Fill BUILD_VERSION if it doesn't exist already with a nightly string
+# Usage: setup_build_version 0.2.0
+setup_build_version() {
+  if [[ -z "$BUILD_VERSION" ]]; then
+    export BUILD_VERSION="$1.dev$(date "+%Y%m%d")$VERSION_SUFFIX"
+  else
+    export BUILD_VERSION="$BUILD_VERSION$VERSION_SUFFIX"
+  fi
+
+  # Set build version based on tag if on tag
+  if [[ -n "${CIRCLE_TAG}" ]]; then
+    # Strip tag
+    export BUILD_VERSION="$(echo "${CIRCLE_TAG}" | sed -e 's/^v//' -e 's/-.*$//')${VERSION_SUFFIX}"
+  fi
+}
+
+# Set some useful variables for OS X, if applicable
+setup_macos() {
+  if [[ "$(uname)" == Darwin ]]; then
+    export MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++
+  fi
+}
+
+
+# Top-level entry point for things every package will need to do
+#
+# Usage: setup_env 0.2.0
+setup_env() {
+  setup_cuda
+  setup_build_version "$1"
+  setup_macos
+}
+
+# Function to retry functions that sometimes timeout or have flaky failures
+retry () {
+    $*  || (sleep 1 && $*) || (sleep 2 && $*) || (sleep 4 && $*) || (sleep 8 && $*)
+}
+
+# Inputs:
+#   PYTHON_VERSION (3.7, 3.8, 3.9)
+#   UNICODE_ABI (bool)
+#
+# Outputs:
+#   PATH modified to put correct Python version in PATH
+#
+# Precondition: If Linux, you are in a soumith/manylinux-cuda* Docker image
+setup_wheel_python() {
+  if [[ "$(uname)" == Darwin || "$OSTYPE" == "msys" ]]; then
+    eval "$(conda shell.bash hook)"
+    conda env remove -n "env$PYTHON_VERSION" || true
+    conda create ${CONDA_CHANNEL_FLAGS} -yn "env$PYTHON_VERSION" python="$PYTHON_VERSION"
+    conda activate "env$PYTHON_VERSION"
+    # Install libpng from Anaconda (defaults)
+    conda install ${CONDA_CHANNEL_FLAGS} libpng "jpeg<=9b" -y
+  else
+    # Install native CentOS libJPEG, freetype and GnuTLS
+    yum install -y libjpeg-turbo-devel freetype gnutls
+    case "$PYTHON_VERSION" in
+      3.7) python_abi=cp37-cp37m ;;
+      3.8) python_abi=cp38-cp38 ;;
+      3.9) python_abi=cp39-cp39 ;;
+      3.10) python_abi=cp310-cp310 ;;
+      *)
+        echo "Unrecognized PYTHON_VERSION=$PYTHON_VERSION"
+        exit 1
+        ;;
+    esac
+    # Download all the dependencies required to compile image and video_reader
+    # extensions
+
+    mkdir -p ext_libraries
+    pushd ext_libraries
+    popd
+    export PATH="/opt/python/$python_abi/bin:$(pwd)/ext_libraries/bin:$PATH"
+  fi
+}
+
+# Install with pip a bit more robustly than the default
+pip_install() {
+  retry pip install --progress-bar off "$@"
+}
+
+# Install torch with pip, respecting PYTORCH_VERSION, and record the installed
+# version into PYTORCH_VERSION, if applicable
+setup_pip_pytorch_version() {
+  if [[ -z "$PYTORCH_VERSION" ]]; then
+    # Install latest prerelease version of torch, per our nightlies, consistent
+    # with the requested cuda version
+    pip_install --pre torch -f "https://download.pytorch.org/whl/test/${WHEEL_DIR}torch_test.html"
+    if [[ "$CUDA_VERSION" == "cpu" ]]; then
+      # CUDA and CPU are ABI compatible on the CPU-only parts, so strip
+      # in this case
+      export PYTORCH_VERSION="$(pip show torch | grep ^Version: | sed 's/Version:  *//' | sed 's/+.\+//')"
+    else
+      export PYTORCH_VERSION="$(pip show torch | grep ^Version: | sed 's/Version:  *//')"
+    fi
+  else
+    pip_install "torch==$PYTORCH_VERSION$PYTORCH_VERSION_SUFFIX" \
+      -f "https://download.pytorch.org/whl/${CU_VERSION}/torch_stable.html" \
+      -f "https://download.pytorch.org/whl/${UPLOAD_CHANNEL}/${CU_VERSION}/torch_${UPLOAD_CHANNEL}.html"
+  fi
+}
+
+# Fill PYTORCH_VERSION with the latest conda nightly version, and
+# CONDA_CHANNEL_FLAGS with appropriate flags to retrieve these versions
+#
+# You MUST have populated PYTORCH_VERSION_SUFFIX before hand.
+setup_conda_pytorch_constraint() {
+  if [[ -z "$PYTORCH_VERSION" ]]; then
+    export CONDA_CHANNEL_FLAGS="${CONDA_CHANNEL_FLAGS} -c pytorch-nightly -c pytorch"
+    export PYTORCH_VERSION="$(conda search --json 'pytorch[channel=pytorch-nightly]' | \
+                              python -c "import os, sys, json, re; cuver = os.environ.get('CU_VERSION'); \
+                               cuver_1 = cuver.replace('cu', 'cuda') if cuver != 'cpu' else cuver; \
+                               cuver_2 = (cuver[:-1] + '.' + cuver[-1]).replace('cu', 'cuda') if cuver != 'cpu' else cuver; \
+                               print(re.sub(r'\\+.*$', '', \
+                                [x['version'] for x in json.load(sys.stdin)['pytorch'] \
+                                  if (x['platform'] == 'darwin' or cuver_1 in x['fn'] or cuver_2 in x['fn']) \
+                                    and 'py' + os.environ['PYTHON_VERSION'] in x['fn']][-1]))")"
+    if [[ -z "$PYTORCH_VERSION" ]]; then
+      echo "PyTorch version auto detection failed"
+      echo "No package found for CU_VERSION=$CU_VERSION and PYTHON_VERSION=$PYTHON_VERSION"
+      exit 1
+    fi
+  else
+    export CONDA_CHANNEL_FLAGS="${CONDA_CHANNEL_FLAGS} -c pytorch -c pytorch-${UPLOAD_CHANNEL}"
+  fi
+  if [[ "$CU_VERSION" == cpu ]]; then
+    export CONDA_PYTORCH_BUILD_CONSTRAINT="- pytorch==$PYTORCH_VERSION${PYTORCH_VERSION_SUFFIX}"
+    export CONDA_PYTORCH_CONSTRAINT="- pytorch==$PYTORCH_VERSION"
+  else
+    export CONDA_PYTORCH_BUILD_CONSTRAINT="- pytorch==${PYTORCH_VERSION}${PYTORCH_VERSION_SUFFIX}"
+    export CONDA_PYTORCH_CONSTRAINT="- pytorch==${PYTORCH_VERSION}${PYTORCH_VERSION_SUFFIX}"
+  fi
+  if [[ "$OSTYPE" == msys && "$CU_VERSION" == cu92 ]]; then
+    export CONDA_CHANNEL_FLAGS="${CONDA_CHANNEL_FLAGS} -c defaults -c numba/label/dev"
+  fi
+}
+
+# Translate CUDA_VERSION into CUDA_CUDATOOLKIT_CONSTRAINT
+setup_conda_cudatoolkit_constraint() {
+  export CONDA_BUILD_VARIANT="cuda"
+  if [[ "$(uname)" == Darwin ]]; then
+    export CONDA_BUILD_VARIANT="cpu"
+  else
+    case "$CU_VERSION" in
+      cu115)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=11.5,<11.6 # [not osx]"
+        ;;
+      cu113)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=11.3,<11.4 # [not osx]"
+        ;;
+      cu112)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=11.2,<11.3 # [not osx]"
+        ;;
+      cu111)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=11.1,<11.2 # [not osx]"
+        ;;
+      cu110)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=11.0,<11.1 # [not osx]"
+        ;;
+      cu102)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=10.2,<10.3 # [not osx]"
+        ;;
+      cu101)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=10.1,<10.2 # [not osx]"
+        ;;
+      cu100)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=10.0,<10.1 # [not osx]"
+        ;;
+      cu92)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="- cudatoolkit >=9.2,<9.3 # [not osx]"
+        ;;
+      cpu)
+        export CONDA_CUDATOOLKIT_CONSTRAINT=""
+        export CONDA_BUILD_VARIANT="cpu"
+        ;;
+      *)
+        echo "Unrecognized CU_VERSION=$CU_VERSION"
+        exit 1
+        ;;
+    esac
+  fi
+}
+
+setup_conda_cudatoolkit_plain_constraint() {
+  export CONDA_BUILD_VARIANT="cuda"
+  export CMAKE_USE_CUDA=1
+  if [[ "$(uname)" == Darwin ]]; then
+    export CONDA_BUILD_VARIANT="cpu"
+    export CMAKE_USE_CUDA=0
+  else
+    case "$CU_VERSION" in
+      cu115)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=11.5"
+        ;;
+      cu113)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=11.3"
+        ;;
+      cu112)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=11.2"
+        ;;
+      cu111)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=11.1"
+        ;;
+      cu102)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=10.2"
+        ;;
+      cu101)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=10.1"
+        ;;
+      cu100)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=10.0"
+        ;;
+      cu92)
+        export CONDA_CUDATOOLKIT_CONSTRAINT="cudatoolkit=9.2"
+        ;;
+      cpu)
+        export CONDA_CUDATOOLKIT_CONSTRAINT=""
+        export CONDA_BUILD_VARIANT="cpu"
+        export CMAKE_USE_CUDA=0
+        ;;
+      *)
+        echo "Unrecognized CU_VERSION=$CU_VERSION"
+        exit 1
+        ;;
+    esac
+  fi
+}
+
+# Build the proper compiler package before building the final package
+setup_visual_studio_constraint() {
+  if [[ "$OSTYPE" == "msys" ]]; then
+      export VSTOOLCHAIN_PACKAGE=vs$VC_YEAR
+      conda build $CONDA_CHANNEL_FLAGS --no-anaconda-upload packaging/$VSTOOLCHAIN_PACKAGE
+      cp packaging/$VSTOOLCHAIN_PACKAGE/conda_build_config.yaml packaging/torchvision/conda_build_config.yaml
+  fi
+}
+
+setup_junit_results_folder() {
+  if [[ "$CI" == "true" ]]; then
+    export CONDA_PYTORCH_BUILD_RESULTS_DIRECTORY="${SOURCE_ROOT_DIR}/build_results/results.xml"
+  fi
+}
+
+
+download_copy_ffmpeg() {
+  if [[ "$OSTYPE" == "msys" ]]; then
+    # conda install -yq ffmpeg=4.2 -c pytorch
+    # curl -L -q https://anaconda.org/pytorch/ffmpeg/4.3/download/win-64/ffmpeg-4.3-ha925a31_0.tar.bz2 --output ffmpeg-4.3-ha925a31_0.tar.bz2
+    # bzip2 --decompress --stdout ffmpeg-4.3-ha925a31_0.tar.bz2 | tar -x --file=-
+    # cp Library/bin/*.dll ../torchvision
+    echo "FFmpeg is disabled currently on Windows"
+  else
+    if [[ "$(uname)" == Darwin ]]; then
+      conda install -yq ffmpeg=4.2 -c pytorch
+      conda install -yq wget
+    else
+      # pushd ext_libraries
+      # wget -q https://anaconda.org/pytorch/ffmpeg/4.2/download/linux-64/ffmpeg-4.2-hf484d3e_0.tar.bz2
+      # tar -xjvf ffmpeg-4.2-hf484d3e_0.tar.bz2
+      # rm -rf ffmpeg-4.2-hf484d3e_0.tar.bz2
+      # ldconfig
+      # which ffmpeg
+      # popd
+      echo "FFmpeg is disabled currently on Linux"
+    fi
+  fi
+}
diff --git a/test/cpp/api/CMakeLists.txt b/test/cpp/api/CMakeLists.txt
index 6b801a07318..2852018034e 100644
--- a/test/cpp/api/CMakeLists.txt
+++ b/test/cpp/api/CMakeLists.txt
@@ -48,7 +48,13 @@ endif()
 
 add_executable(test_api ${TORCH_API_TEST_SOURCES})
 target_include_directories(test_api PRIVATE ${ATen_CPU_INCLUDE})
-target_link_libraries(test_api PRIVATE torch gtest)
+
+if(CMAKE_SYSTEM_NAME STREQUAL "Darwin")
+  target_link_libraries(test_api PRIVATE torch gtest "-Xpreprocessor -fopenmp -lomp -lgomp")
+else()
+  target_link_libraries(test_api PRIVATE torch gtest)
+endif()
+
 if(NOT MSVC)
   target_compile_options_if_supported(test_api -Wno-unused-variable)
 endif()
diff --git a/test/cpp/rpc/CMakeLists.txt b/test/cpp/rpc/CMakeLists.txt
index 3997f8753e5..50989d129ec 100644
--- a/test/cpp/rpc/CMakeLists.txt
+++ b/test/cpp/rpc/CMakeLists.txt
@@ -4,6 +4,7 @@ set(TORCH_RPC_TEST_SOURCES
   ${TORCH_RPC_TEST_DIR}/e2e_test_base.cpp
   ${TORCH_RPC_TEST_DIR}/test_wire_serialization.cpp
 )
+
 set(TORCH_RPC_TEST_DEPENDENCY_LIBS
   torch gtest
 )
diff --git a/test/distributed/elastic/multiprocessing/api_test.py b/test/distributed/elastic/multiprocessing/api_test.py
index 3b44169ae38..b9e1ced10f1 100644
--- a/test/distributed/elastic/multiprocessing/api_test.py
+++ b/test/distributed/elastic/multiprocessing/api_test.py
@@ -32,11 +32,7 @@ from torch.distributed.elastic.multiprocessing.api import (
 )
 from torch.distributed.elastic.multiprocessing.errors import ErrorHandler
 from torch.testing._internal.common_utils import (
-    IS_CI,
-    IS_MACOS,
-    IS_WINDOWS,
     NO_MULTIPROCESSING_SPAWN,
-    TEST_WITH_ASAN,
     TEST_WITH_DEV_DBG_ASAN,
     TEST_WITH_TSAN,
     TestCase,
diff --git a/test/onnx/test_utility_funs.py b/test/onnx/test_utility_funs.py
index e94c7bb8f4e..1014b61c61b 100644
--- a/test/onnx/test_utility_funs.py
+++ b/test/onnx/test_utility_funs.py
@@ -8,10 +8,7 @@ from typing import Callable
 
 import onnx
 import parameterized
-import pytorch_test_common
-
 import torch
-import torch.onnx
 import torch.utils.cpp_extension
 import torchvision
 from autograd_helper import CustomFunction as CustomFunction2
diff --git a/test/test_datapipe.py b/test/test_datapipe.py
index 59abbc28260..f921ca9614c 100644
--- a/test/test_datapipe.py
+++ b/test/test_datapipe.py
@@ -524,6 +524,23 @@ class TestCaptureDataFrame(TestCase):
 
         self.compare_capture_and_eager(operations)
 
+    @suppress_warnings  # Suppress warning for lambda fn
+    def test_map_with_col_file_handle_datapipe(self):
+        temp_dir = self.temp_dir.name
+        datapipe1 = dp.iter.FileLister(temp_dir, '')
+        datapipe2 = dp.iter.FileLoader(datapipe1)
+
+        def _helper(datapipe):
+            dp1 = datapipe.map(lambda x: x.read(), input_col=1)
+            dp2 = datapipe.map(lambda x: (x[0], x[1].read()))
+            self.assertEqual(list(dp1), list(dp2))
+
+        # tuple
+        _helper(datapipe2)
+        # list
+        datapipe3 = datapipe2.map(lambda x: list(x))
+        _helper(datapipe3)
+
 
 class TestDataFramesPipes(TestCase):
     """
diff --git a/test/test_python_dispatch.py b/test/test_python_dispatch.py
index 92df484df9b..db4b3d8e836 100644
--- a/test/test_python_dispatch.py
+++ b/test/test_python_dispatch.py
@@ -1700,5 +1700,15 @@ class TestPythonDispatcher(TestCase):
         python_disp_shape = torch.linalg.lstsq(a, b).solution.shape
         self.assertEqual(expected_shape, python_disp_shape)
 
+    def test_tolist_numpy_with_python_mode(self) -> None:
+        x = LoggingTensor(torch.tensor([2.0, 3.0]))
+        with self.assertRaisesRegex(RuntimeError, "is not supported for tensor subclasses."):
+            x.tolist()
+        with self.assertRaisesRegex(RuntimeError, "is not supported for tensor subclasses."):
+            x.numpy()
+        with self.assertRaises(AssertionError):
+            self.assertEqual(x, None)
+
+
 if __name__ == '__main__':
     run_tests()
diff --git a/test/test_serialization.py b/test/test_serialization.py
index 9b9a71334ba..27889f25126 100644
--- a/test/test_serialization.py
+++ b/test/test_serialization.py
@@ -308,6 +308,12 @@ class SerializationMixin:
     def test_serialization_sparse_safe(self):
         self._test_serialization(True)
 
+    def test_serialization_sparse(self):
+        self._test_serialization(False)
+
+    def test_serialization_sparse_safe(self):
+        self._test_serialization(True)
+
     def test_serialization_sparse_invalid(self):
         x = torch.zeros(3, 3)
         x[1][1] = 1
diff --git a/test/test_transformers.py b/test/test_transformers.py
index 3a85be95cac..c6dc4ea284c 100644
--- a/test/test_transformers.py
+++ b/test/test_transformers.py
@@ -1874,6 +1874,15 @@ class TestSDPA(NNTestCase):
             value = torch.randn(shape, dtype=torch.float16, device=device)
             self.assertRaises(RuntimeError, lambda: F.scaled_dot_product_attention(query, key, value))
 
+    # Test failing MHA when bias was NoneType
+    def test_bias_is_none(self):
+        x = torch.rand((1, 5, 10))
+        model = torch.nn.modules.activation.MultiheadAttention(10, 1, bias=False, batch_first=True)
+        model.eval()
+        model(x, x, x)
+        # completes without error
+
+
 # TODO: Replace this with instantiate_device_type_tests() to take advantage of test framework support for
 # cross device / dtype testing.
 instantiate_parametrized_tests(TestTransformers)
diff --git a/third_party/kineto b/third_party/kineto
index 2da532c91de..9380d640551 160000
--- a/third_party/kineto
+++ b/third_party/kineto
@@ -1 +1 @@
-Subproject commit 2da532c91dee9dc36cccc6088206daa1b69e3966
+Subproject commit 9380d64055137e609709b4b72230143848ca3465
diff --git a/third_party/nvfuser/CMakeLists.txt b/third_party/nvfuser/CMakeLists.txt
index 6dec9136271..d5306ba1bfd 100644
--- a/third_party/nvfuser/CMakeLists.txt
+++ b/third_party/nvfuser/CMakeLists.txt
@@ -152,7 +152,7 @@ else()
   target_link_libraries(${NVFUSER_CODEGEN} PRIVATE ${ROCM_HIPRTC_LIB})
   target_include_directories(${NVFUSER_CODEGEN} PRIVATE ${Caffe2_HIP_INCLUDE})
 endif()
-if(NOT MSVC)
+if(NOT MSVC AND NOT APPLE)
   target_compile_options(${NVFUSER_CODEGEN} PRIVATE -Wno-unused-variable)
 endif()
 target_include_directories(${NVFUSER_CODEGEN}
@@ -199,7 +199,9 @@ if(BUILD_PYTHON)
   # avoid using Python3_add_library, copied from functorch
   set_target_properties(${NVFUSER} PROPERTIES PREFIX "" DEBUG_POSTFIX "")
   if(NOT MSVC)
-    target_compile_options(${NVFUSER} PRIVATE -Wno-unused-variable)
+    if (NOT APPLE)
+      target_compile_options(${NVFUSER} PRIVATE -Wno-unused-variable)
+    endif()
     set_target_properties(${NVFUSER} PROPERTIES SUFFIX ".so")
   else()
     set_target_properties(${NVFUSER} PROPERTIES SUFFIX ".pyd")
@@ -316,7 +318,7 @@ if(BUILD_TEST AND USE_CUDA)
   endif()
   target_include_directories(${NVFUSER_TESTS} PRIVATE "${NVFUSER_ROOT}" "${TORCH_ROOT}/torch/csrc/api/include/")
   target_link_libraries(${NVFUSER_TESTS} PRIVATE ${NVFUSER_CODEGEN} torch ${TORCHLIB_FLAVOR} gtest_main gmock_main)
-  if(NOT MSVC)
+  if(NOT MSVC AND NOT APPLE)
     target_compile_options(${NVFUSER_TESTS} PRIVATE -Wno-unused-variable)
   endif()
 
diff --git a/third_party/nvfuser/csrc/codegen.cpp b/third_party/nvfuser/csrc/codegen.cpp
index da19576dbdd..f575ac61b89 100644
--- a/third_party/nvfuser/csrc/codegen.cpp
+++ b/third_party/nvfuser/csrc/codegen.cpp
@@ -421,7 +421,7 @@ class CudaKernelGenerator : private OptOutConstDispatch {
         } else {
           code_ << "NEG_INFINITY";
         }
-      } else if (std::isnan(val)) {
+      } else if (std::isnan(double(val))) {
         code_ << "NAN";
       } else {
         code_ << val;
diff --git a/third_party/nvfuser/csrc/executor.cpp b/third_party/nvfuser/csrc/executor.cpp
index 0ab2951bda6..46b953e15bd 100644
--- a/third_party/nvfuser/csrc/executor.cpp
+++ b/third_party/nvfuser/csrc/executor.cpp
@@ -363,7 +363,7 @@ void fillTensorWithNan(at::Tensor& t) {
       t.fill_(0x7FFFFFFF);
       break;
     case at::ScalarType::Long:
-      t.fill_(0x7FFFFFFFFFFFFFFFL);
+      t.fill_(0x7FFFFFFFFFFFFFFFLL);
       break;
     case at::ScalarType::Bool:
       t.fill_(true);
diff --git a/third_party/onnx b/third_party/onnx
index e192ba01e43..ad834eb73ee 160000
--- a/third_party/onnx
+++ b/third_party/onnx
@@ -1 +1 @@
-Subproject commit e192ba01e438d22ca2dedd7956e28e3551626c91
+Subproject commit ad834eb73ee0cd9b6fa9ea892caeed5fa17d7dc0
diff --git a/torch/CMakeLists.txt b/torch/CMakeLists.txt
index fb98cda7611..fcd0bbb1ac8 100644
--- a/torch/CMakeLists.txt
+++ b/torch/CMakeLists.txt
@@ -9,6 +9,13 @@ if(NOT CAFFE2_CMAKE_BUILDING_WITH_MAIN_REPO)
   set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
 endif()
 
+if(BUILD_BINARY)
+  add_library(aot_compiler SHARED
+          ${TORCH_SRC_DIR}/csrc/jit/mobile/nnc/aot_compiler.cpp
+          )
+  install(TARGETS aot_compiler DESTINATION lib)
+endif()
+
 if(NOT BUILD_PYTHON)
   return()
 endif()
diff --git a/torch/ao/quantization/qconfig.py b/torch/ao/quantization/qconfig.py
index 80f2f6dd768..5cff2589f2c 100644
--- a/torch/ao/quantization/qconfig.py
+++ b/torch/ao/quantization/qconfig.py
@@ -218,7 +218,7 @@ default_reuse_input_qconfig = QConfig(activation=default_reuse_input_observer,
 Default qconfig for operators that reuse the observers from input Tensor, e.g. reshape
 """
 
-def get_default_qconfig(backend='x86', version=0):
+def get_default_qconfig(backend='fbgemm', version=0):
     """
     Returns the default PTQ qconfig for the specified backend.
 
@@ -301,7 +301,7 @@ default_embedding_qat_qconfig = QConfig(activation=NoopObserver.with_args(dtype=
 default_embedding_qat_qconfig_4bit = QConfig(activation=NoopObserver.with_args(dtype=torch.float32),
                                              weight=default_embedding_fake_quant_4bit)
 
-def get_default_qat_qconfig(backend='x86', version=1):
+def get_default_qat_qconfig(backend='fbgemm', version=1):
     """
     Returns the default QAT qconfig for the specified backend.
 
@@ -419,7 +419,7 @@ def get_default_qconfig_dict(backend='x86', version=0):
         "a future version. Please use torch.ao.quantization.get_default_qconfig_mapping instead.")
     return torch.ao.quantization.get_default_qconfig_mapping(backend, version).to_dict()
 
-def get_default_qat_qconfig_dict(backend='x86', version=1):
+def get_default_qat_qconfig_dict(backend='fbgemm', version=1):
     warnings.warn(
         "torch.ao.quantization.get_default_qat_qconfig_dict is deprecated and will be removed in "
         "a future version. Please use torch.ao.quantization.get_default_qat_qconfig_mapping instead.")
diff --git a/torch/ao/quantization/qconfig_mapping.py b/torch/ao/quantization/qconfig_mapping.py
index 1c0c0a30818..5c462c8705f 100644
--- a/torch/ao/quantization/qconfig_mapping.py
+++ b/torch/ao/quantization/qconfig_mapping.py
@@ -111,7 +111,7 @@ def _get_default_qconfig_mapping(is_qat: bool, backend: str, version: int) -> QC
 
     return qconfig_mapping
 
-def get_default_qconfig_mapping(backend="x86", version=0) -> QConfigMapping:
+def get_default_qconfig_mapping(backend="fbgemm", version=0) -> QConfigMapping:
     """
     Return the default QConfigMapping for post training quantization.
 
@@ -123,7 +123,7 @@ def get_default_qconfig_mapping(backend="x86", version=0) -> QConfigMapping:
     # TODO: add assert for backend choices
     return _get_default_qconfig_mapping(False, backend, version)
 
-def get_default_qat_qconfig_mapping(backend="x86", version=1) -> QConfigMapping:
+def get_default_qat_qconfig_mapping(backend="fbgemm", version=1) -> QConfigMapping:
     """
     Return the default QConfigMapping for quantization aware training.
 
diff --git a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp
index bf87fa1b8b4..9c932e34fa6 100644
--- a/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp
+++ b/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp
@@ -673,7 +673,7 @@ ProcessGroupNCCL::ProcessGroupNCCL(
 
   RECORD_PARAM_COMMS(
       0, // seq
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       rank, // rank
       "init", // colName
       0, // inSize
@@ -1853,7 +1853,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::allreduce(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       tensors, // inputTensors
       tensors, // outputTensors
       rank_, // rank
@@ -1876,7 +1876,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::allreduce_coalesced(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       tensors, // inputTensors
       tensors, // outputTensors
       rank_, // rank
@@ -1902,7 +1902,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::broadcast(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       tensors, // inputTensors
       tensors, // outputTensors
       rank_, // rank
@@ -1960,7 +1960,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::_broadcast_oop(
       static_cast<int>(
           this->getSequenceNumberForGroup() +
           1), // seq + 1 to match collective increment.
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       inputTensors, // inputTensors
       outputTensors, // outputTensors
       rank_, // rank
@@ -2001,7 +2001,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::reduce(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this),
+      reinterpret_cast<int64_t>(this),
       tensors, // inputTensors
       tensors, // outputTensors
       rank_, // rank
@@ -2063,7 +2063,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::_reduce_oop(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       inputTensors, // inputTensors
       outputTensors, // outputTensors
       rank_, // rank
@@ -2119,7 +2119,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::allgather(
         static_cast<int>(
             this->getSequenceNumberForGroup() +
             1), // seq + 1 to match collective
-        reinterpret_cast<std::intptr_t>(this), // process group ptr
+        reinterpret_cast<int64_t>(this), // process group ptr
         inputTensors, // inputTensors
         outputTensors, // outputTensors
         rank_, // rank
@@ -2221,7 +2221,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::reduce_scatter(
         static_cast<int>(
             this->getSequenceNumberForGroup() +
             1), // seq + 1 to match collective
-        reinterpret_cast<std::intptr_t>(this), // process group ptr
+        reinterpret_cast<int64_t>(this), // process group ptr
         inputTensors, // inputTensors
         outputTensors, // outputTensors
         rank_, // rank
@@ -2319,7 +2319,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::_reduce_scatter_base(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       inputTensor, // inputTensor
       outputTensor, // outputTensor
       rank_, // rank
@@ -2366,7 +2366,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::barrier(const BarrierOptions& opts) {
   RECORD_PARAM_COMMS(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       rank_, // rank
       "barrier", // colName
       0, // inSize
@@ -2444,7 +2444,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::alltoall_base(
         static_cast<int>(
             this->getSequenceNumberForGroup() +
             1), // seq + 1 to match collective
-        reinterpret_cast<std::intptr_t>(this), // process group ptr
+        reinterpret_cast<int64_t>(this), // process group ptr
         inputTensor, // inputTensor
         outputTensor, // outputTensor
         rank_, // rank
@@ -2481,7 +2481,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::alltoall_base(
         static_cast<int>(
             this->getSequenceNumberForGroup() +
             1), // seq + 1 to match collective
-        reinterpret_cast<std::intptr_t>(this), // process group ptr
+        reinterpret_cast<int64_t>(this), // process group ptr
         inputTensor, // inputTensor
         outputTensor, // outputTensor
         rank_, // rank
@@ -2698,7 +2698,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::gather(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       inputTensors, // inputTensors
       outputTensors, // outputTensors
       rank_, // rank
@@ -2780,7 +2780,7 @@ c10::intrusive_ptr<Work> ProcessGroupNCCL::scatter(
   RECORD_PARAM_COMMS_DATA(
       static_cast<int>(
           this->getSequenceNumberForGroup() + 1), // seq + 1 to match collective
-      reinterpret_cast<std::intptr_t>(this), // process group ptr
+      reinterpret_cast<int64_t>(this), // process group ptr
       inputTensors, // inputTensors
       outputTensors, // outputTensors
       rank_, // rank
diff --git a/torch/csrc/distributed/c10d/init.cpp b/torch/csrc/distributed/c10d/init.cpp
index abc4359e7dd..8c0d1299e78 100644
--- a/torch/csrc/distributed/c10d/init.cpp
+++ b/torch/csrc/distributed/c10d/init.cpp
@@ -684,8 +684,7 @@ This class does not support ``__members__`` property.)");
       .value("BAND", ::c10d::ReduceOp::RedOpType::BAND)
       .value("BOR", ::c10d::ReduceOp::RedOpType::BOR)
       .value("BXOR", ::c10d::ReduceOp::RedOpType::BXOR)
-      .value("PREMUL_SUM", ::c10d::ReduceOp::RedOpType::PREMUL_SUM)
-      .export_values();
+      .value("PREMUL_SUM", ::c10d::ReduceOp::RedOpType::PREMUL_SUM);
 
   // note(crcrpar): This could be removed because users will not pass
   // `RedOpType` to reduce collective ops Ref: [Implicit
diff --git a/torch/csrc/distributed/rpc/testing/init.cpp b/torch/csrc/distributed/rpc/testing/init.cpp
index fc2dc156f7d..ead0275c8e0 100644
--- a/torch/csrc/distributed/rpc/testing/init.cpp
+++ b/torch/csrc/distributed/rpc/testing/init.cpp
@@ -33,7 +33,6 @@ PyObject* faulty_agent_init(PyObject* _unused, PyObject* noargs) {
 
   // Import the rpc_module so we can subclass TensorPipeAgent
   py::module rpc_module = py::module::import("torch.distributed.rpc");
-
 #ifdef USE_TENSORPIPE
   shared_ptr_class_<FaultyTensorPipeRpcBackendOptions>(
       module,
diff --git a/torch/distributed/distributed_c10d.py b/torch/distributed/distributed_c10d.py
index be0006d9cee..695f1410300 100644
--- a/torch/distributed/distributed_c10d.py
+++ b/torch/distributed/distributed_c10d.py
@@ -1559,7 +1559,7 @@ def broadcast(tensor, src, group=None, async_op=False):
         work.wait()
 
 @exception_handler
-def all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=None, async_op=False):
+def all_reduce_multigpu(tensor_list, op, group=None, async_op=False):
     r"""
     Reduces the tensor data across all machines in such a way that all get
     the final result. This function reduces a number of tensors on every node,
@@ -1620,7 +1620,7 @@ def all_reduce_multigpu(tensor_list, op=ReduceOp.SUM, group=None, async_op=False
         work.wait()
 
 @exception_handler
-def all_reduce(tensor, op=ReduceOp.SUM, group=None, async_op=False):
+def all_reduce(tensor, op, group=None, async_op=False):
     """
     Reduces the tensor data across all machines in such a way that all get
     the final result.
@@ -1651,7 +1651,7 @@ def all_reduce(tensor, op=ReduceOp.SUM, group=None, async_op=False):
         >>> tensor
         tensor([1, 2]) # Rank 0
         tensor([3, 4]) # Rank 1
-        >>> dist.all_reduce(tensor, op=ReduceOp.SUM)
+        >>> dist.all_reduce(tensor, op)
         >>> tensor
         tensor([4, 6]) # Rank 0
         tensor([4, 6]) # Rank 1
@@ -1662,7 +1662,7 @@ def all_reduce(tensor, op=ReduceOp.SUM, group=None, async_op=False):
         >>> tensor
         tensor([1.+1.j, 2.+2.j]) # Rank 0
         tensor([3.+3.j, 4.+4.j]) # Rank 1
-        >>> dist.all_reduce(tensor, op=ReduceOp.SUM)
+        >>> dist.all_reduce(tensor, op)
         >>> tensor
         tensor([4.+4.j, 6.+6.j]) # Rank 0
         tensor([4.+4.j, 6.+6.j]) # Rank 1
@@ -1692,7 +1692,7 @@ def all_reduce(tensor, op=ReduceOp.SUM, group=None, async_op=False):
         work.wait()
 
 @exception_handler
-def all_reduce_coalesced(tensors, op=ReduceOp.SUM, group=None, async_op=False):
+def all_reduce_coalesced(tensors, op, group=None, async_op=False):
     """
     WARNING: at this time individual shape checking is not implemented across nodes.
     For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the
@@ -1756,7 +1756,7 @@ def all_reduce_coalesced(tensors, op=ReduceOp.SUM, group=None, async_op=False):
 
 @exception_handler
 def reduce_multigpu(
-    tensor_list, dst, op=ReduceOp.SUM, group=None, async_op=False, dst_tensor=0
+    tensor_list, dst, op, group=None, async_op=False, dst_tensor=0
 ):
     """
     Reduces the tensor data on multiple GPUs across all machines. Each tensor
@@ -1817,7 +1817,7 @@ def reduce_multigpu(
         work.wait()
 
 @exception_handler
-def reduce(tensor, dst, op=ReduceOp.SUM, group=None, async_op=False):
+def reduce(tensor, dst, op, group=None, async_op=False):
     """
     Reduces the tensor data across all machines.
 
@@ -2805,7 +2805,7 @@ def scatter(tensor, scatter_list=None, src=0, group=None, async_op=False):
 
 @exception_handler
 def reduce_scatter_multigpu(
-    output_tensor_list, input_tensor_lists, op=ReduceOp.SUM, group=None, async_op=False
+    output_tensor_list, input_tensor_lists, op, group=None, async_op=False
 ):
     """
     Reduce and scatter a list of tensors to the whole group.  Only nccl backend
@@ -2874,7 +2874,7 @@ def reduce_scatter_multigpu(
 
 
 @exception_handler
-def reduce_scatter(output, input_list, op=ReduceOp.SUM, group=None, async_op=False):
+def reduce_scatter(output, input_list, op, group=None, async_op=False):
     """
     Reduces, then scatters a list of tensors to all processes in a group.
 
@@ -2916,7 +2916,7 @@ def reduce_scatter(output, input_list, op=ReduceOp.SUM, group=None, async_op=Fal
 
 
 @exception_handler
-def reduce_scatter_tensor(output, input, op=ReduceOp.SUM, group=None, async_op=False):
+def reduce_scatter_tensor(output, input, op, group=None, async_op=False):
     """
     Reduces, then scatters a tensor to all ranks in a group.
 
@@ -2992,7 +2992,7 @@ def reduce_scatter_tensor(output, input, op=ReduceOp.SUM, group=None, async_op=F
         work.wait()
 
 
-def _reduce_scatter_base(output, input, op=ReduceOp.SUM, group=None, async_op=False):
+def _reduce_scatter_base(output, input, op, group=None, async_op=False):
     """
     Reduces, then scatters a flattened tensor to all processes in a group.
 
diff --git a/torch/distributed/elastic/agent/server/local_elastic_agent.py b/torch/distributed/elastic/agent/server/local_elastic_agent.py
index 6f14eb07ff3..181f3031ee8 100644
--- a/torch/distributed/elastic/agent/server/local_elastic_agent.py
+++ b/torch/distributed/elastic/agent/server/local_elastic_agent.py
@@ -32,6 +32,7 @@ from torch.distributed.elastic.multiprocessing import PContext, start_processes
 from torch.distributed.elastic.utils import macros
 from torch.distributed.elastic.utils.logging import get_logger
 
+
 log = get_logger()
 
 __all__ = [
diff --git a/torch/distributed/rpc/constants.py b/torch/distributed/rpc/constants.py
index 3bc525b70d9..e488d86bd53 100644
--- a/torch/distributed/rpc/constants.py
+++ b/torch/distributed/rpc/constants.py
@@ -1,3 +1,8 @@
+'''
+Descripttion: C++ Practice
+Author: TomHeaven
+Date: 2023-03-18 01:49:34
+'''
 from datetime import timedelta
 from typing import List
 from torch._C._distributed_rpc import (
diff --git a/torch/fx/passes/backends/nvfuser.py b/torch/fx/passes/backends/nvfuser.py
new file mode 100644
index 00000000000..fdb1dd9a332
--- /dev/null
+++ b/torch/fx/passes/backends/nvfuser.py
@@ -0,0 +1,286 @@
+from typing import Dict
+
+import torch
+from torch.nn import Module
+from torch._ops import OpOverload
+
+from torch.fx import GraphModule
+from torch.fx.node import Node, _get_qualified_name
+from torch.fx.passes.operator_support import OperatorSupport
+from torch.fx.passes.tools_common import CALLABLE_NODE_OPS
+from torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner
+from torch._prims.executor import execute
+from torch.fx.experimental.proxy_tensor import DecompositionInterpreter
+from torch._decomp import decomposition_table
+
+import typing as t
+
+import logging
+
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.WARNING)
+
+def aten_to_dtype(self, dtype: torch.dtype, **kwargs):
+    if len(kwargs) > 0 or not dtype:
+        raise RuntimeError("No support for other to.dtype() formats other than to.dtype(self, dtype)")
+    return torch._prims.convert_element_type(self, dtype)
+
+# decomposition_table currently contains both aten2aten and aten2prim decomposition
+# this is a hack to separate them, as we only need aten2prim decomposition for nvfuser-supported aten graph lowering
+aten2aten_decomp = {}
+aten2prim_decomp = {}
+
+for op, decomp_fn in decomposition_table.items():
+    if "torch._refs" in decomp_fn.__module__:
+        aten2prim_decomp[op] = decomp_fn
+    else:
+        aten2aten_decomp[op] = decomp_fn
+
+aten2aten_decomp_skips = {
+    "aten.native_layer_norm_backward.default",
+    "aten.embedding_dense_backward.default",   # This is hurting nvfuser's perf
+    "aten.addmm.default"
+}
+
+for op, decomp_fn in decomposition_table.items():
+    if "torch._refs" in decomp_fn.__module__:
+        aten2prim_decomp[op] = decomp_fn
+    else:
+        if str(op) not in aten2aten_decomp_skips:
+            aten2aten_decomp[op] = decomp_fn
+
+
+aten2prim_decomp[torch.ops.aten.to.dtype] = aten_to_dtype
+
+
+class NvFuserOperatorSupport(OperatorSupport):
+    """
+    Operator support for nvFuser backend.
+
+    Currently, partitioning is based on FX ATen graph. The fused subgraph will latter be decomposed into prims.
+    To determine if an ATen ops is supported by nvFuser, we shall check the prim ops used in its ref decomposition.
+    Only if all the prim ops in the ref has a nvfuser_impl, we say this Aten op is suppported by nvFuser.
+
+    Note: When adding a rule, please add it to the corresponding section and follow the
+    alphabetical order.
+    """
+
+    def __init__(self):
+
+        # TODO: current list copied from torch/csrc/jit/codegen/cuda/parser.cpp is incorrect,
+        # as that file is solely for TorchScript and doesn't represent the actual status
+        # whether operation would be runnable by primTorch+nvFuser.
+        # We will iterate on this list to reflect the the reality.
+        support_dict = {
+            # ===============================================================
+            # call_function aten
+            # ===============================================================
+            # Following supported aten ops is copied from torch/csrc/jit/codegen/cuda/parser.cpp
+            # TODO: might need to update according to supported input types
+            "torch.ops.aten.add": None,
+            "torch.ops.aten.sub": None,
+            # "torch.ops.aten.rsub": None,    # rsub decomp is supported at aten2aten level
+            "torch.ops.aten.div": None,
+            "torch.ops.aten.atan2": None,
+            "torch.ops.aten.mul": None,
+            "torch.ops.aten.max": None,
+            "torch.ops.aten.min": None,
+            "torch.ops.aten.pow": None,
+            "torch.ops.aten.remainder": None,
+            "torch.ops.aten.fmod": None,
+            "torch.ops.aten.bitwise_and": None,
+            "torch.ops.aten.__and__": None,
+            "torch.ops.aten.bitwise_or": None,
+            "torch.ops.aten.__or__": None,
+            "torch.ops.aten.bitwise_xor": None,
+            "torch.ops.aten.__xor__": None,
+            "torch.ops.aten.bitwise_left_shift": None,
+            "torch.ops.aten.__lshift__": None,
+            "torch.ops.aten.bitwise_right_shift": None,
+            "torch.ops.aten.__rshift__": None,
+            "torch.ops.aten.eq": None,
+            "torch.ops.aten.ne": None,
+            "torch.ops.aten.ge": None,
+            "torch.ops.aten.gt": None,
+            "torch.ops.aten.le": None,
+            "torch.ops.aten.lt": None,
+            "torch.ops.aten.abs": None,
+            "torch.ops.aten.bitwise_not": None,
+            "torch.ops.aten.ceil": None,
+            "torch.ops.aten.floor": None,
+            "torch.ops.aten.frac": None,
+            "torch.ops.aten.neg": None,
+            "torch.ops.aten.relu": None,
+            "torch.ops.aten.round": None,
+            "torch.ops.aten.silu": None,
+            "torch.ops.aten.trunc": None,
+            "torch.ops.aten.log": None,
+            "torch.ops.aten.log10": None,
+            "torch.ops.aten.log1p": None,
+            "torch.ops.aten.log2": None,
+            "torch.ops.aten.lgamma": None,
+            "torch.ops.aten.exp": None,
+            "torch.ops.aten.expm1": None,
+            "torch.ops.aten.erf": None,
+            "torch.ops.aten.erfc": None,
+            "torch.ops.aten.cos": None,
+            "torch.ops.aten.acos": None,
+            "torch.ops.aten.cosh": None,
+            "torch.ops.aten.sin": None,
+            "torch.ops.aten.asin": None,
+            "torch.ops.aten.sinh": None,
+            "torch.ops.aten.tan": None,
+            "torch.ops.aten.atan": None,
+            "torch.ops.aten.tanh": None,
+            "torch.ops.aten.atanh": None,
+            "torch.ops.aten.sqrt": None,
+            "torch.ops.aten.rsqrt": None,
+            "torch.ops.aten.reciprocal": None,
+            "torch.ops.aten.sigmoid": None,
+            "torch.ops.aten.isfinite": None,
+            "torch.ops.aten.isinf": None,
+            "torch.ops.aten.isnan": None,
+            "torch.ops.aten.isneginf": None,
+            "torch.ops.aten.isposinf": None,
+            "torch.ops.aten.isreal": None,
+            # "torch.ops.aten.rand_like": None,  # causing Node empty_like_default does not support nvfuser
+            "torch.ops.aten.softplus": None,
+            "torch.ops.aten.threshold": None,
+            # relying on aten->aten->prim decomp, aten2aten is using unsupported aten.new_zero op
+            # "torch.ops.aten.threshold_backward": None,
+            "torch.ops.aten.clamp": None,
+            # "torch.ops.aten.clone": None,
+            # Failing with where(): incompatible function arguments: \
+            # [<torch._C._nvfuser.TensorView, tensor, <torch._C._nvfuser.TensorView]
+            # failing with BERT_pytorch_forward_0, which has aten.where.ScalarSelf in the decomps
+            # "torch.ops.aten.where": None,
+            # However, aten.where.self overload is fully supported
+            "torch.ops.aten.where.self": None,
+            "torch.ops.aten.lerp": None,
+            "torch.ops.aten.addcmul": None,
+            # "torch.ops.aten.native_dropout": None,    # missing refs for aten.rank_like
+            "torch.ops.aten.dropout": None,
+            # "torch.ops.aten.native_dropout_backward": None,   # missing refs for aten.type_as
+            "torch.ops.aten.instance_norm": None,
+            "torch.ops.aten._batch_norm_impl_index": None,
+            # "torch.ops.aten.native_batch_norm": None,     # missing refs for aten.var
+            "torch.ops.aten.batch_norm": None,
+            "torch.ops.aten.cudnn_batch_norm": None,
+            "torch.ops.aten._batch_norm_impl_index_backward": None,
+            # "torch.ops.aten.native_batch_norm_backward": None,    # should have been handled at aten2aten decomp
+            "torch.ops.aten.native_layer_norm": None,
+            "torch.ops.aten.layer_norm": None,
+            # relying on aten->aten->prim decomp, aten2aten is using unsupported aten.div
+            # "torch.ops.aten.native_layer_norm_backward": None,
+            "torch.ops.aten.softmax.int": None,
+            "torch.ops.aten.log_softmax.int": None,
+            # relying on aten->aten->prim decomp, aten2aten is using unsupported aten.amax
+            # "torch.ops.aten._softmax": None,
+            "torch.ops.aten._log_softmax_backward_data": None,
+            # "torch.ops.aten._softmax_backward_data": None,  # Node _softmax_backward_data_default does not support nvfuser
+            # "torch.ops.aten.var.dim": None,       # missing refs
+            "torch.ops.aten.std.dim": None,
+            "torch.ops.aten.sum": None,
+            # "torch.ops.aten.mean.dim": None,      # missing refs
+            "torch.ops.aten._grad_sum_to_size": None,
+            "torch.ops.aten.sum_to_size": None,
+            "torch.ops.aten._autocast_to_reduced_precision": None,
+            "torch.ops.aten._autocast_to_full_precision": None,
+            # "torch.ops.aten.to.dtype": None,      # causing segfault
+            # "torch.ops.aten.type_as": None,       # missing refs
+            "torch.ops.aten.linear": None,
+            "torch.ops.aten.gelu": None,
+            # "torch.ops.aten.gelu_backward": None,       # gelu_backward is handled at aten2aten decomp
+            # "torch.ops.aten.hardtanh": None,        # has functional ref, using unsupported aten.clamp
+            "torch.ops.aten.leaky_relu": None,
+            "torch.ops.aten.square": None,
+            # relying on aten->aten->prim decomp, aten2aten is using unsupported aten.conj_physical
+            "torch.ops.aten.tanh_backward": None,
+            # "torch.ops.aten.amax": None,      # missing prim decomp
+            # "torch.ops.aten.amin": None,      # missing prim decomp
+            # "torch.ops.aten.reshape": None,
+            # "torch.ops.aten.view": None,      # missing prim decomp
+            "torch.ops.aten.flatten.using_ints": None,
+
+            # ===============================================================
+            # call_function builtins and operator
+            # ===============================================================
+            "getattr": None,
+            "_operator.getitem": None,
+        }
+
+        super().__init__(support_dict)
+
+    def is_node_supported(
+        self, submodules: t.Mapping[str, Module], node: Node
+    ) -> bool:
+
+        # nvFuser FX subgraph should be purely functional
+        if node.op not in CALLABLE_NODE_OPS:
+            return False
+
+        # ops in supported_dict doesn't have overload name
+        # use overloadpacket's qualified_name for OpOverload
+        if isinstance(node.target, OpOverload):
+            target = _get_qualified_name(node.target.overloadpacket)
+            if target in self._support_dict:
+                return True
+
+        return super().is_node_supported(submodules, node)
+
+
+class NvFuserBackend:
+    def __init__(self):
+        self.supported_ops = NvFuserOperatorSupport()
+
+        # TODO: this is a naive implementation of cache without proper guard
+        self.partitioner_cache: Dict[GraphModule, GraphModule] = {}
+
+        # TODO: this is a naive implementation of cache without proper guard, this will only work for identical inputs
+        self.prim_decomp_cache: Dict[GraphModule, GraphModule] = {}
+
+    def lower_to_prims_and_execute(self, graph_module: GraphModule, *args, **kwargs):
+        # `graph_module` is an Aten-Fx graph
+        # "lowering to prims" and "trace execution" are grouped into this function, as they are both input dependent
+
+        if graph_module in self.prim_decomp_cache:
+            logger.debug("prim_decomp_cache hit!")
+            prim_module = self.prim_decomp_cache[graph_module]
+        else:
+            prim_graph = torch.fx.Graph()
+            DecompositionInterpreter(graph_module, prim_graph, decomposition_table=aten2prim_decomp).run(*args, **kwargs)
+            prim_module = torch.fx.GraphModule(graph_module, prim_graph)
+            self.prim_decomp_cache[graph_module] = prim_module
+
+            logger.debug("Lower to prims graph: ", prim_module.code)
+
+        # invokes trace executor for running the prim graph
+        return execute(prim_module, *args, executor="nvfuser")
+
+    def compile(self, graph_module: GraphModule) -> GraphModule:
+        # entry function for nvFuser backend
+        logger.debug("Compiling graph_module: ", graph_module.code)
+
+        # FX graph based partitioning based on nvfuser supported ops
+        if graph_module in self.partitioner_cache:
+            logger.debug("partitioner_cache hit!")
+            fused_graph_module = self.partitioner_cache[graph_module]
+        else:
+            partitioner = CapabilityBasedPartitioner(
+                graph_module, self.supported_ops, allows_single_node_partition=False)
+            fused_graph_module = partitioner.partition_and_fuse()
+
+            self.partitioner_cache[graph_module] = fused_graph_module
+
+        # Overriding fused_module's __call__() function with lower_to_prims_and_execute()
+        for node in fused_graph_module.graph.nodes:
+            # TODO: use a better way to identify fused submodule
+            if node.op == "call_module" and "fused_" in node.name:
+                fused_module = getattr(fused_graph_module, node.name)
+                fused_module._wrapped_call = self.lower_to_prims_and_execute
+
+        return fused_graph_module
+
+    def __call__(self, graph_module: GraphModule, _) -> GraphModule:
+        # wrap self.compile as __call__ function to fit the interface for AOTAutograd's fw_compiler
+        return self.compile(graph_module)
diff --git a/torch/nn/modules/transformer.py b/torch/nn/modules/transformer.py
index 560028ad53c..f44c27506a6 100644
--- a/torch/nn/modules/transformer.py
+++ b/torch/nn/modules/transformer.py
@@ -492,6 +492,11 @@ class TransformerEncoderLayer(Module):
             target_type=src.dtype
         )
 
+        if src_key_padding_mask is not None:
+            _skpm_dtype = src_key_padding_mask.dtype
+            if _skpm_dtype != torch.bool and not torch.is_floating_point(src_key_padding_mask):
+                raise AssertionError(
+                    "only bool and floating types of key_padding_mask are supported")
         # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf
         why_not_sparsity_fast_path = ''
         if not src.dim() == 3:
diff --git a/torch/nn/utils/stateless.py b/torch/nn/utils/stateless.py
index e35ddd739bb..fca1c376b2e 100644
--- a/torch/nn/utils/stateless.py
+++ b/torch/nn/utils/stateless.py
@@ -1,3 +1,4 @@
+import warnings
 import contextlib
 import warnings
 from collections import defaultdict
diff --git a/torch/onnx/utils.py b/torch/onnx/utils.py
index 5f2a460f908..3a9160813b7 100644
--- a/torch/onnx/utils.py
+++ b/torch/onnx/utils.py
@@ -1007,6 +1007,20 @@ def _get_named_param_dict(graph, params):
     _params_dict = dict(zip(param_names, params))
     return _params_dict
 
+def _get_example_outputs(model, args):
+    input_args = copy.deepcopy(args)
+    input_kwargs = {}
+    if input_args and isinstance(input_args[-1], dict):
+        input_kwargs = input_args[-1]
+        input_args = input_args[:-1]
+
+    example_outputs = model(*input_args, **input_kwargs)
+    if isinstance(example_outputs, (torch.Tensor, int, float, bool)):
+        example_outputs = (example_outputs,)
+
+    if isinstance(example_outputs, list):
+        example_outputs = [example_outputs]
+    return example_outputs
 
 @_beartype.beartype
 def _get_example_outputs(model, args):
diff --git a/torch/utils/data/datapipes/iter/callable.py b/torch/utils/data/datapipes/iter/callable.py
index 4e3dce4b82d..108f4ff1177 100644
--- a/torch/utils/data/datapipes/iter/callable.py
+++ b/torch/utils/data/datapipes/iter/callable.py
@@ -24,24 +24,19 @@ class MapperIterDataPipe(IterDataPipe[T_co]):
     Applies a function over each item from the source DataPipe (functional name: ``map``).
     The function can be any regular Python function or partial object. Lambda
     function is not recommended as it is not supported by pickle.
-
     Args:
         datapipe: Source Iterable DataPipe
         fn: Function being applied over each item
         input_col: Index or indices of data which ``fn`` is applied, such as:
-
             - ``None`` as default to apply ``fn`` to the data directly.
             - Integer(s) is used for list/tuple.
             - Key(s) is used for dict.
-
         output_col: Index of data where result of ``fn`` is placed. ``output_col`` can be specified
             only when ``input_col`` is not ``None``
-
             - ``None`` as default to replace the index that ``input_col`` specified; For ``input_col`` with
               multiple indices, the left-most one is used, and other indices will be removed.
             - Integer is used for list/tuple. ``-1`` represents to append result at the end.
             - Key is used for dict. New key is acceptable.
-
     Example:
         >>> # xdoctest: +SKIP
         >>> from torchdata.datapipes.iter import IterableWrapper, Mapper
@@ -173,11 +168,9 @@ class CollatorIterDataPipe(MapperIterDataPipe):
     r"""
     Collates samples from DataPipe to Tensor(s) by a custom collate function (functional name: ``collate``).
     By default, it uses :func:`torch.utils.data.default_collate`.
-
     .. note::
         While writing a custom collate function, you can import :func:`torch.utils.data.default_collate` for the
         default behavior and `functools.partial` to specify any additional arguments.
-
     Args:
         datapipe: Iterable DataPipe being collated
         collate_fn: Customized collate function to collect and combine data or a batch of data.
@@ -231,4 +224,4 @@ class CollatorIterDataPipe(MapperIterDataPipe):
             else:
                 # TODO(VitalyFedyunin): Validate passed dictionary
                 collate_fn = functools.partial(_collate_helper, conversion)
-                super().__init__(datapipe, fn=collate_fn)
+                super().__init__(datapipe, fn=collate_fn)
\ No newline at end of file
